{"2025-02-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09621v1","updated":"2025-02-13T18:59:46Z","published":"2025-02-13T18:59:46Z","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency","summary":"  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanwei Li","Yu Qi","Xinyan Chen","Liuhui Wang","Jianhan Jin","Claire Guo","Shen Yan","Bo Zhang","Chaoyou Fu","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.09621v1.pdf","comment":"Project Page: https://mmecot.github.io/"},{"id":"http://arxiv.org/abs/2502.09620v1","updated":"2025-02-13T18:59:45Z","published":"2025-02-13T18:59:45Z","title":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","summary":"  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n","authors":["Yiwen Tang","Zoey Guo","Zhuhao Wang","Ray Zhang","Qizhi Chen","Junli Liu","Delin Qu","Zhigang Wang","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.09620v1.pdf","comment":"The code is released at https://github.com/Ivan-Tang-3D/ENEL"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09601v1","updated":"2025-02-13T18:52:36Z","published":"2025-02-13T18:52:36Z","title":"CoT-Valve: Length-Compressible Chain-of-Thought Tuning","summary":"  Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.\n","authors":["Xinyin Ma","Guangnian Wan","Runpeng Yu","Gongfan Fang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09601v1.pdf","comment":"Work in progress. Code will be released at\n  https://github.com/horseee/CoT-Valve"},{"id":"http://arxiv.org/abs/2502.09597v1","updated":"2025-02-13T18:52:03Z","published":"2025-02-13T18:52:03Z","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs","summary":"  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n","authors":["Siyan Zhao","Mingyi Hong","Yang Liu","Devamanyu Hazarika","Kaixiang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09597v1.pdf","comment":"Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/"},{"id":"http://arxiv.org/abs/2502.09589v1","updated":"2025-02-13T18:46:44Z","published":"2025-02-13T18:46:44Z","title":"Logical forms complement probability in understanding language model\n  (and human) performance","summary":"  With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as orthogonal factors. In addition, we show\nsimilarities and differences between the logical reasoning performances of\nhumans and LLMs by comparing LLM and human behavioral results.\n","authors":["Yixuan Wang","Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09589v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09567v1","updated":"2025-02-13T18:22:31Z","published":"2025-02-13T18:22:31Z","title":"MorphNLI: A Stepwise Approach to Natural Language Inference Using Text\n  Morphing","summary":"  We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label.\n","authors":["Vlad Andrei Negru","Robert Vacareanu","Camelia Lemnaru","Mihai Surdeanu","Rodica Potolea"],"pdf_url":"https://arxiv.org/pdf/2502.09567v1.pdf","comment":"16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.09566v1","updated":"2025-02-13T18:21:15Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09560v1","updated":"2025-02-13T18:11:34Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2406.05925v2","updated":"2025-02-13T18:02:34Z","published":"2024-06-09T21:58:32Z","title":"Hello Again! LLM-powered Personalized Agent for Long-term Dialogue","summary":"  Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.\n","authors":["Hao Li","Chenghao Yang","An Zhang","Yang Deng","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.05925v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2406.06773v2","updated":"2025-02-13T17:50:39Z","published":"2024-06-10T20:19:55Z","title":"Evaluating Zero-Shot Long-Context LLM Compression","summary":"  This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.\n","authors":["Chenyu Wang","Yihan Wang","Kai Li"],"pdf_url":"https://arxiv.org/pdf/2406.06773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09532v1","updated":"2025-02-13T17:49:30Z","published":"2025-02-13T17:49:30Z","title":"Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages","summary":"  Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.\n","authors":["Shreyan Biswas","Alexander Erlei","Ujwal Gadiraju"],"pdf_url":"https://arxiv.org/pdf/2502.09532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08489v2","updated":"2025-02-13T17:33:24Z","published":"2025-02-12T15:26:08Z","title":"Salamandra Technical Report","summary":"  This work introduces Salamandra, a suite of open-source decoder-only large\nlanguage models available in three different sizes: 2, 7, and 40 billion\nparameters. The models were trained from scratch on highly multilingual data\nthat comprises text in 35 European languages and code. Our carefully curated\ncorpus is made exclusively from open-access data compiled from a wide variety\nof sources. Along with the base models, supplementary checkpoints that were\nfine-tuned on public-domain instruction data are also released for chat\napplications. Additionally, we also share our preliminary experiments on\nmultimodality, which serve as proof-of-concept to showcase potential\napplications for the Salamandra family. Our extensive evaluations on\nmultilingual benchmarks reveal that Salamandra has strong capabilities,\nachieving competitive performance when compared to similarly sized open-source\nmodels. We provide comprehensive evaluation results both on standard downstream\ntasks as well as key aspects related to bias and safety.With this technical\nreport, we intend to promote open science by sharing all the details behind our\ndesign choices, data curation strategy and evaluation methodology. In addition\nto that, we deviate from the usual practice by making our training and\nevaluation scripts publicly accessible. We release all models under a\npermissive Apache 2.0 license in order to foster future research and facilitate\ncommercial use, thereby contributing to the open-source ecosystem of large\nlanguage models.\n","authors":["Aitor Gonzalez-Agirre","Marc Pàmies","Joan Llop","Irene Baucells","Severino Da Dalt","Daniel Tamayo","José Javier Saiz","Ferran Espuña","Jaume Prats","Javier Aula-Blasco","Mario Mina","Iñigo Pikabea","Adrián Rubio","Alexander Shvets","Anna Sallés","Iñaki Lacunza","Jorge Palomar","Júlia Falcão","Lucía Tormo","Luis Vasquez-Reina","Montserrat Marimon","Oriol Pareras","Valle Ruiz-Fernández","Marta Villegas"],"pdf_url":"https://arxiv.org/pdf/2502.08489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05331v2","updated":"2025-02-13T17:27:15Z","published":"2025-02-07T21:13:27Z","title":"Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books","summary":"  Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch.\n","authors":["Sangmitra Madhusudan","Robert Morabito","Skye Reid","Nikta Gohari Sadr","Ali Emami"],"pdf_url":"https://arxiv.org/pdf/2502.05331v2.pdf","comment":"9 pages (excluding references), accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2408.14792v2","updated":"2025-02-13T17:22:36Z","published":"2024-08-27T05:56:04Z","title":"Measuring Human Contribution in AI-Assisted Content Generation","summary":"  With the growing prevalence of generative artificial intelligence (AI), an\nincreasing amount of content is no longer exclusively generated by humans but\nby generative AI models with human guidance. This shift presents notable\nchallenges for the delineation of originality due to the varying degrees of\nhuman contribution in AI-assisted works. This study raises the research\nquestion of measuring human contribution in AI-assisted content generation and\nintroduces a framework to address this question that is grounded in information\ntheory. By calculating mutual information between human input and AI-assisted\noutput relative to self-information of AI-assisted output, we quantify the\nproportional information contribution of humans in content generation. Our\nexperimental results demonstrate that the proposed measure effectively\ndiscriminates between varying degrees of human contribution across multiple\ncreative domains. We hope that this work lays a foundation for measuring human\ncontributions in AI-assisted content generation in the era of generative AI.\n","authors":["Yueqi Xie","Tao Qi","Jingwei Yi","Xiyuan Yang","Ryan Whalen","Junming Huang","Qian Ding","Yu Xie","Xing Xie","Fangzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06759v2","updated":"2025-02-13T17:12:34Z","published":"2025-02-10T18:38:57Z","title":"Rationalization Models for Text-to-SQL","summary":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","authors":["Gaetano Rossiello","Nhan Pham","Michael Glass","Junkyu Lee","Dharmashankar Subramanian"],"pdf_url":"https://arxiv.org/pdf/2502.06759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08168v2","updated":"2025-02-13T17:11:41Z","published":"2025-02-12T07:19:36Z","title":"SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation","summary":"  As a powerful all-weather Earth observation tool, synthetic aperture radar\n(SAR) remote sensing enables critical military reconnaissance, maritime\nsurveillance, and infrastructure monitoring. Although Vision language models\n(VLMs) have made remarkable progress in natural language processing and image\nunderstanding, their applications remain limited in professional domains due to\ninsufficient domain expertise. This paper innovatively proposes the first\nlarge-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which\ncontains approximately 2 million high-quality image-text pairs, encompasses\ndiverse scenarios with detailed target annotations. This dataset not only\nsupports several key tasks such as visual understanding and object detection\ntasks, but also has unique innovative aspects: this study develop a\nvisual-language dataset and benchmark for the SAR domain, enabling and\nevaluating VLMs' capabilities in SAR image interpretation, which provides a\nparadigmatic framework for constructing multimodal datasets across various\nremote sensing vertical domains. Through experiments on 16 mainstream VLMs, the\neffectiveness of the dataset has been fully verified. The project will be\nreleased at https://github.com/JimmyMa99/SARChat.\n","authors":["Zhiming Ma","Xiayang Xiao","Sihao Dong","Peidong Wang","HaiPeng Wang","Qingyun Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08168v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09497v1","updated":"2025-02-13T17:09:52Z","published":"2025-02-13T17:09:52Z","title":"Improve LLM-based Automatic Essay Scoring with Linguistic Features","summary":"  Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.\n","authors":["Zhaoyi Joey Hou","Alejandro Ciuba","Xiang Lorraine Li"],"pdf_url":"https://arxiv.org/pdf/2502.09497v1.pdf","comment":"To be published in the workshop Innovation and Responsibility in\n  AI-Supported Education (iRaise) at the 2025 Conference on Artificial\n  Intelligence (AAAI)"},{"id":"http://arxiv.org/abs/2312.00326v9","updated":"2025-02-13T17:06:52Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v9.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2502.09457v1","updated":"2025-02-13T16:25:16Z","published":"2025-02-13T16:25:16Z","title":"The Multilingual Mind : A Survey of Multilingual Reasoning in Language\n  Models","summary":"  While reasoning and multilingual capabilities in Language Models (LMs) have\nachieved remarkable progress in recent years, their integration into a unified\nparadigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning\nrequires language models to handle logical reasoning across languages while\naddressing misalignment, biases, and challenges in low-resource settings. This\nsurvey provides the first in-depth review of multilingual reasoning in LMs. In\nthis survey, we provide a systematic overview of existing methods that leverage\nLMs for multilingual reasoning, specifically outlining the challenges,\nmotivations, and foundational aspects of applying language models to reason\nacross diverse languages. We provide an overview of the standard data resources\nused for training multilingual reasoning in LMs and the evaluation benchmarks\nemployed to assess their multilingual capabilities. Next, we analyze various\nstate-of-the-art methods and their performance on these benchmarks. Finally, we\nexplore future research opportunities to improve multilingual reasoning in LMs,\nfocusing on enhancing their ability to handle diverse languages and complex\nreasoning tasks.\n","authors":["Akash Ghosh","Debayan Datta","Sriparna Saha","Chirag Agarwal"],"pdf_url":"https://arxiv.org/pdf/2502.09457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09447v1","updated":"2025-02-13T16:16:54Z","published":"2025-02-13T16:16:54Z","title":"Pixel-Level Reasoning Segmentation via Multi-turn Conversations","summary":"  Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.\n","authors":["Dexian Cai","Xiaocui Yang","Yongkang Liu","Daling Wang","Shi Feng","Yifei Zhang","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2502.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09419v1","updated":"2025-02-13T15:42:44Z","published":"2025-02-13T15:42:44Z","title":"On multi-token prediction for efficient LLM inference","summary":"  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n","authors":["Somesh Mehra","Javier Alonso Garcia","Lukas Mauch"],"pdf_url":"https://arxiv.org/pdf/2502.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09416v1","updated":"2025-02-13T15:39:07Z","published":"2025-02-13T15:39:07Z","title":"Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use\n  a Different Evaluation Process than Human?","summary":"  One of the goals of automatic evaluation metrics in grammatical error\ncorrection (GEC) is to rank GEC systems such that it matches human preferences.\nHowever, current automatic evaluations are based on procedures that diverge\nfrom human evaluation. Specifically, human evaluation derives rankings by\naggregating sentence-level relative evaluation results, e.g., pairwise\ncomparisons, using a rating algorithm, whereas automatic evaluation averages\nsentence-level absolute scores to obtain corpus-level scores, which are then\nsorted to determine rankings. In this study, we propose an aggregation method\nfor existing automatic evaluation metrics which aligns with human evaluation\nmethods to bridge this gap. We conducted experiments using various metrics,\nincluding edit-based metrics, $n$-gram based metrics, and sentence-level\nmetrics, and show that resolving the gap improves results for the most of\nmetrics on the SEEDA benchmark. We also found that even BERT-based metrics\nsometimes outperform the metrics of GPT-4. We publish our unified\nimplementation of the metrics and meta-evaluations.\n","authors":["Takumi Goto","Yusuke Sakai","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.09416v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2310.19347v4","updated":"2025-02-13T15:25:02Z","published":"2023-10-30T08:40:16Z","title":"Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization","summary":"  Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.\n","authors":["Huawen Feng","Yan Fan","Xiong Liu","Ting-En Lin","Zekun Yao","Yuchuan Wu","Fei Huang","Yongbin Li","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2310.19347v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02280v2","updated":"2025-02-13T15:21:43Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2412.17395v2","updated":"2025-02-13T15:11:24Z","published":"2024-12-23T08:47:42Z","title":"WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models","summary":"  Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs.\n","authors":["Huawen Feng","Pu Zhao","Qingfeng Sun","Can Xu","Fangkai Yang","Lu Wang","Qianli Ma","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09387v1","updated":"2025-02-13T15:04:53Z","published":"2025-02-13T15:04:53Z","title":"Truth Knows No Language: Evaluating Truthfulness Beyond English","summary":"  We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.\n","authors":["Blanca Calvo Figueras","Eneko Sagarzazu","Julen Etxaniz","Jeremy Barnes","Pablo Gamallo","Iria De Dios Flores","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2502.09387v1.pdf","comment":"13 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.15927v2","updated":"2025-02-13T14:55:26Z","published":"2024-11-24T17:32:20Z","title":"Generative Prompt Internalization","summary":"  Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Prompt Internalization (GenPI), a\nlightweight method that employs a joint training approach. GenPI not only\nreplicates the behavior of models with prompt inputs but also generates the\ncontent of the prompt along with reasons for why the model's behavior should\nchange accordingly. We demonstrate that our approach effectively internalizes\ncomplex prompts across various agent-based application scenarios. For effective\ntraining without interactions with the dedicated environments, we introduce a\ndata synthesis technique that autonomously collects conversational datasets by\nswapping the roles of the agent and environment. This method is especially\nuseful in scenarios where only a predefined prompt is available without a\ncorresponding training dataset. By internalizing complex prompts, Generative\nPrompt Internalization enables high performance and efficient inference without\nthe need for explicit prompts.\n","authors":["Haebin Shin","Lei Ji","Yeyun Gong","Sungdong Kim","Eunbi Choi","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2411.15927v2.pdf","comment":"NAACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna Pîslar","Michiel A. Bakker","Michael Henry Tessler","Raphael Köster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08514v2","updated":"2025-02-13T14:34:29Z","published":"2025-02-12T15:46:50Z","title":"Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation","summary":"  Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.\n","authors":["Mahnaz Koupaee","Jake W. Vincent","Saab Mansour","Igor Shalyminov","Han He","Hwanjun Song","Raphael Shu","Jianfeng He","Yi Nian","Amy Wing-mei Wong","Kyu J. Han","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2502.08514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10853v3","updated":"2025-02-13T14:13:41Z","published":"2024-07-15T16:04:44Z","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","summary":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.\n","authors":["Dylan Bouchard"],"pdf_url":"https://arxiv.org/pdf/2407.10853v3.pdf","comment":"LangFair repository: https://github.com/cvs-health/langfair"},{"id":"http://arxiv.org/abs/2411.05031v2","updated":"2025-02-13T14:02:53Z","published":"2024-11-06T09:52:29Z","title":"On-Device Emoji Classifier Trained with GPT-based Data Augmentation for\n  a Mobile Keyboard","summary":"  Emojis improve communication quality among smart-phone users that use mobile\nkeyboards to exchange text. To predict emojis for users based on input text, we\nshould consider the on-device low memory and time constraints, ensure that the\non-device emoji classifier covers a wide range of emoji classes even though the\nemoji dataset is typically imbalanced, and adapt the emoji classifier output to\nuser favorites. This paper proposes an on-device emoji classifier based on\nMobileBert with reasonable memory and latency requirements for SwiftKey. To\naccount for the data imbalance, we utilize the widely used GPT to generate one\nor more tags for each emoji class. For each emoji and corresponding tags, we\nmerge the original set with GPT-generated sentences and label them with this\nemoji without human intervention to alleviate the data imbalance. At inference\ntime, we interpolate the emoji output with the user history for emojis for\nbetter emoji classifications. Results show that the proposed on-device emoji\nclassifier deployed for SwiftKey increases the accuracy performance of emoji\nprediction particularly on rare emojis and emoji engagement.\n","authors":["Hossam Amer","Joe Osborne","Michael Zaki","Mohamed Afify"],"pdf_url":"https://arxiv.org/pdf/2411.05031v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.09331v1","updated":"2025-02-13T13:49:30Z","published":"2025-02-13T13:49:30Z","title":"Beyond English: The Impact of Prompt Translation Strategies across\n  Languages and Tasks in Multilingual LLMs","summary":"  Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings.\n","authors":["Itai Mondshine","Tzuf Paz-Argaman","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2502.09331v1.pdf","comment":"Accepted for NAACL findings 2025"},{"id":"http://arxiv.org/abs/2502.09316v1","updated":"2025-02-13T13:30:54Z","published":"2025-02-13T13:30:54Z","title":"A Judge-free LLM Open-ended Generation Benchmark Based on the\n  Distributional Hypothesis","summary":"  Evaluating the open-ended text generation of large language models (LLMs) is\nchallenging because of the lack of a clear ground truth and the high cost of\nhuman or LLM-based assessments. We propose a novel benchmark that evaluates\nLLMs using n-gram statistics and rules, without relying on human judgement or\nLLM-as-a-judge approaches. Using 50 question and reference answer sets, we\nintroduce three new metrics based on n-grams and rules: Fluency, Truthfulness,\nand Helpfulness. Our benchmark strongly correlates with GPT-4o-based\nevaluations while requiring significantly fewer computational resources,\ndemonstrating its effectiveness as a scalable alternative for assessing LLMs'\nopen-ended generation capabilities.\n","authors":["Kentaro Imajo","Masanori Hirano","Shuji Suzuki","Hiroaki Mikami"],"pdf_url":"https://arxiv.org/pdf/2502.09316v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.05497v2","updated":"2025-02-13T13:22:40Z","published":"2025-02-08T09:04:16Z","title":"DeepThink: Aligning Language Models with Domain-Specific User Intents","summary":"  Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability.\n","authors":["Yang Li","Mingxuan Luo","Yeyun Gong","Chen Lin","Jian Jiao","Yi Liu","Kaili Huang"],"pdf_url":"https://arxiv.org/pdf/2502.05497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09307v1","updated":"2025-02-13T13:19:33Z","published":"2025-02-13T13:19:33Z","title":"When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models","summary":"  Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions.\n","authors":["Samuel Joseph Amouyal","Aya Meltzer-Asscher","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2502.09307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15330v2","updated":"2025-02-13T13:06:00Z","published":"2024-06-21T17:42:52Z","title":"Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection","summary":"  Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.\n","authors":["Haoling Li","Xin Zhang","Xiao Liu","Yeyun Gong","Yifan Wang","Qi Chen","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.15330v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09284v1","updated":"2025-02-13T12:57:15Z","published":"2025-02-13T12:57:15Z","title":"SparQLe: Speech Queries to Text Translation Through LLMs","summary":"  With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.\n","authors":["Amirbek Djanibekov","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2502.09284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12851v3","updated":"2025-02-13T12:43:59Z","published":"2025-01-22T12:59:08Z","title":"ACEBench: Who Wins the Match Point in Tool Usage?","summary":"  Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.\n","authors":["Chen Chen","Xinlong Hao","Weiwen Liu","Xu Huang","Xingshan Zeng","Shuai Yu","Dexun Li","Shuai Wang","Weinan Gan","Yuefeng Huang","Wulong Liu","Xinzhi Wang","Defu Lian","Baoqun Yin","Yasheng Wang","Wu Liu"],"pdf_url":"https://arxiv.org/pdf/2501.12851v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17301v2","updated":"2025-02-13T12:25:54Z","published":"2024-11-26T10:48:55Z","title":"ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation","summary":"  Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.\n","authors":["Yunyi Liu","Yingshu Li","Zhanyu Wang","Xinyu Liang","Lingqiao Liu","Lei Wang","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.17301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00008v5","updated":"2025-02-13T12:10:33Z","published":"2023-03-27T18:00:01Z","title":"On the Creativity of Large Language Models","summary":"  Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2304.00008v5.pdf","comment":"Published in AI & SOCIETY at\n  https://link.springer.com/article/10.1007/s00146-024-02127-3"},{"id":"http://arxiv.org/abs/2502.09247v1","updated":"2025-02-13T12:03:36Z","published":"2025-02-13T12:03:36Z","title":"The Joint Entity-Relation Extraction Model Based on Span and Interactive\n  Fusion Representation for Chinese Medical Texts with Complex Semantics","summary":"  Joint entity-relation extraction is a critical task in transforming\nunstructured or semi-structured text into triplets, facilitating the\nconstruction of large-scale knowledge graphs, and supporting various downstream\napplications. Despite its importance, research on Chinese text, particularly\nwith complex semantics in specialized domains like medicine, remains limited.\nTo address this gap, we introduce the CH-DDI, a Chinese drug-drug interactions\ndataset designed to capture the intricacies of medical text. Leveraging the\nstrengths of attention mechanisms in capturing long-range dependencies, we\npropose the SEA module, which enhances the extraction of complex contextual\nsemantic information, thereby improving entity recognition and relation\nextraction. Additionally, to address the inefficiencies of existing methods in\nfacilitating information exchange between entity recognition and relation\nextraction, we present an interactive fusion representation module. This module\nemploys Cross Attention for bidirectional information exchange between the\ntasks and further refines feature extraction through BiLSTM. Experimental\nresults on both our CH-DDI dataset and public CoNLL04 dataset demonstrate that\nour model exhibits strong generalization capabilities. On the CH-DDI dataset,\nour model achieves an F1-score of 96.73% for entity recognition and 78.43% for\nrelation extraction. On the CoNLL04 dataset, it attains an entity recognition\nprecision of 89.54% and a relation extraction accuracy of 71.64%.\n","authors":["Danni Feng","Runzhi Li","Jing Wang","Siyu Yan","Lihong Ma","Yunli Xing"],"pdf_url":"https://arxiv.org/pdf/2502.09247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09245v1","updated":"2025-02-13T12:00:50Z","published":"2025-02-13T12:00:50Z","title":"You Do Not Fully Utilize Transformer's Representation Capacity","summary":"  In contrast to RNNs, which compress previous tokens into a single hidden\nstate, Transformers can attend to all previous tokens directly. However,\nstandard Transformers only use representations from the immediately preceding\nlayer. In this paper, we show that this design choice causes representation\ncollapse and leads to suboptimal performance. To address this issue, we\nintroduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that\npreserves the model's overall memory footprint while expanding its\nrepresentational capacity by allowing access to hidden states from earlier\nlayers. Through extensive experiments across various architectures and\ndifferent lookup mechanisms, we demonstrate consistent performance improvements\non a wide range of tasks. Moreover, our analysis of the learned representation\ndynamics and our exploration of depthwise circuits reveal how LIMe integrates\ninformation across layers, pointing to promising directions for future\nresearch.\n","authors":["Gleb Gerasimov","Yaroslav Aksenov","Nikita Balagansky","Viacheslav Sinii","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2502.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09237v1","updated":"2025-02-13T11:54:28Z","published":"2025-02-13T11:54:28Z","title":"Reliable Conversational Agents under ASP Control that Understand Natural\n  Language","summary":"  Efforts have been made to make machines converse like humans in the past few\ndecades. The recent techniques of Large Language Models (LLMs) make it possible\nto have human-like conversations with machines, but LLM's flaws of lacking\nunderstanding and reliability are well documented. We believe that the best way\nto eliminate this problem is to use LLMs only as parsers to translate text to\nknowledge and vice versa and carry out the conversation by reasoning over this\nknowledge using the answer set programming. I have been developing a framework\nbased on LLMs and ASP to realize reliable chatbots that \"understand\" human\nconversation. This framework has been used to develop task-specific chatbots as\nwell as socialbots. My future research is focused on making these chatbots\nscalable and trainable.\n","authors":["Yankai Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.09237v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09231v1","updated":"2025-02-13T11:52:55Z","published":"2025-02-13T11:52:55Z","title":"Answer Set Counting and its Applications","summary":"  We have focused on Answer Set Programming (ASP), more specifically, answer\nset counting, exploring both exact and approximate methodologies. We developed\nan exact ASP counter, sharpASP, which utilizes a compact encoding for\npropositional formulas, significantly enhancing efficiency compared to existing\nmethods that often struggle with inefficient encodings. Our evaluations\nindicate that sharpASP outperforms current ASP counters on several benchmarks.\nIn addition, we proposed an approximate ASP counter, named ApproxASP, a\nhashing-based counter integrating Gauss-Jordan elimination within the ASP\nsolver, clingo. As a practical application, we employed ApproxASP for network\nreliability estimation, demonstrating superior performance over both\ntraditional reliability estimators and #SAT-based methods.\n","authors":["Mohimenul Kabir"],"pdf_url":"https://arxiv.org/pdf/2502.09231v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09216v1","updated":"2025-02-13T11:49:17Z","published":"2025-02-13T11:49:17Z","title":"Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for\n  Autonomous Vehicles","summary":"  In this paper, we present a modular system for representing and reasoning\nwith legal aspects of traffic rules for autonomous vehicles. We focus on a\nsubset of the United Kingdom's Highway Code (HC) related to junctions. As human\ndrivers and automated vehicles (AVs) will interact on the roads, especially in\nurban environments, we claim that an accessible, unitary, high-level\ncomputational model should exist and be applicable to both users. Autonomous\nvehicles introduce a shift in liability that should not bring disadvantages or\nincreased burden on human drivers. We develop a system \"in silico\" of the\nmodel. The proposed system is built of three main components: a natural\nlanguage interface, using Logical English, which encodes the rules; an internal\nrepresentation of the rules in Prolog; and an multi-agent-based simulation\nenvironment, built in NetLogo. The three components interact: Logical English\nis translated into and out of Prolog (along with some support code); Prolog and\nNetLogo interface via predicates. Such a modular approach enables the different\ncomponents to carry different \"burdens\" in the overall system; it also allows\nswapping of modules. Given NetLogo, we can visualize the effect of the modeled\nrules as well as validate the system with a simple dynamic running scenario.\nDesignated agents monitor the behaviour of the vehicles for compliance and\nrecord potential violations where they occur. The information on potential\nviolations is then utilized by Validators, to determine whether the violation\nis punishable, differentiating between exceptions and cases.\n","authors":["Galileo Sartor","Adam Wyner","Giuseppe Contissa"],"pdf_url":"https://arxiv.org/pdf/2502.09216v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09213v1","updated":"2025-02-13T11:48:46Z","published":"2025-02-13T11:48:46Z","title":"Neuro-Symbolic Contrastive Learning for Cross-domain Inference","summary":"  Pre-trained language models (PLMs) have made significant advances in natural\nlanguage inference (NLI) tasks, however their sensitivity to textual\nperturbations and dependence on large datasets indicate an over-reliance on\nshallow heuristics. In contrast, inductive logic programming (ILP) excels at\ninferring logical relationships across diverse, sparse and limited datasets,\nbut its discrete nature requires the inputs to be precisely specified, which\nlimits their application. This paper proposes a bridge between the two\napproaches: neuro-symbolic contrastive learning. This allows for smooth and\ndifferentiable optimisation that improves logical accuracy across an otherwise\ndiscrete, noisy, and sparse topological space of logical functions. We show\nthat abstract logical relationships can be effectively embedded within a\nneuro-symbolic paradigm, by representing data as logic programs and sets of\nlogic rules. The embedding space captures highly varied textual information\nwith similar semantic logical relations, but can also separate similar textual\nrelations that have dissimilar logical relations. Experimental results\ndemonstrate that our approach significantly improves the inference capabilities\nof the models in terms of generalisation and reasoning.\n","authors":["Mingyue Liu","Ryo Ueda","Zhen Wan","Katsumi Inoue","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2502.09213v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09212v1","updated":"2025-02-13T11:48:31Z","published":"2025-02-13T11:48:31Z","title":"LP-LM: No Hallucinations in Question Answering with Logic Programming","summary":"  Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM.\n","authors":["Katherine Wu","Yanhong A. Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09212v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2411.16495v3","updated":"2025-02-13T11:46:25Z","published":"2024-11-25T15:35:51Z","title":"AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning","summary":"  Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets.\n","authors":["Amy Xin","Jinxin Liu","Zijun Yao","Zhicheng Lee","Shulin Cao","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2411.16495v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10053v2","updated":"2025-02-13T11:43:39Z","published":"2024-08-19T14:48:04Z","title":"Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory","summary":"  Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.\n","authors":["Haoran Li","Wei Fan","Yulin Chen","Jiayang Cheng","Tianshu Chu","Xuebing Zhou","Peizhao Hu","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2408.10053v2.pdf","comment":"To appear at NAACL 25"},{"id":"http://arxiv.org/abs/2412.15151v3","updated":"2025-02-13T11:37:45Z","published":"2024-12-19T18:28:41Z","title":"Language Models as Continuous Self-Evolving Data Engineers","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting a ceiling on the performance of LLMs.\nTo address this issue, we propose a novel paradigm named LANCE (LANguage models\nas Continuous self-Evolving data engineers) that enables LLMs to train\nthemselves by autonomously generating, cleaning, reviewing, and annotating data\nwith preference information. Our approach demonstrates that LLMs can serve as\ncontinuous self-evolving data engineers, significantly reducing the time and\ncost of the post-training data construction. Through iterative fine-tuning on\nQwen2 series models, we validate the effectiveness of LANCE across various\ntasks, showing that it can maintain high-quality data generation and\ncontinuously improve model performance. Across multiple benchmark dimensions,\nLANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for\nQwen2-7B-Instruct. This training paradigm with autonomous data construction not\nonly reduces the reliance on human experts or external models but also ensures\nthat the data aligns with human preferences, paving the way for the development\nof future superintelligent systems that can exceed human capabilities. Codes\nare available at: https://github.com/Control-derek/LANCE.\n","authors":["Peidong Wang","Ming Wang","Zhiming Ma","Xiaocui Yang","Shi Feng","Daling Wang","Yifei Zhang","Kaisong Song"],"pdf_url":"https://arxiv.org/pdf/2412.15151v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09192v1","updated":"2025-02-13T11:32:09Z","published":"2025-02-13T11:32:09Z","title":"Thinking beyond the anthropomorphic paradigm benefits LLM research","summary":"  Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof computer science research articles from the past decade and present\nempirical evidence of the prevalence and growth of anthropomorphic terminology\nin research on large language models (LLMs). This terminology reflects deeper\nanthropomorphic conceptualizations which shape how we think about and conduct\nLLM research. We argue these conceptualizations may be limiting, and that\nchallenging them opens up new pathways for understanding and improving LLMs\nbeyond human analogies. To illustrate this, we identify and analyze five core\nanthropomorphic assumptions shaping prominent methodologies across the LLM\ndevelopment lifecycle, from the assumption that models must use natural\nlanguage for reasoning tasks to the assumption that model capabilities should\nbe evaluated through human-centric benchmarks. For each assumption, we\ndemonstrate how non-anthropomorphic alternatives can open new directions for\nresearch and development.\n","authors":["Lujain Ibrahim","Myra Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08441v2","updated":"2025-02-13T11:30:41Z","published":"2024-07-11T12:30:19Z","title":"Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation","summary":"  Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence.\n","authors":["Riccardo Cantini","Giada Cosenza","Alessio Orsino","Domenico Talia"],"pdf_url":"https://arxiv.org/pdf/2407.08441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09188v1","updated":"2025-02-13T11:22:19Z","published":"2025-02-13T11:22:19Z","title":"Matina: A Large-Scale 73B Token Persian Text Corpus","summary":"  Text corpora are essential for training models used in tasks like\nsummarization, translation, and large language models (LLMs). While various\nefforts have been made to collect monolingual and multilingual datasets in many\nlanguages, Persian has often been underrepresented due to limited resources for\ndata collection and preprocessing. Existing Persian datasets are typically\nsmall and lack content diversity, consisting mainly of weblogs and news\narticles. This shortage of high-quality, varied data has slowed the development\nof NLP models and open-source LLMs for Persian. Since model performance depends\nheavily on the quality of training data, we address this gap by introducing the\nMatina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed\nand deduplicated to ensure high data quality. We further assess its\neffectiveness by training and evaluating transformer-based models on key NLP\ntasks. Both the dataset and preprocessing codes are publicly available,\nenabling researchers to build on and improve this resource for future Persian\nNLP advancements.\n","authors":["Sara Bourbour Hosseinbeigi","Fatemeh Taherinezhad","Heshaam Faili","Hamed Baghbani","Fatemeh Nadi","Mostafa Amiri"],"pdf_url":"https://arxiv.org/pdf/2502.09188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09183v1","updated":"2025-02-13T11:17:53Z","published":"2025-02-13T11:17:53Z","title":"RefineCoder: Iterative Improving of Large Language Models via Adaptive\n  Critique Refinement for Code Generation","summary":"  Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data.\n","authors":["Changzhi Zhou","Xinyu Zhang","Dandan Song","Xiancai Chen","Wanli Gu","Huipeng Ma","Yuhang Tian","Mengdi Zhang","Linmei Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09183v1.pdf","comment":"work in process"},{"id":"http://arxiv.org/abs/2502.09175v1","updated":"2025-02-13T11:05:55Z","published":"2025-02-13T11:05:55Z","title":"FLAME: Flexible LLM-Assisted Moderation Engine","summary":"  The rapid advancement of Large Language Models (LLMs) has introduced\nsignificant challenges in moderating user-model interactions. While LLMs\ndemonstrate remarkable capabilities, they remain vulnerable to adversarial\nattacks, particularly ``jailbreaking'' techniques that bypass content safety\nmeasures. Current content moderation systems, which primarily rely on input\nprompt filtering, have proven insufficient, with techniques like Best-of-N\n(BoN) jailbreaking achieving success rates of 80% or more against popular LLMs.\nIn this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a\nnew approach that shifts the focus from input filtering to output moderation.\nUnlike traditional circuit-breaking methods that analyze user queries, FLAME\nevaluates model responses, offering several key advantages: (1) computational\nefficiency in both training and inference, (2) enhanced resistance to BoN\njailbreaking attacks, and (3) flexibility in defining and updating safety\ncriteria through customizable topic filtering. Our experiments demonstrate that\nFLAME significantly outperforms current moderation systems. For example, FLAME\nreduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9,\nwhile maintaining low computational overhead. We provide comprehensive\nevaluation on various LLMs and analyze the engine's efficiency against the\nstate-of-the-art jailbreaking. This work contributes to the development of more\nrobust and adaptable content moderation systems for LLMs.\n","authors":["Ivan Bakulin","Ilia Kopanichuk","Iaroslav Bespalov","Nikita Radchenko","Vladimir Shaposhnikov","Dmitry Dylov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2502.09175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09168v1","updated":"2025-02-13T10:51:40Z","published":"2025-02-13T10:51:40Z","title":"Musical Heritage Historical Entity Linking","summary":"  Linking named entities occurring in text to their corresponding entity in a\nKnowledge Base (KB) is challenging, especially when dealing with historical\ntexts. In this work, we introduce Musical Heritage named Entities Recognition,\nClassification and Linking (MHERCL), a novel benchmark consisting of manually\nannotated sentences extrapolated from historical periodicals of the music\ndomain. MHERCL contains named entities under-represented or absent in the most\nfamous KBs. We experiment with several State-of-the-Art models on the Entity\nLinking (EL) task and show that MHERCL is a challenging dataset for all of\nthem. We propose a novel unsupervised EL model and a method to extend\nsupervised entity linkers by using Knowledge Graphs (KGs) to tackle the main\ndifficulties posed by historical documents. Our experiments reveal that relying\non unsupervised techniques and improving models with logical constraints based\non KGs and heuristics to predict NIL entities (entities not represented in the\nKB of reference) results in better EL performance on historical documents.\n","authors":["Arianna Graciotti","Nicolas Lazzari","Valentina Presutti","Rocco Tripodi"],"pdf_url":"https://arxiv.org/pdf/2502.09168v1.pdf","comment":"To appear in Artificial Intelligence Review Journal"},{"id":"http://arxiv.org/abs/2308.13916v5","updated":"2025-02-13T10:45:15Z","published":"2023-08-26T16:51:17Z","title":"Exploring Large Language Models for Knowledge Graph Completion","summary":"  Knowledge graphs play a vital role in numerous artificial intelligence tasks,\nyet they frequently face the issue of incompleteness. In this study, we explore\nutilizing Large Language Models (LLM) for knowledge graph completion. We\nconsider triples in knowledge graphs as text sequences and introduce an\ninnovative framework called Knowledge Graph LLM (KG-LLM) to model these\ntriples. Our technique employs entity and relation descriptions of a triple as\nprompts and utilizes the response for predictions. Experiments on various\nbenchmark knowledge graphs demonstrate that our method attains state-of-the-art\nperformance in tasks such as triple classification and relation prediction. We\nalso find that fine-tuning relatively smaller models (e.g., LLaMA-7B,\nChatGLM-6B) outperforms recent ChatGPT and GPT-4.\n","authors":["Liang Yao","Jiazhen Peng","Chengsheng Mao","Yuan Luo"],"pdf_url":"https://arxiv.org/pdf/2308.13916v5.pdf","comment":"Accepted by the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2502.09156v1","updated":"2025-02-13T10:36:18Z","published":"2025-02-13T10:36:18Z","title":"Improving TCM Question Answering through Tree-Organized Self-Reflective\n  Retrieval with LLMs","summary":"  Objectives: Large language models (LLMs) can harness medical knowledge for\nintelligent question answering (Q&A), promising support for auxiliary diagnosis\nand medical talent cultivation. However, there is a deficiency of highly\nefficient retrieval-augmented generation (RAG) frameworks within the domain of\nTraditional Chinese Medicine (TCM). Our purpose is to observe the effect of the\nTree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A\ntasks.\n  Materials and Methods: We introduce the novel approach of knowledge\norganization, constructing a tree structure knowledge base with hierarchy. At\ninference time, our self-reflection framework retrieves from this knowledge\nbase, integrating information across chapters. Questions from the TCM Medical\nLicensing Examination (MLE) and the college Classics Course Exam (CCE) were\nrandomly selected as benchmark datasets.\n  Results: By coupling with GPT-4, the framework can improve the best\nperformance on the TCM MLE benchmark by 19.85% in absolute accuracy, and\nimprove recall accuracy from 27% to 38% on CCE datasets. In manual evaluation,\nthe framework improves a total of 18.52 points across dimensions of safety,\nconsistency, explainability, compliance, and coherence.\n  Conclusion: The TOSRR framework can effectively improve LLM's capability in\nQ&A tasks of TCM.\n","authors":["Chang Liu","Ying Chang","Jianmin Li","Yiqian Qu","Yu Li","Lingyong Cao","Shuyuan Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09128v1","updated":"2025-02-13T10:05:44Z","published":"2025-02-13T10:05:44Z","title":"A Novel Dialect-Aware Framework for the Classification of Arabic\n  Dialects and Emotions","summary":"  Arabic is one of the oldest languages still in use today. As a result,\nseveral Arabic-speaking regions have developed dialects that are unique to\nthem. Dialect and emotion recognition have various uses in Arabic text\nanalysis, such as determining an online customer's origin based on their\ncomments. Furthermore, intelligent chatbots that are aware of a user's emotions\ncan respond appropriately to the user. Current research in emotion detection in\nthe Arabic language lacks awareness of how emotions are exhibited in different\ndialects, which motivates the work found in this study. This research addresses\nthe problems of dialect and emotion classification in Arabic. Specifically,\nthis is achieved by building a novel framework that can identify and predict\nArabic dialects and emotions from a given text. The framework consists of three\nmodules: A text-preprocessing module, a classification module, and a clustering\nmodule with the novel capability of building new dialect-aware emotion\nlexicons. The proposed framework generated a new emotional lexicon for\ndifferent dialects. It achieved an accuracy of 88.9% in classifying Arabic\ndialects, which outperforms the state-of-the-art results by 6.45 percentage\npoints. Furthermore, the framework achieved 89.1-79% accuracy in detecting\nemotions in the Egyptian and Gulf dialects, respectively.\n","authors":["Nasser A Alsadhan"],"pdf_url":"https://arxiv.org/pdf/2502.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09120v1","updated":"2025-02-13T09:55:48Z","published":"2025-02-13T09:55:48Z","title":"The influence of visual and linguistic cues on ignorance inference in\n  Vision-Language Models (VLMs)","summary":"  This study explored how Vision-Language Models (VLMs) process ignorance\nimplicatures with visual and linguistic cues. Particularly, we focused on the\neffects of contexts (precise and approximate contexts) and modifier types (bare\nnumerals, superlative, and comparative modifiers), which were considered\npragmatic and semantic factors respectively. Methodologically, we conducted a\ntruth-value judgment task in visually grounded settings using GPT-4o and Gemini\n1.5 Pro. The results indicate that while both models exhibited sensitivity to\nlinguistic cues (modifier), they failed to process ignorance implicatures with\nvisual cues (context) as humans do. Specifically, the influence of context was\nweaker and inconsistent across models, indicating challenges in pragmatic\nreasoning for VLMs. On the other hand, superlative modifiers were more strongly\nassociated with ignorance implicatures as compared to comparative modifiers,\nsupporting the semantic view. These findings highlight the need for further\nadvancements in VLMs to process language-vision information in a\ncontext-dependent way to achieve human-like pragmatic inference.\n","authors":["Ye-eun Cho","Yunho Maeng"],"pdf_url":"https://arxiv.org/pdf/2502.09120v1.pdf","comment":"13 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09100v1","updated":"2025-02-13T09:19:14Z","published":"2025-02-13T09:19:14Z","title":"Logical Reasoning in Large Language Models: A Survey","summary":"  With the emergence of advanced reasoning models like OpenAI o3 and\nDeepSeek-R1, large language models (LLMs) have demonstrated remarkable\nreasoning capabilities. However, their ability to perform rigorous logical\nreasoning remains an open question. This survey synthesizes recent advancements\nin logical reasoning within LLMs, a critical area of AI research. It outlines\nthe scope of logical reasoning in LLMs, its theoretical foundations, and the\nbenchmarks used to evaluate reasoning proficiency. We analyze existing\ncapabilities across different reasoning paradigms - deductive, inductive,\nabductive, and analogical - and assess strategies to enhance reasoning\nperformance, including data-centric tuning, reinforcement learning, decoding\nstrategies, and neuro-symbolic approaches. The review concludes with future\ndirections, emphasizing the need for further exploration to strengthen logical\nreasoning in AI systems.\n","authors":["Hanmeng Liu","Zhizhang Fu","Mengru Ding","Ruoxi Ning","Chaoli Zhang","Xiaozhang Liu","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09097v1","updated":"2025-02-13T09:13:23Z","published":"2025-02-13T09:13:23Z","title":"A Hybrid Transformer Model for Fake News Detection: Leveraging Bayesian\n  Optimization and Bidirectional Recurrent Unit","summary":"  In this paper, we propose an optimized Transformer model that integrates\nBayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and\napply it to fake news classification for the first time. First, we employ the\nTF-IDF method to extract features from news texts and transform them into\nnumeric representations to facilitate subsequent machine learning tasks. Two\nsets of experiments are then conducted for fake news detection and\nclassification: one using a Transformer model optimized only with BiGRU, and\nthe other incorporating Bayesian algorithms into the BiGRU-based Transformer.\nExperimental results show that the BiGRU-optimized Transformer achieves 100%\naccuracy on the training set and 99.67% on the test set, while the addition of\nthe Bayesian algorithm maintains 100% accuracy on the training set and slightly\nimproves test-set accuracy to 99.73%. This indicates that the Bayesian\nalgorithm boosts model accuracy by 0.06%, further enhancing the detection\ncapability for fake news. Moreover, the proposed algorithm converges rapidly at\naround the 10th training epoch with accuracy nearing 100%, demonstrating both\nits effectiveness and its fast classification ability. Overall, the optimized\nTransformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits\nexcellent continuous learning and detection performance, offering a robust\ntechnical means to combat the spread of fake news in the current era of\ninformation overload.\n","authors":["Tianyi Huang","Zeqiu Xu","Peiyang Yu","Jingyuan Yi","Xiaochuan Xu"],"pdf_url":"https://arxiv.org/pdf/2502.09097v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13166v4","updated":"2025-02-13T09:08:42Z","published":"2024-10-17T02:47:10Z","title":"An Evolved Universal Transformer Memory","summary":"  Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.\n","authors":["Edoardo Cetin","Qi Sun","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.13166v4.pdf","comment":"Published at ICLR 2025. Source code available at\n  https://github.com/SakanaAI/evo-memory"},{"id":"http://arxiv.org/abs/2405.14445v2","updated":"2025-02-13T09:07:10Z","published":"2024-05-23T11:24:23Z","title":"Exploring the use of a Large Language Model for data extraction in\n  systematic reviews: a rapid feasibility study","summary":"  This paper describes a rapid feasibility study of using GPT-4, a large\nlanguage model (LLM), to (semi)automate data extraction in systematic reviews.\nDespite the recent surge of interest in LLMs there is still a lack of\nunderstanding of how to design LLM-based automation tools and how to robustly\nevaluate their performance. During the 2023 Evidence Synthesis Hackathon we\nconducted two feasibility studies. Firstly, to automatically extract study\ncharacteristics from human clinical, animal, and social science domain studies.\nWe used two studies from each category for prompt-development; and ten for\nevaluation. Secondly, we used the LLM to predict Participants, Interventions,\nControls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP\ndataset. Overall, results indicated an accuracy of around 80%, with some\nvariability between domains (82% for human clinical, 80% for animal, and 72%\nfor studies of human social sciences). Causal inference methods and study\ndesign were the data extraction items with the most errors. In the PICO study,\nparticipants and intervention/control showed high accuracy (>80%), outcomes\nwere more challenging. Evaluation was done manually; scoring methods such as\nBLEU and ROUGE showed limited value. We observed variability in the LLMs\npredictions and changes in response quality. This paper presents a template for\nfuture evaluations of LLMs in the context of data extraction for systematic\nreview automation. Our results show that there might be value in using LLMs,\nfor example as second or third reviewers. However, caution is advised when\nintegrating models such as GPT-4 into tools. Further research on stability and\nreliability in practical settings is warranted for each type of data that is\nprocessed by the LLM.\n","authors":["Lena Schmidt","Kaitlyn Hair","Sergio Graziosi","Fiona Campbell","Claudia Kapp","Alireza Khanteymoori","Dawn Craig","Mark Engelbert","James Thomas"],"pdf_url":"https://arxiv.org/pdf/2405.14445v2.pdf","comment":"Conference proceedings, peer-reviewed and presented at the 3rd\n  Workshop on Augmented Intelligence for Technology-Assisted Reviews Systems,\n  Glasgow, 2024"},{"id":"http://arxiv.org/abs/2502.09086v1","updated":"2025-02-13T09:00:32Z","published":"2025-02-13T09:00:32Z","title":"A Hybrid Model for Few-Shot Text Classification Using Transfer and\n  Meta-Learning","summary":"  With the continuous development of natural language processing (NLP)\ntechnology, text classification tasks have been widely used in multiple\napplication fields. However, obtaining labeled data is often expensive and\ndifficult, especially in few-shot learning scenarios. To solve this problem,\nthis paper proposes a few-shot text classification model based on transfer\nlearning and meta-learning. The model uses the knowledge of the pre-trained\nmodel for transfer and optimizes the model's rapid adaptability in few-sample\ntasks through a meta-learning mechanism. Through a series of comparative\nexperiments and ablation experiments, we verified the effectiveness of the\nproposed method. The experimental results show that under the conditions of few\nsamples and medium samples, the model based on transfer learning and\nmeta-learning significantly outperforms traditional machine learning and deep\nlearning methods. In addition, ablation experiments further analyzed the\ncontribution of each component to the model performance and confirmed the key\nrole of transfer learning and meta-learning in improving model accuracy.\nFinally, this paper discusses future research directions and looks forward to\nthe potential of this method in practical applications.\n","authors":["Jia Gao","Shuangquan Lyu","Guiran Liu","Binrong Zhu","Hongye Zheng","Xiaoxuan Liao"],"pdf_url":"https://arxiv.org/pdf/2502.09086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09083v1","updated":"2025-02-13T08:56:25Z","published":"2025-02-13T08:56:25Z","title":"Show Me the Work: Fact-Checkers' Requirements for Explainable Automated\n  Fact-Checking","summary":"  The pervasiveness of large language models and generative AI in online media\nhas amplified the need for effective automated fact-checking to assist\nfact-checkers in tackling the increasing volume and sophistication of\nmisinformation. The complex nature of fact-checking demands that automated\nfact-checking systems provide explanations that enable fact-checkers to\nscrutinise their outputs. However, it is unclear how these explanations should\nalign with the decision-making and reasoning processes of fact-checkers to be\neffectively integrated into their workflows. Through semi-structured interviews\nwith fact-checking professionals, we bridge this gap by: (i) providing an\naccount of how fact-checkers assess evidence, make decisions, and explain their\nprocesses; (ii) examining how fact-checkers use automated tools in practice;\nand (iii) identifying fact-checker explanation requirements for automated\nfact-checking tools. The findings show unmet explanation needs and identify\nimportant criteria for replicable fact-checking explanations that trace the\nmodel's reasoning path, reference specific evidence, and highlight uncertainty\nand information gaps.\n","authors":["Greta Warren","Irina Shklovski","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2502.09083v1.pdf","comment":"Conditionally accepted to CHI'25"},{"id":"http://arxiv.org/abs/2502.09082v1","updated":"2025-02-13T08:55:24Z","published":"2025-02-13T08:55:24Z","title":"CoSER: Coordinating LLM-Based Persona Simulation of Established Roles","summary":"  Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively.\n","authors":["Xintao Wang","Heng Wang","Yifei Zhang","Xinfeng Yuan","Rui Xu","Jen-tse Huang","Siyu Yuan","Haoran Guo","Jiangjie Chen","Wei Wang","Yanghua Xiao","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.09082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09073v1","updated":"2025-02-13T08:42:29Z","published":"2025-02-13T08:42:29Z","title":"Enhancing RAG with Active Learning on Conversation Records: Reject\n  Incapables and Answer Capables","summary":"  Retrieval-augmented generation (RAG) is a key technique for leveraging\nexternal knowledge and reducing hallucinations in large language models (LLMs).\nHowever, RAG still struggles to fully prevent hallucinated responses. To\naddress this, it is essential to identify samples prone to hallucination or\nguide LLMs toward correct responses, which experts then annotate to develop\nhigh-quality datasets for refining LLMs. However, the growing scarcity of such\ndatasets makes their creation challenging. This paper proposes using the vast\namount of conversations from widespread LLM usage to build these datasets,\ntraining LLMs to avoid hallucination-prone questions while accurately\nresponding to manageable ones. Given the impracticality of expert-annotating\nall conversation records, the paper introduces AL4RAG, which uses active\nlearning to select the most suitable conversation samples for annotation,\noptimizing performance within an annotation budget. Additionally, recognizing\nthat traditional active learning methods are not fully compatible with RAG due\nto unsuitable distance metrics, we develop a novel sample distance measurement\nfor RAG active learning. Extensive experiments show that our method\nconsistently outperforms baselines across multiple metrics.\n","authors":["Xuzhao Geng","Haozhao Wang","Jun Wang","Wei Liu","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.09073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13835v2","updated":"2025-02-13T08:13:52Z","published":"2024-01-24T22:21:04Z","title":"What Large Language Models Know and What People Think They Know","summary":"  As artificial intelligence (AI) systems, particularly large language models\n(LLMs), become increasingly integrated into decision-making processes, the\nability to trust their outputs is crucial. To earn human trust, LLMs must be\nwell calibrated such that they can accurately assess and communicate the\nlikelihood of their predictions being correct. Whereas recent work has focused\non LLMs' internal confidence, less is understood about how effectively they\nconvey uncertainty to users. Here we explore the calibration gap, which refers\nto the difference between human confidence in LLM-generated answers and the\nmodels' actual confidence, and the discrimination gap, which reflects how well\nhumans and models can distinguish between correct and incorrect answers. Our\nexperiments with multiple-choice and short-answer questions reveal that users\ntend to overestimate the accuracy of LLM responses when provided with default\nexplanations. Moreover, longer explanations increased user confidence, even\nwhen the extra length did not improve answer accuracy. By adjusting LLM\nexplanations to better reflect the models' internal confidence, both the\ncalibration gap and the discrimination gap narrowed, significantly improving\nuser perception of LLM accuracy. These findings underscore the importance of\naccurate uncertainty communication and highlight the effect of explanation\nlength in influencing user trust in AI-assisted decision-making environments.\nCode and Data can be found at https://osf.io/y7pr6/ . Journal publication can\nbe found on Nature Machine Intelligence at\nhttps://www.nature.com/articles/s42256-024-00976-7 .\n","authors":["Mark Steyvers","Heliodoro Tejeda","Aakriti Kumar","Catarina Belem","Sheer Karny","Xinyue Hu","Lukas Mayer","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2401.13835v2.pdf","comment":"27 pages, 10 figures For the journal publication on Nature Machine\n  Intelligence see https://www.nature.com/articles/s42256-024-00976-7 For the\n  data and code see https://osf.io/y7pr6/"},{"id":"http://arxiv.org/abs/2401.11817v2","updated":"2025-02-13T08:11:25Z","published":"2024-01-22T10:26:14Z","title":"Hallucination is Inevitable: An Innate Limitation of Large Language\n  Models","summary":"  Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all the computable functions and will\ntherefore inevitably hallucinate if used as general problem solvers. Since the\nformal world is a part of the real world which is much more complicated,\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\nworld LLMs constrained by provable time complexity, we describe the\nhallucination-prone tasks and empirically validate our claims. Finally, using\nthe formal world framework, we discuss the possible mechanisms and efficacies\nof existing hallucination mitigators as well as the practical implications on\nthe safe deployment of LLMs.\n","authors":["Ziwei Xu","Sanjay Jain","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2401.11817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09056v1","updated":"2025-02-13T08:10:45Z","published":"2025-02-13T08:10:45Z","title":"An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in\n  One Day via Model Merging","summary":"  This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.\n","authors":["Kunat Pipatanakul","Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2502.09056v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2405.19778v4","updated":"2025-02-13T08:03:52Z","published":"2024-05-30T07:44:16Z","title":"CharacterGPT: A Persona Reconstruction Framework for Role-Playing Agents","summary":"  With the recent introduction of Assistants API, it is expected that\ndocument-based language models will be actively used in various domains,\nespecially Role-playing. However, a key challenge lies in utilizing\nprotagonist's persona: Assistants API often fails to achieve with its search\nbecause the information extraction part is different each time and it often\nomits important information such as protagonist's backstory or relationships.\nIt is hard to maintain a consistent persona simply by using the persona\ndocument as input to the Assistants API. To address the challenge of achieving\nstable persona consistency, we propose CharacterGPT, a novel persona\nreconstruction framework to alleviate the shortcomings of the Assistants API.\nOur method involves Character Persona Training (CPT), an effective persona\nrebuilding process that updates the character persona by extracting the\ncharacter's traits from given summary of the novel for each character as if the\nstory in a novel progresses. In our experiments, we ask each character to take\nthe Big Five Inventory personality test in various settings and analyze the\nresults. To assess whether it can think outside the box, we let each character\ngenerate short novels. Extensive experiments and human evaluation demonstrate\nthat CharacterGPT presents new possibilities for role-playing agent research.\nCode and results are available at: https://github.com/Jeiyoon/charactergpt\n","authors":["Jeiyoon Park","Chanjun Park","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2405.19778v4.pdf","comment":"NAACL 2025 Industry Track (Oral)"},{"id":"http://arxiv.org/abs/2502.09042v1","updated":"2025-02-13T07:55:54Z","published":"2025-02-13T07:55:54Z","title":"Typhoon T1: An Open Thai Reasoning Model","summary":"  This paper introduces Typhoon T1, an open effort to develop an open Thai\nreasoning model. A reasoning model is a relatively new type of generative model\nbuilt on top of large language models (LLMs). A reasoning model generates a\nlong chain of thought before arriving at a final answer, an approach found to\nimprove performance on complex tasks. However, details on developing such a\nmodel are limited, especially for reasoning models that can generate traces in\na low-resource language. Typhoon T1 presents an open effort that dives into the\ndetails of developing a reasoning model in a more cost-effective way by\nleveraging supervised fine-tuning using open datasets, instead of reinforcement\nlearning. This paper shares the details about synthetic data generation and\ntraining, as well as our dataset and model weights. Additionally, we provide\ninsights gained from developing a reasoning model that generalizes across\ndomains and is capable of generating reasoning traces in a low-resource\nlanguage, using Thai as an example. We hope this open effort provides a\nfoundation for further research in this field.\n","authors":["Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai","Kunat Pipatanakul"],"pdf_url":"https://arxiv.org/pdf/2502.09042v1.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.00511v2","updated":"2025-02-13T07:35:08Z","published":"2025-02-01T18:09:49Z","title":"Bridging Internal Probability and Self-Consistency for Effective and\n  Efficient LLM Reasoning","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities. However, single-shot inference often yields\nunreliable results for complex reasoning tasks, leading researchers to explore\nmultiple reasoning paths through methods such as perplexity and\nself-consistency. In this paper, we present the first theoretical error\ndecomposition analysis of these techniques, breaking down their error into\nestimation error and model error. Our analysis reveals a fundamental trade-off:\nperplexity methods suffer from substantial model error due to the absence of a\nproper consistency function, while self-consistency exhibits high estimation\nerror due to a slow error convergence rate. To overcome these limitations, we\npropose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines\nPerplexity Consistency, which seamlessly integrates LLM perplexity with\nself-consistency, and Reasoning Pruning, which eliminates low-probability\nreasoning paths to effectively prevent the degeneration of estimation error\nreduction. Theoretical analysis demonstrates that RPC not only accelerates the\nconvergence rate of estimation error to an exponential level but also holds\nstrong potential for further reducing model error. Extensive empirical\nevaluations on seven benchmark datasets confirm that RPC can significantly\nimprove reasoning performance, sample efficiency, and confidence reliability.\n","authors":["Zhi Zhou","Tan Yuhao","Zenan Li","Yuan Yao","Lan-Zhe Guo","Xiaoxing Ma","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2502.00511v2.pdf","comment":"Preliminary work"},{"id":"http://arxiv.org/abs/2502.06635v2","updated":"2025-02-13T07:31:55Z","published":"2025-02-10T16:31:37Z","title":"Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM","summary":"  Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.\n","authors":["Qingshui Gu","Shu Li","Tianyu Zheng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07367v2","updated":"2025-02-13T07:31:13Z","published":"2024-12-10T10:06:46Z","title":"My Words Imply Your Opinion: Reader Agent-Based Propagation Enhancement\n  for Personalized Implicit Emotion Analysis","summary":"  The subtlety of emotional expressions makes implicit emotion analysis (IEA)\nparticularly sensitive to user-specific characteristics. Current studies\npersonalize emotion analysis by focusing on the author but neglect the impact\nof the intended reader on implicit emotional feedback. In this paper, we\nintroduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses\nsubjective variability by incorporating reader feedback. In particular, (1) we\ncreate reader agents based on large language models to simulate reader\nfeedback, overcoming the issue of ``spiral of silence effect'' and data\nincompleteness of real reader reaction. (2) We develop a role-aware multi-view\ngraph learning to model the emotion interactive propagation process in\nscenarios with sparse reader information. (3) We construct two new PIEA\ndatasets covering English and Chinese social media with detailed user metadata,\naddressing the text-centric limitation of existing datasets. Extensive\nexperiments show that RAPPIE significantly outperforms state-of-the-art\nbaselines, demonstrating the value of incorporating reader feedback in PIEA.\n","authors":["Jian Liao","Yu Feng","Yujin Zheng","Jun Zhao","Suge Wang","Jianxing Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.07367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02364v2","updated":"2025-02-13T07:29:46Z","published":"2024-02-04T06:23:05Z","title":"Loss Landscape Degeneracy Drives Stagewise Development in Transformers","summary":"  Deep learning involves navigating a high-dimensional loss landscape over the\nneural network parameter space. Over the course of training, complex\ncomputational structures form and re-form inside the neural network, leading to\nshifts in input/output behavior. It is a priority for the science of deep\nlearning to uncover principles governing the development of neural network\nstructure and behavior. Drawing on the framework of singular learning theory,\nwe propose that model development is deeply linked to degeneracy in the local\ngeometry of the loss landscape. We investigate this link by monitoring loss\nlandscape degeneracy throughout training, as quantified by the local learning\ncoefficient, for a transformer language model and an in-context linear\nregression transformer. We show that training can be divided into distinct\nperiods of change in loss landscape degeneracy, and that these changes in\ndegeneracy coincide with significant changes in the internal computational\nstructure and the input/output behavior of the transformers. This finding\nunderscores the potential of a degeneracy-based perspective for understanding\nmodern deep learning.\n","authors":["Jesse Hoogland","George Wang","Matthew Farrugia-Roberts","Liam Carroll","Susan Wei","Daniel Murfet"],"pdf_url":"https://arxiv.org/pdf/2402.02364v2.pdf","comment":"Material on essential dynamics from v1 of this preprint has been\n  removed from v2 and developed in arXiv:2501.17745"},{"id":"http://arxiv.org/abs/2502.06572v2","updated":"2025-02-13T07:24:46Z","published":"2025-02-10T15:40:35Z","title":"LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM","summary":"  Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://github.com/LAMDASZ-ML/Knowledge-Guide-Data-Generation .\n","authors":["Zhi Zhou","Kun-Yang Yu","Shi-Yu Tian","Xiao-Wen Yang","Jiang-Xin Shi","Pengxiao Song","Yi-Xuan Jin","Lan-Zhe Guo","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2502.06572v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09017v1","updated":"2025-02-13T07:11:01Z","published":"2025-02-13T07:11:01Z","title":"Diversity Enhances an LLM's Performance in RAG and Long-context Task","summary":"  The rapid advancements in large language models (LLMs) have highlighted the\nchallenge of context window limitations, primarily due to the quadratic time\ncomplexity of the self-attention mechanism (\\(O(N^2)\\), where \\(N\\) denotes the\ncontext window length). This constraint impacts tasks such as\nretrieval-augmented generation (RAG) in question answering (Q\\&A) and long\ncontext summarization. A common approach involves selecting content with the\nhighest similarity to the query; however, this often leads to redundancy and\nthe exclusion of diverse yet relevant information. Building on principles from\nMaximal Marginal Relevance (MMR) and Farthest Point Sampling (FPS), we\nintegrate diversity into the content selection process. Our findings reveal\nthat incorporating diversity substantially increases the recall of selecting\nrelevant sentences or chunks before LLM-based Q\\&A and summarization. These\nresults highlight the importance of maintaining diversity in future LLM\napplications to further improve summarization and Q\\&A outcomes.\n","authors":["Zhchao Wang","Bin Bi","Yanqi Luo","Sitaram Asur","Claire Na Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09004v1","updated":"2025-02-13T06:49:14Z","published":"2025-02-13T06:49:14Z","title":"Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content\n  in Mainstream US News Media through the Lens of Hope Speech","summary":"  This paper makes three contributions. First, via a substantial corpus of\n1,419,047 comments posted on 3,161 YouTube news videos of major US cable news\noutlets, we analyze how users engage with LGBTQ+ news content. Our analyses\nfocus both on positive and negative content. In particular, we construct a\nfine-grained hope speech classifier that detects positive (hope speech),\nnegative, neutral, and irrelevant content. Second, in consultation with a\npublic health expert specializing on LGBTQ+ health, we conduct an annotation\nstudy with a balanced and diverse political representation and release a\ndataset of 3,750 instances with fine-grained labels and detailed annotator\ndemographic information. Finally, beyond providing a vital resource for the\nLGBTQ+ community, our annotation study and subsequent in-the-wild assessments\nreveal (1) strong association between rater political beliefs and how they rate\ncontent relevant to a marginalized community; (2) models trained on individual\npolitical beliefs exhibit considerable in-the-wild disagreement; and (3)\nzero-shot large language models (LLMs) align more with liberal raters.\n","authors":["Jonathan Pofcher","Christopher M. Homan","Randall Sell","Ashiqur R. KhudaBukhsh"],"pdf_url":"https://arxiv.org/pdf/2502.09004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13258v2","updated":"2025-02-13T06:34:43Z","published":"2024-10-17T06:30:55Z","title":"How Does Knowledge Selection Help Retrieval Augmented Generation?","summary":"  Retrieval-augmented generation (RAG) is a powerful method for enhancing\nnatural language generation by integrating external knowledge into a model's\noutput. While prior work has demonstrated the importance of improving knowledge\nretrieval for boosting generation quality, the role of knowledge selection\nremains less clear. In this paper, we perform a comprehensive analysis of how\nknowledge selection influences downstream generation performance in RAG\nsystems. By simulating different retrieval and selection conditions through a\ncontrolled mixture of gold and distractor knowledge, we assess the impact of\nthese factors on generation outcomes. Our findings indicate that the downstream\ngenerator model's capability, as well as the complexity of the task and\ndataset, significantly influence the impact of knowledge selection on the\noverall RAG system performance. In typical scenarios, improving the knowledge\nrecall score is key to enhancing generation outcomes, with the knowledge\nselector providing a limited additional benefit when a strong generator model\nis used on clear, well-defined tasks. For weaker generator models or more\nambiguous tasks and datasets, the knowledge F1 score becomes a critical factor,\nand the knowledge selector plays a more prominent role in improving overall\nperformance.\n","authors":["Xiangci Li","Jessica Ouyang"],"pdf_url":"https://arxiv.org/pdf/2410.13258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06876v2","updated":"2025-02-13T06:28:33Z","published":"2025-02-08T11:56:58Z","title":"Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and\n  Harmlessness of Large Language Model via Model Merging","summary":"  Achieving balanced alignment of large language models (LLMs) in terms of\nHelpfulness, Honesty, and Harmlessness (3H optimization) constitutes a\ncornerstone of responsible AI, with existing methods like data mixture\nstrategies facing limitations including reliance on expert knowledge and\nconflicting optimization signals. While model merging offers a promising\nalternative by integrating specialized models, its potential for 3H\noptimization remains underexplored. This paper establishes the first\ncomprehensive benchmark for model merging in 3H-aligned LLMs, systematically\nevaluating 15 methods (12 training-free merging and 3 data mixture techniques)\nacross 10 datasets associated with 5 annotation dimensions, 2 LLM families, and\n2 training paradigms. Our analysis reveals three pivotal insights: (i)\npreviously overlooked collaborative/conflicting relationships among 3H\ndimensions, (ii) the consistent superiority of model merging over data mixture\napproaches in balancing alignment trade-offs, and (iii) the critical role of\nparameter-level conflict resolution through redundant component pruning and\noutlier mitigation. Building on these findings, we propose R-TSVM, a\nReweighting-enhanced Task Singular Vector Merging method that incorporates\noutlier-aware parameter weighting and sparsity-adaptive rank selection\nstrategies adapted to the heavy-tailed parameter distribution and sparsity for\nLLMs, further improving LLM alignment across multiple evaluations. We release\nour trained models for further exploration.\n","authors":["Jinluan Yang","Dingnan Jin","Anke Tang","Li Shen","Didi Zhu","Zhengyu Chen","Daixin Wang","Qing Cui","Zhiqiang Zhang","Jun Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2502.06876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06147v2","updated":"2025-02-13T05:37:45Z","published":"2025-02-10T04:25:05Z","title":"LegalViz: Legal Text Visualization by Text To Diagram Generation","summary":"  Legal documents including judgments and court orders require highly\nsophisticated legal knowledge for understanding. To disclose expert knowledge\nfor non-experts, we explore the problem of visualizing legal texts with\neasy-to-understand diagrams and propose a novel dataset of LegalViz with 23\nlanguages and 7,010 cases of legal document and visualization pairs, using the\nDOT graph description language of Graphviz. LegalViz provides a simple diagram\nfrom a complicated legal corpus identifying legal entities, transactions, legal\nsources, and statements at a glance, that are essential in each judgment. In\naddition, we provide new evaluation metrics for the legal diagram visualization\nby considering graph structures, textual similarities, and legal contents. We\nconducted empirical studies on few-shot and finetuning large language models\nfor generating legal diagrams and evaluated them with these metrics, including\nlegal content-based evaluation within 23 languages. Models trained with\nLegalViz outperform existing models including GPTs, confirming the\neffectiveness of our dataset.\n","authors":["Eri Onami","Taiki Miyanishi","Koki Maeda","Shuhei Kurita"],"pdf_url":"https://arxiv.org/pdf/2502.06147v2.pdf","comment":"NAACL2025"},{"id":"http://arxiv.org/abs/2501.19093v2","updated":"2025-02-13T05:32:21Z","published":"2025-01-31T12:39:28Z","title":"Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations","summary":"  Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings.\n","authors":["Peichao Lai","Jiaxin Gan","Feiyang Ye","Yilei Wang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2501.19093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08972v1","updated":"2025-02-13T05:20:21Z","published":"2025-02-13T05:20:21Z","title":"Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context\n  Learning","summary":"  Language models are aligned to the collective voice of many, resulting in\ngeneric outputs that do not align with specific users' styles. In this work, we\npresent Trial-Error-Explain In-Context Learning (TICL), a tuning-free method\nthat personalizes language models for text generation tasks with fewer than 10\nexamples per user. TICL iteratively expands an in-context learning prompt via a\ntrial-error-explain process, adding model-generated negative samples and\nexplanations that provide fine-grained guidance towards a specific user's\nstyle. TICL achieves favorable win rates on pairwise comparisons with\nLLM-as-a-judge up to 91.5% against the previous state-of-the-art and\noutperforms competitive tuning-free baselines for personalized alignment tasks\nof writing emails, essays and news articles. Both lexical and qualitative\nanalyses show that the negative samples and explanations enable language models\nto learn stylistic context more effectively and overcome the bias towards\nstructural and formal phrases observed in their zero-shot outputs. By\nfront-loading inference compute to create a user-specific in-context learning\nprompt that does not require extra generation steps at test time, TICL presents\na novel yet simple approach for personalized alignment.\n","authors":["Hyundong Cho","Karishma Sharma","Nicolaas Jedema","Leonardo F. R. Ribeiro","Alessandro Moschitti","Ravi Krishnan","Jonathan May"],"pdf_url":"https://arxiv.org/pdf/2502.08972v1.pdf","comment":"NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.02494v2","updated":"2025-02-13T05:14:49Z","published":"2025-02-04T17:09:44Z","title":"Analyzing Similarity Metrics for Data Selection for Language Model\n  Pretraining","summary":"  Similarity between training examples is used to curate pretraining datasets\nfor language models by many methods -- for diversification and to select\nexamples similar to high-quality data. However, similarity is typically\nmeasured with off-the-shelf embedding models that are generic or trained for\ntasks such as retrieval. This paper introduces a framework to analyze the\nsuitability of embedding models specifically for data curation in the language\nmodel pretraining setting. We quantify the correlation between similarity in\nthe embedding space to similarity in pretraining loss between different\ntraining examples, and how diversifying in the embedding space affects\npretraining quality. We analyze a variety of embedding models in our framework,\nwith experiments using the Pile dataset for pretraining a 1.7B parameter\ndecoder-only language model. We find that the embedding models we consider are\nall useful for pretraining data curation. Moreover, a simple approach of\naveraging per-token embeddings proves to be surprisingly competitive with more\nsophisticated embedding models -- likely because the latter are not designed\nspecifically for pretraining data curation. Indeed, we believe our analysis and\nevaluation framework can serve as a foundation for the design of embedding\nmodels that specifically reason about similarity in pretraining datasets.\n","authors":["Dylan Sam","Ayan Chakrabarti","Afshin Rostamizadeh","Srikumar Ramalingam","Gui Citovsky","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.02494v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.07058v2","updated":"2025-02-13T04:55:27Z","published":"2025-02-10T21:49:35Z","title":"Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties","summary":"  A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.\n","authors":["Zixin Tang","Chieh-Yang Huang","Tsung-Chi Li","Ho Yin Sam Ng","Hen-Hsen Huang","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07058v2.pdf","comment":"Accepted by 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL), theme track"},{"id":"http://arxiv.org/abs/2502.08954v1","updated":"2025-02-13T04:35:55Z","published":"2025-02-13T04:35:55Z","title":"Medicine on the Edge: Comparative Performance Analysis of On-Device LLMs\n  for Clinical Reasoning","summary":"  The deployment of Large Language Models (LLM) on mobile devices offers\nsignificant potential for medical applications, enhancing privacy, security,\nand cost-efficiency by eliminating reliance on cloud-based services and keeping\nsensitive health data local. However, the performance and accuracy of on-device\nLLMs in real-world medical contexts remain underexplored. In this study, we\nbenchmark publicly available on-device LLMs using the AMEGA dataset, evaluating\naccuracy, computational efficiency, and thermal limitation across various\nmobile devices. Our results indicate that compact general-purpose models like\nPhi-3 Mini achieve a strong balance between speed and accuracy, while medically\nfine-tuned models such as Med42 and Aloe attain the highest accuracy. Notably,\ndeploying LLMs on older devices remains feasible, with memory constraints\nposing a greater challenge than raw processing power. Our study underscores the\npotential of on-device LLMs for healthcare while emphasizing the need for more\nefficient inference and models tailored to real-world clinical reasoning.\n","authors":["Leon Nissen","Philipp Zagar","Vishnu Ravi","Aydin Zahedivash","Lara Marie Reimer","Stephan Jonas","Oliver Aalami","Paul Schmiedmayer"],"pdf_url":"https://arxiv.org/pdf/2502.08954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08947v1","updated":"2025-02-13T04:01:54Z","published":"2025-02-13T04:01:54Z","title":"Structured Convergence in Large Language Model Representations via\n  Hierarchical Latent Space Folding","summary":"  Token representations in high-dimensional latent spaces often exhibit\nredundancy, limiting computational efficiency and reducing structural coherence\nacross model layers. Hierarchical latent space folding introduces a structured\ntransformation mechanism that enforces a multi-scale organization within\nlearned embeddings, refining representational compactness while preserving\nessential contextual distinctions. The proposed approach incorporates dynamic\nfolding operations that iteratively adjust token embeddings through structured\ntransformations, influencing both short-range and long-range dependencies in\nsequential processing tasks. Empirical evaluation demonstrates a reduction in\nrepresentational variance across layers, contributing to more stable perplexity\ndistributions and enhancing predictive confidence in text generation. The\nstructured redistribution of attention head utilization leads to more efficient\nallocation of computational resources, particularly in deeper layers, where\nhierarchical refinements improve contextual abstraction. Comparative analysis\nof activation sparsity patterns suggests that hierarchical adjustments\nselectively reinforce critical pathways while reducing computational overhead\nin non-essential regions of the model. Statistical assessments of token\nreordering frequencies reveal that hierarchical modifications introduce subtle\nshifts in sequential dependencies, improving contextual alignment while\nmaintaining syntactic correctness. Computational trade-offs associated with\nhierarchical folding introduce marginal increases in training time per epoch,\nyet empirical findings indicate that inference efficiency benefits from the\nstructured representation adjustments. The results highlight the impact of\nhierarchical latent space folding on optimizing model performance through\nimproved representation structuring and computational efficiency.\n","authors":["Fenella Harcourt","Naderdel Piero","Gilbert Sutherland","Daphne Holloway","Harriet Bracknell","Julian Ormsby"],"pdf_url":"https://arxiv.org/pdf/2502.08947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08946v1","updated":"2025-02-13T04:00:03Z","published":"2025-02-13T04:00:03Z","title":"The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding","summary":"  In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.\n","authors":["Mo Yu","Lemao Liu","Junjie Wu","Tsz Ting Chung","Shunchi Zhang","Jiangnan Li","Dit-Yan Yeung","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08946v1.pdf","comment":"NAACL 2025 Main Conference. First 5 authors contributed equally.\n  Project page: https://physico-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2502.08943v1","updated":"2025-02-13T03:43:33Z","published":"2025-02-13T03:43:33Z","title":"Beyond the Singular: The Essential Role of Multiple Generations in\n  Effective Benchmark Evaluation and Analysis","summary":"  Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction.\n","authors":["Wenbo Zhang","Hengrui Cai","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08943v1.pdf","comment":"10 pages, 1 table, 4 Figures"},{"id":"http://arxiv.org/abs/2502.08924v1","updated":"2025-02-13T03:20:37Z","published":"2025-02-13T03:20:37Z","title":"Escaping Collapse: The Strength of Weak Data for Large Language Model\n  Training","summary":"  Synthetically-generated data plays an increasingly larger role in training\nlarge language models. However, while synthetic data has been found to be\nuseful, studies have also shown that without proper curation it can cause LLM\nperformance to plateau, or even \"collapse\", after many training iterations. In\nthis paper, we formalize this question and develop a theoretical framework to\ninvestigate how much curation is needed in order to ensure that LLM performance\ncontinually improves. We find that the requirements are nearly minimal. We\ndescribe a training procedure that converges to an optimal LLM even if almost\nall of the non-synthetic training data is of poor quality. Our analysis is\ninspired by boosting, a classic machine learning technique that leverages a\nvery weak learning algorithm to produce an arbitrarily good classifier. Our\ntraining procedure subsumes many recently proposed methods for training LLMs on\nsynthetic data, and thus our analysis sheds light on why they are successful,\nand also suggests opportunities for future improvement. We present experiments\nthat validate our theory, and show that dynamically focusing labeling resources\non the most challenging examples -- in much the same way that boosting focuses\nthe efforts of the weak learner -- leads to improved performance.\n","authors":["Kareem Amin","Sara Babakniya","Alex Bie","Weiwei Kong","Umar Syed","Sergei Vassilvitskii"],"pdf_url":"https://arxiv.org/pdf/2502.08924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08923v1","updated":"2025-02-13T03:19:50Z","published":"2025-02-13T03:19:50Z","title":"CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without\n  Compromising Quality","summary":"  We introduce CopySpec, an innovative technique designed to tackle the\ninefficiencies LLMs face when generating responses that closely resemble\nprevious outputs. CopySpec identifies repeated sequences in the model's chat\nhistory and speculates that the same tokens will follow, enabling seamless\ncopying without compromising output quality or requiring additional GPU memory.\nTo evaluate the effectiveness of our approach, we conducted experiments using\nfive LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly\ncreated dataset, MT-Redundant. MT-Redundant, introduced in this paper,\ntransforms the second turn of MT-Bench into a request for variations of the\nfirst turn's answer, simulating real-world scenarios where users request\nmodifications to prior responses. Our results demonstrate significant\nspeed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select\nMT-Redundant categories, and 2.66x on the third turn of GSM-8K's\nself-correction tasks. Moreover, we show that CopySpec integrates seamlessly\nwith speculative decoding, yielding an average 49% additional speed-up over\nspeculative decoding for the second turn of MT-Redundant across all eight\ncategories. While LLMs, even with speculative decoding, suffer from slower\ninference as context sizes grow, CopySpec leverages the expanded context to\naccelerate inference, making it faster as the context size increases. Our code\nand dataset are publicly available at https://github.com/RazvanDu/CopySpec.\n","authors":["Razvan-Gabriel Dumitru","Minglai Yang","Vikas Yadav","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2502.08923v1.pdf","comment":"33 pages, 18 figures, 19 tables"},{"id":"http://arxiv.org/abs/2502.08916v1","updated":"2025-02-13T03:08:02Z","published":"2025-02-13T03:08:02Z","title":"PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic\n  Decision-Making Applied to Histopathology","summary":"  Diagnosing diseases through histopathology whole slide images (WSIs) is\nfundamental in modern pathology but is challenged by the gigapixel scale and\ncomplexity of WSIs. Trained histopathologists overcome this challenge by\nnavigating the WSI, looking for relevant patches, taking notes, and compiling\nthem to produce a final holistic diagnostic. Traditional AI approaches, such as\nmultiple instance learning and transformer-based models, fail short of such a\nholistic, iterative, multi-scale diagnostic procedure, limiting their adoption\nin the real-world. We introduce PathFinder, a multi-modal, multi-agent\nframework that emulates the decision-making process of expert pathologists.\nPathFinder integrates four AI agents, the Triage Agent, Navigation Agent,\nDescription Agent, and Diagnosis Agent, that collaboratively navigate WSIs,\ngather evidence, and provide comprehensive diagnoses with natural language\nexplanations. The Triage Agent classifies the WSI as benign or risky; if risky,\nthe Navigation and Description Agents iteratively focus on significant regions,\ngenerating importance maps and descriptive insights of sampled patches.\nFinally, the Diagnosis Agent synthesizes the findings to determine the\npatient's diagnostic classification. Our Experiments show that PathFinder\noutperforms state-of-the-art methods in skin melanoma diagnosis by 8% while\noffering inherent explainability through natural language descriptions of\ndiagnostically relevant patches. Qualitative analysis by pathologists shows\nthat the Description Agent's outputs are of high quality and comparable to\nGPT-4o. PathFinder is also the first AI-based system to surpass the average\nperformance of pathologists in this challenging melanoma classification task by\n9%, setting a new record for efficient, accurate, and interpretable AI-assisted\ndiagnostics in pathology. Data, code and models available at\nhttps://pathfinder-dx.github.io/\n","authors":["Fatemeh Ghezloo","Mehmet Saygin Seyfioglu","Rustin Soraki","Wisdom O. Ikezogwo","Beibin Li","Tejoram Vivekanandan","Joann G. Elmore","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2502.08916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02075v2","updated":"2025-02-13T02:57:47Z","published":"2023-07-05T07:32:34Z","title":"Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for\n  Entity Alignment","summary":"  Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling\nas an effective means to enable more accurate determination of entity\ncorrespondences across two KGs and to mitigate the adverse impact of erroneous\nmatches. A simple but highly effective criterion is further devised to derive\npseudo-labeled entity pairs that satisfy one-to-one correspondences at each\niteration. (2) The cross-iteration pseudo-label calibration operates across\nmultiple consecutive iterations to further improve the pseudo-labeling\nprecision rate by reducing the local pseudo-label selection variability with a\ntheoretical guarantee. The two components are respectively designed to\neliminate Type I and Type II pseudo-labeling errors identified through our\nanalyse. The calibrated pseudo-labels are thereafter used to augment prior\nalignment seeds to reinforce subsequent model training for alignment inference.\nThe effectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. The experimental results\nshow that our approach achieves competitive performance with limited prior\nalignment seeds.\n","authors":["Qijie Ding","Jie Yin","Daokun Zhang","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2307.02075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08910v1","updated":"2025-02-13T02:52:01Z","published":"2025-02-13T02:52:01Z","title":"InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU","summary":"  In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.\n","authors":["Heejun Lee","Geon Park","Jaduk Suh","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2502.08910v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2502.08909v1","updated":"2025-02-13T02:51:17Z","published":"2025-02-13T02:51:17Z","title":"Towards Automated Fact-Checking of Real-World Claims: Exploring Task\n  Formulation and Assessment with LLMs","summary":"  Fact-checking is necessary to address the increasing volume of\nmisinformation. Traditional fact-checking relies on manual analysis to verify\nclaims, but it is slow and resource-intensive. This study establishes baseline\ncomparisons for Automated Fact-Checking (AFC) using Large Language Models\n(LLMs) across multiple labeling schemes (binary, three-class, five-class) and\nextends traditional claim verification by incorporating analysis, verdict\nclassification, and explanation in a structured setup to provide comprehensive\njustifications for real-world claims. We evaluate Llama-3 models of varying\nsizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024)\nusing evidence retrieved via restricted web searches. We utilize TIGERScore as\na reference-free evaluation metric to score the justifications. Our results\nshow that larger LLMs consistently outperform smaller LLMs in classification\naccuracy and justification quality without fine-tuning. We find that smaller\nLLMs in a one-shot scenario provide comparable task performance to fine-tuned\nSmall Language Models (SLMs) with large context sizes, while larger LLMs\nconsistently surpass them. Evidence integration improves performance across all\nmodels, with larger LLMs benefiting most. Distinguishing between nuanced labels\nremains challenging, emphasizing the need for further exploration of labeling\nschemes and alignment with evidences. Our findings demonstrate the potential of\nretrieval-augmented AFC with LLMs.\n","authors":["Premtim Sahitaj","Iffat Maab","Junichi Yamagishi","Jawan Kolanowski","Sebastian Möller","Vera Schmitt"],"pdf_url":"https://arxiv.org/pdf/2502.08909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09213v2","updated":"2025-02-13T02:44:07Z","published":"2025-01-16T00:19:19Z","title":"FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training","summary":"  Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the advanced reasoning required for\ncomplex clinical scenarios, such as differential diagnosis or personalized\ntreatment suggestions. We proposed FineMedLM-o1, which leverages high-quality\nsynthetic medical data and long-form reasoning data for Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and\ndeep reasoning capabilities. Additionally, we introduced Test-Time Training\n(TTT) in the medical domain for the first time, facilitating domain adaptation\nand ensuring reliable, accurate reasoning. Experimental results demonstrate\nthat FineMedLM-o1 achieves a 23% average performance improvement over prior\nmodels on key medical benchmarks. Furthermore, the introduction of TTT provides\nan additional 14% performance boost, highlighting its effectiveness in\nenhancing medical reasoning capabilities. To support this process, we also\nproposed a novel method for synthesizing medical dialogue. Compared to other\nopen-source datasets, our dataset stands out as superior in both quality and\ncomplexity. The project and data will be released on GitHub.\n","authors":["Hongzhou Yu","Tianhao Cheng","Ying Cheng","Rui Feng"],"pdf_url":"https://arxiv.org/pdf/2501.09213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18409v4","updated":"2025-02-13T02:37:06Z","published":"2024-02-28T15:28:36Z","title":"A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the Cookie Theft task in human cognitive tests, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs\nusing images with rich semantics. The benchmark consists of 251 images along\nwith comprehensive annotations. It defines eight reasoning capabilities and\ncomprises an image description task and a visual question answering task. Our\nevaluation of well-known LVLMs shows that there is still a significant gap in\ncognitive abilities between LVLMs and humans.\n","authors":["Xiujie Song","Mengyue Wu","Kenny Q. Zhu","Chunhao Zhang","Yanyi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18409v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02315v2","updated":"2025-02-13T02:33:32Z","published":"2025-02-04T13:36:54Z","title":"VaiBot: Shuttle Between the Instructions and Parameters of Large\n  Language Models","summary":"  How to interact with LLMs through \\emph{instructions} has been widely studied\nby researchers. However, previous studies have treated the emergence of\ninstructions and the training of LLMs on task data as separate processes,\noverlooking the inherent unity between the two. This paper proposes a neural\nnetwork framework, VaiBot, that integrates VAE and VIB, designed to uniformly\nmodel, learn, and infer both deduction and induction tasks under LLMs. Through\nexperiments, we demonstrate that VaiBot performs on par with existing baseline\nmethods in terms of deductive capabilities while significantly surpassing them\nin inductive capabilities. We also find that VaiBot can scale up using general\ninstruction-following data and exhibits excellent one-shot induction abilities.\nWe finally synergistically integrate the deductive and inductive processes of\nVaiBot. Through T-SNE dimensionality reduction, we observe that its\ninductive-deductive process significantly improves the distribution of training\nparameters, enabling it to outperform baseline methods in inductive reasoning\ntasks. The code and data for this paper can be found at\nhttps://anonymous.4open.science/r/VaiBot-021F.\n","authors":["Wangtao Sun","Haotian Xu","Huanxuan Liao","Xuanqing Yu","Zhongtao Jiang","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.02315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08900v1","updated":"2025-02-13T02:27:30Z","published":"2025-02-13T02:27:30Z","title":"Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous\n  Languages?","summary":"  While ChatGPT and GPT-based models are able to effectively perform many tasks\nwithout additional fine-tuning, they struggle with related to extremely\nlow-resource languages and indigenous languages. Uniform Meaning Representation\n(UMR), a semantic representation designed to capture the meaning of texts in\nmany languages, is well-poised to be leveraged in the development of\nlow-resource language technologies. In this work, we explore the downstream\ntechnical utility of UMR for low-resource languages by incorporating it into\nGPT-4 prompts. Specifically, we examine the ability of GPT-4 to perform\ntranslation from three indigenous languages (Navajo, Ar\\'apaho, and Kukama),\nwith and without demonstrations, as well as with and without UMR annotations.\nUltimately we find that in the majority of our test cases, integrating UMR into\nthe prompt results in a statistically significant increase in performance,\nwhich is a promising indication of future applications of the UMR formalism.\n","authors":["Shira Wein"],"pdf_url":"https://arxiv.org/pdf/2502.08900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08896v1","updated":"2025-02-13T02:22:48Z","published":"2025-02-13T02:22:48Z","title":"Communication is All You Need: Persuasion Dataset Construction via\n  Multi-LLM Communication","summary":"  Large Language Models (LLMs) have shown proficiency in generating persuasive\ndialogue, yet concerns about the fluency and sophistication of their outputs\npersist. This paper presents a multi-LLM communication framework designed to\nenhance the generation of persuasive data automatically. This framework\nfacilitates the efficient production of high-quality, diverse linguistic\ncontent with minimal human oversight. Through extensive evaluations, we\ndemonstrate that the generated data excels in naturalness, linguistic\ndiversity, and the strategic use of persuasion, even in complex scenarios\ninvolving social taboos. The framework also proves adept at generalizing across\nnovel contexts. Our results highlight the framework's potential to\nsignificantly advance research in both computational and social science domains\nconcerning persuasive communication.\n","authors":["Weicheng Ma","Hefan Zhang","Ivory Yang","Shiyu Ji","Joice Chen","Farnoosh Hashemi","Shubham Mohole","Ethan Gearey","Michael Macy","Saeed Hassanpour","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2502.08896v1.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.08888v1","updated":"2025-02-13T02:03:30Z","published":"2025-02-13T02:03:30Z","title":"LLM-Enhanced Multiple Instance Learning for Joint Rumor and Stance\n  Detection with Social Context Information","summary":"  The proliferation of misinformation, such as rumors on social media, has\ndrawn significant attention, prompting various expressions of stance among\nusers. Although rumor detection and stance detection are distinct tasks, they\ncan complement each other. Rumors can be identified by cross-referencing\nstances in related posts, and stances are influenced by the nature of the\nrumor. However, existing stance detection methods often require post-level\nstance annotations, which are costly to obtain. We propose a novel LLM-enhanced\nMIL approach to jointly predict post stance and claim class labels, supervised\nsolely by claim labels, using an undirected microblog propagation model. Our\nweakly supervised approach relies only on bag-level labels of claim veracity,\naligning with multi-instance learning (MIL) principles. To achieve this, we\ntransform the multi-class problem into multiple MIL-based binary classification\nproblems. We then employ a discriminative attention layer to aggregate the\noutputs from these classifiers into finer-grained classes. Experiments\nconducted on three rumor datasets and two stance datasets demonstrate the\neffectiveness of our approach, highlighting strong connections between rumor\nveracity and expressed stances in responding posts. Our method shows promising\nperformance in joint rumor and stance detection compared to the\nstate-of-the-art methods.\n","authors":["Ruichao Yang","Jing Ma","Wei Gao","Hongzhan Lin"],"pdf_url":"https://arxiv.org/pdf/2502.08888v1.pdf","comment":"Accepted by ACM TIST"},{"id":"http://arxiv.org/abs/2501.02844v2","updated":"2025-02-13T01:58:12Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07972v2","updated":"2025-02-13T01:23:29Z","published":"2025-02-11T21:36:31Z","title":"Training Sparse Mixture Of Experts Text Embedding Models","summary":"  Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline at\n\\href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.\n","authors":["Zach Nussbaum","Brandon Duderstadt"],"pdf_url":"https://arxiv.org/pdf/2502.07972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08866v1","updated":"2025-02-13T00:37:27Z","published":"2025-02-13T00:37:27Z","title":"BrainWavLM: Fine-tuning Speech Representations with Brain Responses to\n  Language","summary":"  Speech encoding models use auditory representations to predict how the human\nbrain responds to spoken language stimuli. Most performant encoding models\nlinearly map the hidden states of artificial neural networks to brain data, but\nthis linear restriction may limit their effectiveness. In this work, we use\nlow-rank adaptation (LoRA) to fine-tune a WavLM-based encoding model end-to-end\non a brain encoding objective, producing a model we name BrainWavLM. We show\nthat fine-tuning across all of cortex improves average encoding performance\nwith greater stability than without LoRA. This improvement comes at the expense\nof low-level regions like auditory cortex (AC), but selectively fine-tuning on\nthese areas improves performance in AC, while largely retaining gains made in\nthe rest of cortex. Fine-tuned models generalized across subjects, indicating\nthat they learned robust brain-like representations of the speech stimuli.\nFinally, by training linear probes, we showed that the brain data strengthened\nsemantic representations in the speech model without any explicit annotations.\nOur results demonstrate that brain fine-tuning produces best-in-class speech\nencoding models, and that non-linear methods have the potential to bridge the\ngap between artificial and biological representations of semantics.\n","authors":["Nishitha Vattikonda","Aditya R. Vaidya","Richard J. Antonello","Alexander G. Huth"],"pdf_url":"https://arxiv.org/pdf/2502.08866v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08859v1","updated":"2025-02-13T00:18:34Z","published":"2025-02-13T00:18:34Z","title":"EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges","summary":"  As language models master existing reasoning benchmarks, we need new\nchallenges to evaluate their cognitive frontiers. Puzzle-solving events are\nrich repositories of challenging multimodal problems that test a wide range of\nadvanced reasoning and knowledge capabilities, making them a unique testbed for\nevaluating frontier language models. We introduce EnigmaEval, a dataset of\nproblems and solutions derived from puzzle competitions and events that probes\nmodels' ability to perform implicit knowledge synthesis and multi-step\ndeductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle\nsolving challenges models to discover hidden connections between seemingly\nunrelated pieces of information to uncover solution paths. The benchmark\ncomprises 1184 puzzles of varying complexity -- each typically requiring teams\nof skilled solvers hours to days to complete -- with unambiguous, verifiable\nsolutions that enable efficient evaluation. State-of-the-art language models\nachieve extremely low accuracy on these puzzles, even lower than other\ndifficult benchmarks such as Humanity's Last Exam, unveiling models'\nshortcomings when challenged with problems requiring unstructured and lateral\nreasoning.\n","authors":["Clinton J. Wang","Dean Lee","Cristina Menghini","Johannes Mols","Jack Doughty","Adam Khoja","Jayson Lynch","Sean Hendryx","Summer Yue","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08859v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.09623v1","updated":"2025-02-13T18:59:50Z","published":"2025-02-13T18:59:50Z","title":"Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures","summary":"  Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights.\n","authors":["Francesco Ballerini","Pierluigi Zama Ramirez","Samuele Salti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2502.09623v1.pdf","comment":"Under review"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.15118v2","updated":"2025-02-13T17:27:18Z","published":"2025-01-25T08:09:37Z","title":"ABXI: Invariant Interest Adaptation for Task-Guided Cross-Domain\n  Sequential Recommendation","summary":"  Cross-Domain Sequential Recommendation (CDSR) has recently gained attention\nfor countering data sparsity by transferring knowledge across domains. A common\napproach merges domain-specific sequences into cross-domain sequences, serving\nas bridges to connect domains. One key challenge is to correctly extract the\nshared knowledge among these sequences and appropriately transfer it. Most\nexisting works directly transfer unfiltered cross-domain knowledge rather than\nextracting domain-invariant components and adaptively integrating them into\ndomain-specific modelings. Another challenge lies in aligning the\ndomain-specific and cross-domain sequences. Existing methods align these\nsequences based on timestamps, but this approach can cause prediction\nmismatches when the current tokens and their targets belong to different\ndomains. In such cases, the domain-specific knowledge carried by the current\ntokens may degrade performance. To address these challenges, we propose the\nA-B-Cross-to-Invariant Learning Recommender (ABXI). Specifically, leveraging\nLoRA's effectiveness for efficient adaptation, ABXI incorporates two types of\nLoRAs to facilitate knowledge adaptation. First, all sequences are processed\nthrough a shared encoder that employs a domain LoRA for each sequence, thereby\npreserving unique domain characteristics. Next, we introduce an invariant\nprojector that extracts domain-invariant interests from cross-domain\nrepresentations, utilizing an invariant LoRA to adapt these interests into\nmodeling each specific domain. Besides, to avoid prediction mismatches, all\ndomain-specific sequences are aligned to match the domains of the cross-domain\nground truths. Experimental results on three datasets demonstrate that our\napproach outperforms other CDSR counterparts by a large margin. The codes are\navailable in https://github.com/DiMarzioBian/ABXI.\n","authors":["Qingtian Bian","Marcus Vinícius de Carvalho","Tieying Li","Jiaxing Xu","Hui Fang","Yiping Ke"],"pdf_url":"https://arxiv.org/pdf/2501.15118v2.pdf","comment":"Accepted by WebConf '25 (WWW '25)"},{"id":"http://arxiv.org/abs/2312.00326v9","updated":"2025-02-13T17:06:52Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v9.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09375v1","updated":"2025-02-13T14:44:15Z","published":"2025-02-13T14:44:15Z","title":"FARM: Frequency-Aware Model for Cross-Domain Live-Streaming\n  Recommendation","summary":"  Live-streaming services have attracted widespread popularity due to their\nreal-time interactivity and entertainment value. Users can engage with\nlive-streaming authors by participating in live chats, posting likes, or\nsending virtual gifts to convey their preferences and support. However, the\nlive-streaming services faces serious data-sparsity problem, which can be\nattributed to the following two points: (1) User's valuable behaviors are\nusually sparse, e.g., like, comment and gift, which are easily overlooked by\nthe model, making it difficult to describe user's personalized preference. (2)\nThe main exposure content on our platform is short-video, which is 9 times\nhigher than the exposed live-streaming, leading to the inability of\nlive-streaming content to fully model user preference. To this end, we propose\na Frequency-Aware Model for Cross-Domain Live-Streaming Recommendation, termed\nas FARM. Specifically, we first present the intra-domain frequency aware module\nto enable our model to perceive user's sparse yet valuable behaviors, i.e.,\nhigh-frequency information, supported by the Discrete Fourier Transform (DFT).\nTo transfer user preference across the short-video and live-streaming domains,\nwe propose a novel preference align before fuse strategy, which consists of two\nparts: the cross-domain preference align module to align user preference in\nboth domains with contrastive learning, and the cross-domain preference fuse\nmodule to further fuse user preference in both domains using a serious of\ntailor-designed attention mechanisms. Extensive offline experiments and online\nA/B testing on Kuaishou live-streaming services demonstrate the effectiveness\nand superiority of FARM. Our FARM has been deployed in online live-streaming\nservices and currently serves hundreds of millions of users on Kuaishou.\n","authors":["Xiaodong Li","Ruochen Yang","Shuang Wen","Shen Wang","Yueyang Liu","Guoquan Wang","Weisong Hu","Qiang Luo","Jiawei Sheng","Tingwen Liu","Jiangxia Cao","Shuang Yang","Zhaojie Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18378v3","updated":"2025-02-13T13:35:51Z","published":"2024-12-24T12:07:48Z","title":"RaSeRec: Retrieval-Augmented Sequential Recommendation","summary":"  Although prevailing supervised and self-supervised learning augmented\nsequential recommendation (SeRec) models have achieved improved performance\nwith powerful neural network architectures, we argue that they still suffer\nfrom two limitations: (1) Preference Drift, where models trained on past data\ncan hardly accommodate evolving user preference; and (2) Implicit Memory, where\nhead patterns dominate parametric learning, making it harder to recall long\ntails. In this work, we explore retrieval augmentation in SeRec, to address\nthese limitations. Specifically, we propose a Retrieval-Augmented Sequential\nRecommendation framework, named RaSeRec, the main idea of which is to maintain\na dynamic memory bank to accommodate preference drifts and retrieve relevant\nmemories to augment user modeling explicitly. It consists of two stages: (i)\ncollaborative-based pre-training, which learns to recommend and retrieve; (ii)\nretrieval-augmented fine-tuning, which learns to leverage retrieved memories.\nExtensive experiments on three datasets fully demonstrate the superiority and\neffectiveness of RaSeRec. The implementation code is available at\nhttps://github.com/HITsz-TMG/RaSeRec.\n","authors":["Xinping Zhao","Baotian Hu","Yan Zhong","Shouzheng Huang","Zihao Zheng","Meng Wang","Haofen Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.18378v3.pdf","comment":"14 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.09319v1","updated":"2025-02-13T13:33:45Z","published":"2025-02-13T13:33:45Z","title":"Bridging Jensen Gap for Max-Min Group Fairness Optimization in\n  Recommendation","summary":"  Group max-min fairness (MMF) is commonly used in fairness-aware recommender\nsystems (RS) as an optimization objective, as it aims to protect marginalized\nitem groups and ensures a fair competition platform. However, our theoretical\nanalysis indicates that integrating MMF constraint violates the assumption of\nsample independence during optimization, causing the loss function to deviate\nfrom linear additivity. Such nonlinearity property introduces the Jensen gap\nbetween the model's convergence point and the optimal point if mini-batch\nsampling is applied. Both theoretical and empirical studies show that as the\nmini-batch size decreases and the group size increases, the Jensen gap will\nwiden accordingly. Some methods using heuristic re-weighting or debiasing\nstrategies have the potential to bridge the Jensen gap. However, they either\nlack theoretical guarantees or suffer from heavy computational costs. To\novercome these limitations, we first theoretically demonstrate that the\nMMF-constrained objective can be essentially reformulated as a group-weighted\noptimization objective. Then we present an efficient and effective algorithm\nnamed FairDual, which utilizes a dual optimization technique to minimize the\nJensen gap. Our theoretical analysis demonstrates that FairDual can achieve a\nsub-linear convergence rate to the globally optimal solution and the Jensen gap\ncan be well bounded under a mini-batch sampling strategy with random shuffle.\nExtensive experiments conducted using six large-scale RS backbone models on\nthree publicly available datasets demonstrate that FairDual outperforms all\nbaselines in terms of both accuracy and fairness. Our data and codes are shared\nat https://github.com/XuChen0427/FairDual.\n","authors":["Chen Xu","Yuxin Li","Wenjie Wang","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09319v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09304v1","updated":"2025-02-13T13:16:16Z","published":"2025-02-13T13:16:16Z","title":"KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for\n  Graph-RAG","summary":"  Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\nin Large Language Model (LLM)-based question answering. It is particularly\nuseful in domains such as biomedicine, law, and political science, where\nretrieval often requires multi-hop reasoning over proprietary documents. Some\nexisting Graph-RAG systems construct KNN graphs based on text chunk relevance,\nbut this coarse-grained approach fails to capture entity relationships within\ntexts, leading to sub-par retrieval and generation quality. To address this,\nrecent solutions leverage LLMs to extract entities and relationships from text\nchunks, constructing triplet-based knowledge graphs. However, this approach\nincurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose\nKET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\nset of key text chunks and leverages an LLM to construct a knowledge graph\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\nserving as a lightweight alternative to a full knowledge graph. During\nretrieval, KET-RAG searches both structures: it follows the local search\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\nsearch on the bipartite graph to improve retrieval quality. We evaluate eight\nsolutions on two real-world datasets, demonstrating that KET-RAG outperforms\nall competitors in indexing cost, retrieval effectiveness, and generation\nquality. Notably, it achieves comparable or superior retrieval quality to\nMicrosoft's Graph-RAG while reducing indexing costs by over an order of\nmagnitude. Additionally, it improves the generation quality by up to 32.4%\nwhile lowering indexing costs by around 20%.\n","authors":["Yiqian Huang","Shiqi Zhang","Xiaokui Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.09304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05033v2","updated":"2025-02-13T12:09:50Z","published":"2024-07-06T09:58:58Z","title":"PeaPOD: Personalized Prompt Distillation for Generative Recommendation","summary":"  Recently, researchers have investigated the capabilities of Large Language\nModels (LLMs) for generative recommender systems. Existing LLM-based\nrecommender models are trained by adding user and item IDs to a discrete prompt\ntemplate. However, the disconnect between IDs and natural language makes it\ndifficult for the LLM to learn the relationship between users. To address this\nissue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to\ndistill user preferences as personalized soft prompts. Considering the\ncomplexities of user preferences in the real world, we maintain a shared set of\nlearnable prompts that are dynamically weighted based on the user's interests\nto construct the user-personalized prompt in a compositional manner.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof our PeaPOD model on sequential recommendation, top-n recommendation, and\nexplanation generation tasks.\n","authors":["Jerome Ramos","Bin Wu","Aldo Lipani"],"pdf_url":"https://arxiv.org/pdf/2407.05033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09155v1","updated":"2025-02-13T10:36:17Z","published":"2025-02-13T10:36:17Z","title":"Use of Air Quality Sensor Network Data for Real-time Pollution-Aware POI\n  Suggestion","summary":"  This demo paper presents AirSense-R, a privacy-preserving mobile application\nthat provides real-time, pollution-aware recommendations for points of interest\n(POIs) in urban environments. By combining real-time air quality monitoring\ndata with user preferences, the proposed system aims to help users make\nhealth-conscious decisions about the locations they visit. The application\nutilizes collaborative filtering for personalized suggestions, and federated\nlearning for privacy protection, and integrates air pollutant readings from\nAirSENCE sensor networks in cities such as Bari, Italy, and Cork, Ireland.\nAdditionally, the AirSENCE prediction engine can be employed to detect anomaly\nreadings and interpolate for air quality readings in areas with sparse sensor\ncoverage. This system offers a promising, health-oriented POI recommendation\nsolution that adapts dynamically to current urban air quality conditions while\nsafeguarding user privacy. The code of AirTOWN and a demonstration video is\nmade available at the following repo:\nhttps://github.com/AirtownApp/Airtown-Application.git.\n","authors":["Giuseppe Fasano","Yashar Deldjoo","Tommaso di Noia","Bianca Lau","Sina Adham-Khiabani","Eric Morris","Xia Liu","Ganga Chinna Rao Devarapu","Liam O'Faolain"],"pdf_url":"https://arxiv.org/pdf/2502.09155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09089v1","updated":"2025-02-13T09:01:34Z","published":"2025-02-13T09:01:34Z","title":"Semantic Ads Retrieval at Walmart eCommerce with Language Models\n  Progressively Trained on Multiple Knowledge Domains","summary":"  Sponsored search in e-commerce poses several unique and complex challenges.\nThese challenges stem from factors such as the asymmetric language structure\nbetween search queries and product names, the inherent ambiguity in user search\nintent, and the vast volume of sparse and imbalanced search corpus data. The\nrole of the retrieval component within a sponsored search system is pivotal,\nserving as the initial step that directly affects the subsequent ranking and\nbidding systems. In this paper, we present an end-to-end solution tailored to\noptimize the ads retrieval system on Walmart.com. Our approach is to pretrain\nthe BERT-like classification model with product category information, enhancing\nthe model's understanding of Walmart product semantics. Second, we design a\ntwo-tower Siamese Network structure for embedding structures to augment\ntraining efficiency. Third, we introduce a Human-in-the-loop Progressive Fusion\nTraining method to ensure robust model performance. Our results demonstrate the\neffectiveness of this pipeline. It enhances the search relevance metric by up\nto 16% compared to a baseline DSSM-based model. Moreover, our large-scale\nonline A/B testing demonstrates that our approach surpasses the ad revenue of\nthe existing production model.\n","authors":["Zhaodong Wang","Weizhi Du","Md Omar Faruk Rokon","Pooshpendu Adhikary","Yanbing Xue","Jiaxuan Xu","Jianghong Zhou","Kuang-chih Lee","Musen Wen"],"pdf_url":"https://arxiv.org/pdf/2502.09089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09058v1","updated":"2025-02-13T08:19:45Z","published":"2025-02-13T08:19:45Z","title":"Unleashing the Power of Large Language Model for Denoising\n  Recommendation","summary":"  Recommender systems are crucial for personalizing user experiences but often\ndepend on implicit feedback data, which can be noisy and misleading. Existing\ndenoising studies involve incorporating auxiliary information or learning\nstrategies from interaction data. However, they struggle with the inherent\nlimitations of external knowledge and interaction data, as well as the\nnon-universality of certain predefined assumptions, hindering accurate noise\nidentification. Recently, large language models (LLMs) have gained attention\nfor their extensive world knowledge and reasoning abilities, yet their\npotential in enhancing denoising in recommendations remains underexplored. In\nthis paper, we introduce LLaRD, a framework leveraging LLMs to improve\ndenoising in recommender systems, thereby boosting overall recommendation\nperformance. Specifically, LLaRD generates denoising-related knowledge by first\nenriching semantic insights from observational data via LLMs and inferring\nuser-item preference knowledge. It then employs a novel Chain-of-Thought (CoT)\ntechnique over user-item interaction graphs to reveal relation knowledge for\ndenoising. Finally, it applies the Information Bottleneck (IB) principle to\nalign LLM-generated denoising knowledge with recommendation targets, filtering\nout noise and irrelevant LLM knowledge. Empirical results demonstrate LLaRD's\neffectiveness in enhancing denoising and recommendation accuracy.\n","authors":["Shuyao Wang","Zhi Zheng","Yongduo Sui","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.09058v1.pdf","comment":"12 pages, 5 figures, 4 tables. Accecpted by WWW 2025"},{"id":"http://arxiv.org/abs/2502.09050v1","updated":"2025-02-13T08:05:14Z","published":"2025-02-13T08:05:14Z","title":"Leveraging Member-Group Relations via Multi-View Graph Filtering for\n  Effective Group Recommendation","summary":"  Group recommendation aims at providing optimized recommendations tailored to\ndiverse groups, enabling groups to enjoy appropriate items. On the other hand,\nmost existing group recommendation methods are built upon deep neural network\n(DNN) architectures designed to capture the intricate relationships between\nmember-level and group-level interactions. While these DNN-based approaches\nhave proven their effectiveness, they require complex and expensive training\nprocedures to incorporate group-level interactions in addition to member-level\ninteractions. To overcome such limitations, we introduce Group-GF, a new\napproach for extremely fast recommendations of items to each group via\nmulti-view graph filtering (GF) that offers a holistic view of complex\nmember-group dynamics, without the need for costly model training.\nSpecifically, in Group-GF, we first construct three item similarity graphs\nmanifesting different viewpoints for GF. Then, we discover a distinct\npolynomial graph filter for each similarity graph and judiciously aggregate the\nthree graph filters. Extensive experiments demonstrate the effectiveness of\nGroup-GF in terms of significantly reducing runtime and achieving\nstate-of-the-art recommendation accuracy.\n","authors":["Chae-Hyun Kim","Yoon-Ryung Choi","Jin-Duk Park","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2502.09050v1.pdf","comment":"5 pages, 3 figures, 4 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)"},{"id":"http://arxiv.org/abs/2502.09046v1","updated":"2025-02-13T08:01:38Z","published":"2025-02-13T08:01:38Z","title":"Criteria-Aware Graph Filtering: Extremely Fast Yet Accurate\n  Multi-Criteria Recommendation","summary":"  Multi-criteria (MC) recommender systems, which utilize MC rating information\nfor recommendation, are increasingly widespread in various e-commerce domains.\nHowever, the MC recommendation using training-based collaborative filtering,\nrequiring consideration of multiple ratings compared to single-criterion\ncounterparts, often poses practical challenges in achieving state-of-the-art\nperformance along with scalable model training. To solve this problem, we\npropose CA-GF, a training-free MC recommendation method, which is built upon\ncriteria-aware graph filtering for efficient yet accurate MC recommendations.\nSpecifically, first, we construct an item-item similarity graph using an MC\nuser-expansion graph. Next, we design CA-GF composed of the following key\ncomponents, including 1) criterion-specific graph filtering where the optimal\nfilter for each criterion is found using various types of polynomial low-pass\nfilters and 2) criteria preference-infused aggregation where the smoothed\nsignals from each criterion are aggregated. We demonstrate that CA-GF is (a)\nefficient: providing the computational efficiency, offering the extremely fast\nruntime of less than 0.2 seconds even on the largest benchmark dataset, (b)\naccurate: outperforming benchmark MC recommendation methods, achieving\nsubstantial accuracy gains up to 24% compared to the best competitor, and (c)\ninterpretable: providing interpretations for the contribution of each criterion\nto the model prediction based on visualizations.\n","authors":["Jin-Duk Park","Jaemin Yoo","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2502.09046v1.pdf","comment":"12 pages, 8 figures, 7 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)"},{"id":"http://arxiv.org/abs/2502.05561v2","updated":"2025-02-13T07:44:07Z","published":"2025-02-08T13:20:13Z","title":"Diffusion Model for Interest Refinement in Multi-Interest Recommendation","summary":"  Multi-interest candidate matching plays a pivotal role in personalized\nrecommender systems, as it captures diverse user interests from their\nhistorical behaviors. Most existing methods utilize attention mechanisms to\ngenerate interest representations by aggregating historical item embeddings.\nHowever, these methods only capture overall item-level relevance, leading to\ncoarse-grained interest representations that include irrelevant information. To\naddress this issue, we propose the Diffusion Multi-Interest model (DMI), a\nnovel framework for refining user interest representations at the dimension\nlevel. Specifically, DMI first introduces controllable noise into\ncoarse-grained interest representations at the dimensional level. Then, in the\niterative reconstruction process, DMI combines a cross-attention mechanism and\nan item pruning strategy to reconstruct the personalized interest vectors with\nthe guidance of tailored collaborative information. Extensive experiments\ndemonstrate the effectiveness of DMI, surpassing state-of-the-art methods on\noffline evaluations and an online A/B test. Successfully deployed in the\nreal-world recommender system, DMI effectively enhances user satisfaction and\nsystem performance at scale, serving the major traffic of hundreds of millions\nof daily active users. \\footnote{The code will be released for reproducibility\nonce the paper is accepted.}\n","authors":["Yankun Le","Haoran Li","Baoyuan Ou","Yingjie Qin","Zhixuan Yang","Ruilong Su","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09027v1","updated":"2025-02-13T07:31:34Z","published":"2025-02-13T07:31:34Z","title":"A Contextual-Aware Position Encoding for Sequential Recommendation","summary":"  Sequential recommendation (SR), which encodes user activity to predict the\nnext action, has emerged as a widely adopted strategy in developing commercial\npersonalized recommendation systems. A critical component of modern SR models\nis the attention mechanism, which synthesizes users' historical activities.\nThis mechanism is typically order-invariant and generally relies on position\nencoding (PE). Conventional SR models simply assign a learnable vector to each\nposition, resulting in only modest gains compared to traditional recommendation\nmodels. Moreover, limited research has been conducted on position encoding\ntailored for sequential recommendation, leaving a significant gap in addressing\nits unique requirements. To bridge this gap, we propose a novel\nContextual-Aware Position Encoding method for sequential recommendation,\nabbreviated as CAPE. To the best of our knowledge, CAPE is the first PE method\nspecifically designed for sequential recommendation. Comprehensive experiments\nconducted on benchmark SR datasets demonstrate that CAPE consistently enhances\nmultiple mainstream backbone models and achieves state-of-the-art performance,\nacross small and large scale model size. Furthermore, we deployed CAPE in an\nindustrial setting on a real-world commercial platform, clearly showcasing the\neffectiveness of our approach. Our source code is available at\nhttps://github.com/yjdy/CAPE.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2502.09027v1.pdf","comment":"Accepted by WWW'25 Industry Track"},{"id":"http://arxiv.org/abs/2502.03375v3","updated":"2025-02-13T02:17:49Z","published":"2025-02-05T17:14:45Z","title":"Interactive Visualization Recommendation with Hier-SUCB","summary":"  Visualization recommendation aims to enable rapid visual analysis of massive\ndatasets. In real-world scenarios, it is essential to quickly gather and\ncomprehend user preferences to cover users from diverse backgrounds, including\nvarying skill levels and analytical tasks. Previous approaches to personalized\nvisualization recommendations are non-interactive and rely on initial user data\nfor new users. As a result, these models cannot effectively explore options or\nadapt to real-time feedback. To address this limitation, we propose an\ninteractive personalized visualization recommendation (PVisRec) system that\nlearns on user feedback from previous interactions. For more interactive and\naccurate recommendations, we propose Hier-SUCB, a contextual combinatorial\nsemi-bandit in the PVisRec setting. Theoretically, we show an improved overall\nregret bound with the same rank of time but an improved rank of action space.\nWe further demonstrate the effectiveness of Hier-SUCB through extensive\nexperiments where it is comparable to offline methods and outperforms other\nbandit algorithms in the setting of visualization recommendation.\n","authors":["Songwen Hu","Ryan A. Rossi","Tong Yu","Junda Wu","Handong Zhao","Sungchul Kim","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2502.03375v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02844v2","updated":"2025-02-13T01:58:12Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07972v2","updated":"2025-02-13T01:23:29Z","published":"2025-02-11T21:36:31Z","title":"Training Sparse Mixture Of Experts Text Embedding Models","summary":"  Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline at\n\\href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.\n","authors":["Zach Nussbaum","Brandon Duderstadt"],"pdf_url":"https://arxiv.org/pdf/2502.07972v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09619v1","updated":"2025-02-13T18:59:44Z","published":"2025-02-13T18:59:44Z","title":"Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights","summary":"  With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.\n","authors":["Jonathan Kahana","Or Nathan","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2502.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09616v1","updated":"2025-02-13T18:59:15Z","published":"2025-02-13T18:59:15Z","title":"Variational Rectified Flow Matching","summary":"  We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.\n","authors":["Pengsheng Guo","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2502.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09614v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References","summary":"  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n","authors":["Xueyi Liu","Jianibieke Adalibieke","Qianwei Han","Yuzhe Qin","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.09614v1.pdf","comment":"Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE"},{"id":"http://arxiv.org/abs/2402.17767v2","updated":"2025-02-13T18:59:11Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Objects in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v2.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-articulated-objects/"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09611v1","updated":"2025-02-13T18:58:15Z","published":"2025-02-13T18:58:15Z","title":"Designing a Conditional Prior Distribution for Flow-Based Generative\n  Models","summary":"  Flow-based generative models have recently shown impressive performance for\nconditional generation tasks, such as text-to-image generation. However,\ncurrent methods transform a general unimodal noise distribution to a specific\nmode of the target data distribution. As such, every point in the initial\nsource distribution can be mapped to every point in the target distribution,\nresulting in long average paths. To this end, in this work, we tap into a\nnon-utilized property of conditional flow-based models: the ability to design a\nnon-trivial prior distribution. Given an input condition, such as a text\nprompt, we first map it to a point lying in data space, representing an\n``average\" data point with the minimal average distance to all data points of\nthe same conditional mode (e.g., class). We then utilize the flow matching\nformulation to map samples from a parametric distribution centered around this\npoint to the conditional target distribution. Experimentally, our method\nsignificantly improves training times and generation efficiency (FID, KID and\nCLIP alignment scores) compared to baselines, producing high quality samples\nusing fewer sampling steps.\n","authors":["Noam Issachar","Mohammad Salama","Raanan Fattal","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13904v3","updated":"2025-02-13T18:58:14Z","published":"2025-01-23T18:34:09Z","title":"Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models","summary":"  Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank factorization scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.\n","authors":["Linh Tran","Wei Sun","Stacy Patterson","Ana Milanova"],"pdf_url":"https://arxiv.org/pdf/2501.13904v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09609v1","updated":"2025-02-13T18:57:20Z","published":"2025-02-13T18:57:20Z","title":"Score-of-Mixture Training: Training One-Step Generative Models Made\n  Simple","summary":"  We propose Score-of-Mixture Training (SMT), a novel framework for training\none-step generative models by minimizing a class of divergences called the\n$\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score\nof mixture distributions between real and fake samples across multiple noise\nlevels. Similar to consistency models, our approach supports both training from\nscratch (SMT) and distillation using a pretrained diffusion model, which we\ncall Score-of-Mixture Distillation (SMD). It is simple to implement, requires\nminimal hyperparameter tuning, and ensures stable training. Experiments on\nCIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even\noutperform existing methods.\n","authors":["Tejas Jayashankar","J. Jon Ryu","Gregory Wornell"],"pdf_url":"https://arxiv.org/pdf/2502.09609v1.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09597v1","updated":"2025-02-13T18:52:03Z","published":"2025-02-13T18:52:03Z","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs","summary":"  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n","authors":["Siyan Zhao","Mingyi Hong","Yang Liu","Devamanyu Hazarika","Kaixiang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09597v1.pdf","comment":"Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/"},{"id":"http://arxiv.org/abs/2502.09591v1","updated":"2025-02-13T18:48:04Z","published":"2025-02-13T18:48:04Z","title":"Censor Dependent Variational Inference","summary":"  This paper provides a comprehensive analysis of variational inference in\nlatent variable models for survival analysis, emphasizing the distinctive\nchallenges associated with applying variational methods to survival data. We\nidentify a critical weakness in the existing methodology, demonstrating how a\npoorly designed variational distribution may hinder the objective of survival\nanalysis tasks--modeling time-to-event distributions. We prove that the optimal\nvariational distribution, which perfectly bounds the log-likelihood, may depend\non the censoring mechanism. To address this issue, we propose censor-dependent\nvariational inference (CDVI), tailored for latent variable models in survival\nanalysis. More practically, we introduce CD-CVAE, a V-structure Variational\nAutoencoder (VAE) designed for the scalable implementation of CDVI. Further\ndiscussion extends some existing theories and training techniques to survival\nanalysis. Extensive experiments validate our analysis and demonstrate\nsignificant improvements in the estimation of individual survival\ndistributions.\n","authors":["Chuanhui Liu","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09587v1","updated":"2025-02-13T18:45:56Z","published":"2025-02-13T18:45:56Z","title":"Rolling Ahead Diffusion for Traffic Scene Simulation","summary":"  Realistic driving simulation requires that NPCs not only mimic natural\ndriving behaviors but also react to the behavior of other simulated agents.\nRecent developments in diffusion-based scenario generation focus on creating\ndiverse and realistic traffic scenarios by jointly modelling the motion of all\nthe agents in the scene. However, these traffic scenarios do not react when the\nmotion of agents deviates from their modelled trajectories. For example, the\nego-agent can be controlled by a stand along motion planner. To produce\nreactive scenarios with joint scenario models, the model must regenerate the\nscenario at each timestep based on new observations in a Model Predictive\nControl (MPC) fashion. Although reactive, this method is time-consuming, as one\ncomplete possible future for all NPCs is generated per simulation step.\nAlternatively, one can utilize an autoregressive model (AR) to predict only the\nimmediate next-step future for all NPCs. Although faster, this method lacks the\ncapability for advanced planning. We present a rolling diffusion based traffic\nscene generation model which mixes the benefits of both methods by predicting\nthe next step future and simultaneously predicting partially noised further\nfuture steps at the same time. We show that such model is efficient compared to\ndiffusion model based AR, achieving a beneficial compromise between reactivity\nand computational efficiency.\n","authors":["Yunpeng Liu","Matthew Niedoba","William Harvey","Adam Scibior","Berend Zwartsenberg","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2502.09587v1.pdf","comment":"Accepted to Workshop on Machine Learning for Autonomous Driving at\n  AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09583v1","updated":"2025-02-13T18:41:55Z","published":"2025-02-13T18:41:55Z","title":"Learning to Coordinate with Experts","summary":"  When deployed in dynamic environments, AI agents will inevitably encounter\nchallenges that exceed their individual capabilities. Leveraging assistance\nfrom expert agents-whether human or AI-can significantly enhance safety and\nperformance in such situations. However, querying experts is often costly,\nnecessitating the development of agents that can efficiently request and\nutilize expert guidance. In this paper, we introduce a fundamental coordination\nproblem called Learning to Yield and Request Control (YRC), where the objective\nis to learn a strategy that determines when to act autonomously and when to\nseek expert assistance. We consider a challenging practical setting in which an\nagent does not interact with experts during training but must adapt to novel\nenvironmental changes and expert interventions at test time. To facilitate\nempirical research, we introduce YRC-Bench, an open-source benchmark featuring\ndiverse domains. YRC-Bench provides a standardized Gym-like API, simulated\nexperts, evaluation pipeline, and implementation of competitive baselines.\nTowards tackling the YRC problem, we propose a novel validation approach and\ninvestigate the performance of various learning methods across diverse\nenvironments, yielding insights that can guide future research.\n","authors":["Mohamad H. Danesh","Tu Trinh","Benjamin Plaut","Nguyen X. Khanh"],"pdf_url":"https://arxiv.org/pdf/2502.09583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20092v2","updated":"2025-02-13T18:38:13Z","published":"2024-10-26T06:06:08Z","title":"OGBench: Benchmarking Offline Goal-Conditioned RL","summary":"  Offline goal-conditioned reinforcement learning (GCRL) is a major problem in\nreinforcement learning (RL) because it provides a simple, unsupervised, and\ndomain-agnostic way to acquire diverse behaviors and representations from\nunlabeled data without rewards. Despite the importance of this setting, we lack\na standard benchmark that can systematically evaluate the capabilities of\noffline GCRL algorithms. In this work, we propose OGBench, a new, high-quality\nbenchmark for algorithms research in offline goal-conditioned RL. OGBench\nconsists of 8 types of environments, 85 datasets, and reference implementations\nof 6 representative offline GCRL algorithms. We have designed these challenging\nand realistic environments and datasets to directly probe different\ncapabilities of algorithms, such as stitching, long-horizon reasoning, and the\nability to handle high-dimensional inputs and stochasticity. While\nrepresentative algorithms may rank similarly on prior benchmarks, our\nexperiments reveal stark strengths and weaknesses in these different\ncapabilities, providing a strong foundation for building new algorithms.\nProject page: https://seohong.me/projects/ogbench\n","authors":["Seohong Park","Kevin Frans","Benjamin Eysenbach","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.20092v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09571v1","updated":"2025-02-13T18:29:48Z","published":"2025-02-13T18:29:48Z","title":"DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra","summary":"  Mass spectrometry plays a fundamental role in elucidating the structures of\nunknown molecules and subsequent scientific discoveries. One formulation of the\nstructure elucidation task is the conditional $\\textit{de novo}$ generation of\nmolecular structure given a mass spectrum. Toward a more accurate and efficient\nscientific discovery pipeline for small molecules, we present DiffMS, a\nformula-restricted encoder-decoder generative network that achieves\nstate-of-the-art performance on this task. The encoder utilizes a transformer\narchitecture and models mass spectra domain knowledge such as peak formulae and\nneutral losses, and the decoder is a discrete graph diffusion model restricted\nby the heavy-atom composition of a known chemical formula. To develop a robust\ndecoder that bridges latent embeddings and molecular structures, we pretrain\nthe diffusion decoder with fingerprint-structure pairs, which are available in\nvirtually infinite quantities, compared to structure-spectrum pairs that number\nin the tens of thousands. Extensive experiments on established benchmarks show\nthat DiffMS outperforms existing models on $\\textit{de novo}$ molecule\ngeneration. We provide several ablations to demonstrate the effectiveness of\nour diffusion and pretraining approaches and show consistent performance\nscaling with increasing pretraining dataset size. DiffMS code is publicly\navailable at https://github.com/coleygroup/DiffMS.\n","authors":["Montgomery Bohde","Mrunali Manjrekar","Runzhong Wang","Shuiwang Ji","Connor W. Coley"],"pdf_url":"https://arxiv.org/pdf/2502.09571v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09570v1","updated":"2025-02-13T18:28:17Z","published":"2025-02-13T18:28:17Z","title":"Enhancing the Utility of Higher-Order Information in Relational Learning","summary":"  Higher-order information is crucial for relational learning in many domains\nwhere relationships extend beyond pairwise interactions. Hypergraphs provide a\nnatural framework for modeling such relationships, which has motivated recent\nextensions of graph neural net- work architectures to hypergraphs. However,\ncomparisons between hypergraph architectures and standard graph-level models\nremain limited. In this work, we systematically evaluate a selection of\nhypergraph-level and graph-level architectures, to determine their\neffectiveness in leveraging higher-order information in relational learning.\nOur results show that graph-level architectures applied to hypergraph\nexpansions often outperform hypergraph- level ones, even on inputs that are\nnaturally parametrized as hypergraphs. As an alternative approach for\nleveraging higher-order information, we propose hypergraph-level encodings\nbased on classical hypergraph characteristics. While these encodings do not\nsignificantly improve hypergraph architectures, they yield substantial\nperformance gains when combined with graph-level models. Our theoretical\nanalysis shows that hypergraph-level encodings provably increase the\nrepresentational power of message-passing graph neural networks beyond that of\ntheir graph-level counterparts.\n","authors":["Raphael Pellegrin","Lukas Fesser","Melanie Weber"],"pdf_url":"https://arxiv.org/pdf/2502.09570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08593v2","updated":"2025-02-13T18:24:40Z","published":"2025-02-12T17:32:23Z","title":"Toward Universal Laws of Outlier Propagation","summary":"  We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.\n","authors":["Aram Ebtekar","Yuhao Wang","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10238v2","updated":"2025-02-13T18:22:34Z","published":"2024-07-14T15:11:13Z","title":"Asymptotic Normality of Generalized Low-Rank Matrix Sensing via\n  Riemannian Geometry","summary":"  We prove an asymptotic normality guarantee for generalized low-rank matrix\nsensing -- i.e., matrix sensing under a general convex loss $\\bar\\ell(\\langle\nX,M\\rangle,y^*)$, where $M\\in\\mathbb{R}^{d\\times d}$ is the unknown rank-$k$\nmatrix, $X$ is a measurement matrix, and $y^*$ is the corresponding\nmeasurement. Our analysis relies on tools from Riemannian geometry to handle\ndegeneracy of the Hessian of the loss due to rotational symmetry in the\nparameter space. In particular, we parameterize the manifold of low-rank\nmatrices by $\\bar\\theta\\bar\\theta^\\top$, where\n$\\bar\\theta\\in\\mathbb{R}^{d\\times k}$. Then, assuming the minimizer of the\nempirical loss $\\bar\\theta^0\\in\\mathbb{R}^{d\\times k}$ is in a constant size\nball around the true parameters $\\bar\\theta^*$, we prove\n$\\sqrt{n}(\\phi^0-\\phi^*)\\xrightarrow{D}N(0,(H^*)^{-1})$ as $n\\to\\infty$, where\n$\\phi^0$ and $\\phi^*$ are representations of $\\bar\\theta^*$ and $\\bar\\theta^0$\nin the horizontal space of the Riemannian quotient manifold\n$\\mathbb{R}^{d\\times k}/\\text{O}(k)$, and $H^*$ is the Hessian of the true loss\nin the same representation.\n","authors":["Osbert Bastani"],"pdf_url":"https://arxiv.org/pdf/2407.10238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09566v1","updated":"2025-02-13T18:21:15Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09564v1","updated":"2025-02-13T18:17:03Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data which, whenever containing strong\nspurious correlations between specific attributes and target labels, can result\nin unrecoverable biases in model predictions. Tackling these biases is crucial\nin improving model generalization and trust, especially in real-world\nscenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting\nas a plug-in for common methods in model debiasing while exploiting the\ninherent bias-learning tendency of diffusion models. Our approach leverages\nconditional diffusion models to generate synthetic bias-aligned images, used to\ntrain a bias amplifier model, to be further employed as an auxiliary method in\ndifferent unsupervised debiasing approaches. Our proposed method, which also\ntackles the common issue of training set memorization typical of this type of\ntech- niques, beats current state-of-the-art in multiple benchmark datasets by\nsignificant margins, demonstrating its potential as a versatile and effective\ntool for tackling dataset bias in deep learning applications.\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v1.pdf","comment":"29 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2502.07864v2","updated":"2025-02-13T18:07:04Z","published":"2025-02-11T18:20:18Z","title":"TransMLA: Multi-Head Latent Attention Is All You Need","summary":"  Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.\n","authors":["Fanxu Meng","Zengwei Yao","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07864v2.pdf","comment":"https://github.com/fxmeng/TransMLA"},{"id":"http://arxiv.org/abs/2502.09553v1","updated":"2025-02-13T18:05:12Z","published":"2025-02-13T18:05:12Z","title":"SyntheticPop: Attacking Speaker Verification Systems With Synthetic\n  VoicePops","summary":"  Voice Authentication (VA), also known as Automatic Speaker Verification\n(ASV), is a widely adopted authentication method, particularly in automated\nsystems like banking services, where it serves as a secondary layer of user\nauthentication. Despite its popularity, VA systems are vulnerable to various\nattacks, including replay, impersonation, and the emerging threat of deepfake\naudio that mimics the voice of legitimate users. To mitigate these risks,\nseveral defense mechanisms have been proposed. One such solution, Voice Pops,\naims to distinguish an individual's unique phoneme pronunciations during the\nenrollment process. While promising, the effectiveness of VA+VoicePop against a\nbroader range of attacks, particularly logical or adversarial attacks, remains\ninsufficiently explored. We propose a novel attack method, which we refer to as\nSyntheticPop, designed to target the phoneme recognition capabilities of the\nVA+VoicePop system. The SyntheticPop attack involves embedding synthetic \"pop\"\nnoises into spoofed audio samples, significantly degrading the model's\nperformance. We achieve an attack success rate of over 95% while poisoning 20%\nof the training dataset. Our experiments demonstrate that VA+VoicePop achieves\n69% accuracy under normal conditions, 37% accuracy when subjected to a baseline\nlabel flipping attack, and just 14% accuracy under our proposed SyntheticPop\nattack, emphasizing the effectiveness of our method.\n","authors":["Eshaq Jamdar","Amith Kamath Belman"],"pdf_url":"https://arxiv.org/pdf/2502.09553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18970v3","updated":"2025-02-13T17:57:28Z","published":"2024-10-24T17:59:16Z","title":"WASP: A Weight-Space Approach to Detecting Learned Spuriousness","summary":"  It is of crucial importance to train machine learning models such that they\nclearly understand what defines each class in a given task. Though there is a\nsum of works dedicated to identifying the spurious correlations featured by a\ndataset that may impact the model's understanding of the classes, all current\napproaches rely solely on data or error analysis. That is, they cannot point\nout spurious correlations learned by the model that are not already pointed out\nby the counterexamples featured in the validation or training sets. We propose\na method that transcends this limitation, switching the focus from analyzing a\nmodel's predictions to analyzing the model's weights, the mechanism behind the\nmaking of the decisions, which proves to be more insightful. Our proposed\nWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing the\nweights of foundation models as they drift towards capturing various (spurious)\ncorrelations while being fine-tuned on a given dataset. We demonstrate that\ndifferent from previous works, our method (i) can expose spurious correlations\nfeatured by a dataset even when they are not exposed by training or validation\ncounterexamples, (ii) it works for multiple modalities such as image and text,\nand (iii) it can uncover previously untapped spurious correlations learned by\nImageNet-1k classifiers.\n","authors":["Cristian Daniel Păduraru","Antonio Bărbălau","Radu Filipescu","Andrei Liviu Nicolicioiu","Elena Burceanu"],"pdf_url":"https://arxiv.org/pdf/2410.18970v3.pdf","comment":"8 pages, 4 figures, 6 tables, under review"},{"id":"http://arxiv.org/abs/2502.09534v1","updated":"2025-02-13T17:50:27Z","published":"2025-02-13T17:50:27Z","title":"Fast Tensor Completion via Approximate Richardson Iteration","summary":"  We study tensor completion (TC) through the lens of low-rank tensor\ndecomposition (TD). Many TD algorithms use fast alternating minimization\nmethods, which solve highly structured linear regression problems at each step\n(e.g., for CP, Tucker, and tensor-train decompositions). However, such\nalgebraic structure is lost in TC regression problems, making direct extensions\nunclear. To address this, we propose a lifting approach that approximately\nsolves TC regression problems using structured TD regression algorithms as\nblackbox subroutines, enabling sublinear-time methods. We theoretically analyze\nthe convergence rate of our approximate Richardson iteration based algorithm,\nand we demonstrate on real-world tensors that its running time can be 100x\nfaster than direct methods for CP completion.\n","authors":["Mehrdad Ghadiri","Matthew Fahrbach","Yunbum Kook","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2502.09534v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.09525v1","updated":"2025-02-13T17:37:42Z","published":"2025-02-13T17:37:42Z","title":"Robust Learning of Multi-index Models via Iterative Subspace\n  Approximation","summary":"  We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.\n","authors":["Ilias Diakonikolas","Giannis Iakovidis","Daniel M. Kane","Nikos Zarifis"],"pdf_url":"https://arxiv.org/pdf/2502.09525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09511v1","updated":"2025-02-13T17:22:50Z","published":"2025-02-13T17:22:50Z","title":"Diffusion Models for Molecules: A Survey of Methods and Tasks","summary":"  Generative tasks about molecules, including but not limited to molecule\ngeneration, are crucial for drug discovery and material design, and have\nconsistently attracted significant attention. In recent years, diffusion models\nhave emerged as an impressive class of deep generative models, sparking\nextensive research and leading to numerous studies on their application to\nmolecular generative tasks. Despite the proliferation of related work, there\nremains a notable lack of up-to-date and systematic surveys in this area.\nParticularly, due to the diversity of diffusion model formulations, molecular\ndata modalities, and generative task types, the research landscape is\nchallenging to navigate, hindering understanding and limiting the area's\ngrowth. To address this, this paper conducts a comprehensive survey of\ndiffusion model-based molecular generative methods. We systematically review\nthe research from the perspectives of methodological formulations, data\nmodalities, and task types, offering a novel taxonomy. This survey aims to\nfacilitate understanding and further flourishing development in this area. The\nrelevant papers are summarized at:\nhttps://github.com/AzureLeon1/awesome-molecular-diffusion-models.\n","authors":["Liang Wang","Chao Song","Zhiyuan Liu","Yu Rong","Qiang Liu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09509v1","updated":"2025-02-13T17:21:51Z","published":"2025-02-13T17:21:51Z","title":"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling","summary":"  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n","authors":["Theodoros Kouzelis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2502.09509v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09507v1","updated":"2025-02-13T17:21:37Z","published":"2025-02-13T17:21:37Z","title":"When and How Does CLIP Enable Domain and Compositional Generalization?","summary":"  The remarkable generalization performance of contrastive vision-language\nmodels like CLIP is often attributed to the diversity of their training\ndistributions. However, key questions remain unanswered: Can CLIP generalize to\nan entirely unseen domain when trained on a diverse mixture of domains (domain\ngeneralization)? Can it generalize to unseen classes within partially seen\ndomains (compositional generalization)? What factors affect such\ngeneralization? To answer these questions, we trained CLIP models on\nsystematically constructed training distributions with controlled domain\ndiversity and object class exposure. Our experiments show that domain diversity\nis essential for both domain and compositional generalization, yet\ncompositional generalization can be surprisingly weaker than domain\ngeneralization when the training distribution contains a suboptimal subset of\nthe test domain. Through data-centric and mechanistic analyses, we find that\nsuccessful generalization requires learning of shared representations already\nin intermediate layers and shared circuitry.\n","authors":["Elias Kempf","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2502.09507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09503v1","updated":"2025-02-13T17:15:26Z","published":"2025-02-13T17:15:26Z","title":"AttentionSmithy: A Modular Framework for Rapid Transformer Development\n  and Customization","summary":"  Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.\n","authors":["Caleb Cranney","Jesse G. Meyer"],"pdf_url":"https://arxiv.org/pdf/2502.09503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09502v1","updated":"2025-02-13T17:14:18Z","published":"2025-02-13T17:14:18Z","title":"Scalable First-order Method for Certifying Optimal k-Sparse GLMs","summary":"  This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.\n","authors":["Jiachang Liu","Soroosh Shafiee","Andrea Lodi"],"pdf_url":"https://arxiv.org/pdf/2502.09502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09500v1","updated":"2025-02-13T17:10:43Z","published":"2025-02-13T17:10:43Z","title":"Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting","summary":"  Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\n\\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.\n","authors":["Nicholas Dronen","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2502.09500v1.pdf","comment":"16 pages, 6 figures; code is available at\n  https://github.com/amazon-science/eideticnet-training"},{"id":"http://arxiv.org/abs/2501.14346v2","updated":"2025-02-13T17:03:04Z","published":"2025-01-24T09:17:57Z","title":"HorNets: Learning from Discrete and Continuous Signals with Routing\n  Neural Networks","summary":"  Construction of neural network architectures suitable for learning from both\ncontinuous and discrete tabular data is a challenging research endeavor.\nContemporary high-dimensional tabular data sets are often characterized by a\nrelatively small instance count, requiring data-efficient learning. We propose\nHorNets (Horn Networks), a neural network architecture with state-of-the-art\nperformance on synthetic and real-life data sets from scarce-data tabular\ndomains. HorNets are based on a clipped polynomial-like activation function,\nextended by a custom discrete-continuous routing mechanism that decides which\npart of the neural network to optimize based on the input's cardinality. By\nexplicitly modeling parts of the feature combination space or combining whole\nspace in a linear attention-like manner, HorNets dynamically decide which mode\nof operation is the most suitable for a given piece of data with no explicit\nsupervision. This architecture is one of the few approaches that reliably\nretrieves logical clauses (including noisy XNOR) and achieves state-of-the-art\nclassification performance on 14 real-life biomedical high-dimensional data\nsets. HorNets are made freely available under a permissive license alongside a\nsynthetic generator of categorical benchmarks.\n","authors":["Boshko Koloski","Nada Lavrač","Blaž Škrlj"],"pdf_url":"https://arxiv.org/pdf/2501.14346v2.pdf","comment":"Accepted to the ACML conference journal track with the Machine\n  Learning journal. The first and the last authors share an equal contribution"},{"id":"http://arxiv.org/abs/2502.09496v1","updated":"2025-02-13T17:03:03Z","published":"2025-02-13T17:03:03Z","title":"On Agnostic PAC Learning in the Small Error Regime","summary":"  Binary classification in the classic PAC model exhibits a curious phenomenon:\nEmpirical Risk Minimization (ERM) learners are suboptimal in the realizable\ncase yet optimal in the agnostic case. Roughly speaking, this owes itself to\nthe fact that non-realizable distributions $\\mathcal{D}$ are simply more\ndifficult to learn than realizable distributions -- even when one discounts a\nlearner's error by $\\mathrm{err}(h^*_{\\mathcal{D}})$, the error of the best\nhypothesis in $\\mathcal{H}$ for $\\mathcal{D}$. Thus, optimal agnostic learners\nare permitted to incur excess error on (easier-to-learn) distributions\n$\\mathcal{D}$ for which $\\tau = \\mathrm{err}(h^*_{\\mathcal{D}})$ is small.\n  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this\nshortcoming by including $\\tau$ itself as a parameter in the agnostic error\nterm. In this more fine-grained model, they demonstrate tightness of the error\nlower bound $\\tau + \\Omega \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} +\n\\frac{d + \\log(1 / \\delta)}{m} \\right)$ in a regime where $\\tau > d/m$, and\nleave open the question of whether there may be a higher lower bound when $\\tau\n\\approx d/m$, with $d$ denoting $\\mathrm{VC}(\\mathcal{H})$. In this work, we\nresolve this question by exhibiting a learner which achieves error $c \\cdot\n\\tau + O \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} + \\frac{d + \\log(1\n/ \\delta)}{m} \\right)$ for a constant $c \\leq 2.1$, thus matching the lower\nbound when $\\tau \\approx d/m$. Further, our learner is computationally\nefficient and is based upon careful aggregations of ERM classifiers, making\nprogress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS\n`24). We leave open the interesting question of whether our approach can be\nrefined to lower the constant from 2.1 to 1, which would completely settle the\ncomplexity of agnostic learning.\n","authors":["Julian Asilis","Mikael Møller Høgsgaard","Grigoris Velegkas"],"pdf_url":"https://arxiv.org/pdf/2502.09496v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2502.09495v1","updated":"2025-02-13T17:01:45Z","published":"2025-02-13T17:01:45Z","title":"Cracking the Code: Enhancing Development finance understanding with\n  artificial intelligence","summary":"  Analyzing development projects is crucial for understanding donors aid\nstrategies, recipients priorities, and to assess development finance capacity\nto adress development issues by on-the-ground actions. In this area, the\nOrganisation for Economic Co-operation and Developments (OECD) Creditor\nReporting System (CRS) dataset is a reference data source. This dataset\nprovides a vast collection of project narratives from various sectors\n(approximately 5 million projects). While the OECD CRS provides a rich source\nof information on development strategies, it falls short in informing project\npurposes due to its reporting process based on donors self-declared main\nobjectives and pre-defined industrial sectors. This research employs a novel\napproach that combines Machine Learning (ML) techniques, specifically Natural\nLanguage Processing (NLP), an innovative Python topic modeling technique called\nBERTopic, to categorise (cluster) and label development projects based on their\nnarrative descriptions. By revealing existing yet hidden topics of development\nfinance, this application of artificial intelligence enables a better\nunderstanding of donor priorities and overall development funding and provides\nmethods to analyse public and private projects narratives.\n","authors":["Pierre Beaucoral"],"pdf_url":"https://arxiv.org/pdf/2502.09495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09494v1","updated":"2025-02-13T17:00:11Z","published":"2025-02-13T17:00:11Z","title":"Communicating Likelihoods with Normalising Flows","summary":"  We present a machine-learning-based workflow to model an unbinned likelihood\nfrom its samples. A key advancement over existing approaches is the validation\nof the learned likelihood using rigorous statistical tests of the joint\ndistribution, such as the Kolmogorov-Smirnov test of the joint distribution.\nOur method enables the reliable communication of experimental and\nphenomenological likelihoods for subsequent analyses. We demonstrate its\neffectiveness through three case studies in high-energy physics. To support\nbroader adoption, we provide an open-source reference implementation, nabu.\n","authors":["Jack Y. Araz","Anja Beck","Méril Reboud","Michael Spannowsky","Danny van Dyk"],"pdf_url":"https://arxiv.org/pdf/2502.09494v1.pdf","comment":"4 pages + references, 1 figure"},{"id":"http://arxiv.org/abs/2502.09490v1","updated":"2025-02-13T16:57:07Z","published":"2025-02-13T16:57:07Z","title":"Inverse Design with Dynamic Mode Decomposition","summary":"  We introduce a computationally efficient method for the automation of inverse\ndesign in science and engineering. Based on simple least-square regression, the\nunderlying dynamic mode decomposition algorithm can be used to construct a\nlow-rank subspace spanning multiple experiments in parameter space. The\nproposed inverse design dynamic mode composition (ID-DMD) algorithm leverages\nthe computed low-dimensional subspace to enable fast digital design and\noptimization on laptop-level computing, including the potential to prescribe\nthe dynamics themselves. Moreover, the method is robust to noise, physically\ninterpretable, and can provide uncertainty quantification metrics. The\narchitecture can also efficiently scale to large-scale design problems using\nrandomized algorithms in the ID-DMD. The simplicity of the method and its\nimplementation are highly attractive in practice, and the ID-DMD has been\ndemonstrated to be an order of magnitude more accurate than competing methods\nwhile simultaneously being 3-5 orders faster on challenging engineering design\nproblems ranging from structural vibrations to fluid dynamics. Due to its\nspeed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with\nother leading machine learning methods represents a significant advancement in\ndata-driven methods for inverse design and optimization, promising a paradigm\nshift in how to approach inverse design in practice.\n","authors":["Yunpeng Zhu","Liangliang Cheng","Anping Jing","Hanyu Huo","Ziqiang Lang","Bo Zhang","J. Nathan Kutz"],"pdf_url":"https://arxiv.org/pdf/2502.09490v1.pdf","comment":"29 pages, 19 figures"},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2410.13879v2","updated":"2025-02-13T16:43:47Z","published":"2024-10-03T00:48:07Z","title":"Mixed-curvature decision trees and random forests","summary":"  Decision trees (DTs) and their random forest (RF) extensions are workhorses\nof classification and regression in Euclidean spaces. However, algorithms for\nlearning in non-Euclidean spaces are still limited. We extend DT and RF\nalgorithms to product manifolds: Cartesian products of several hyperbolic,\nhyperspherical, or Euclidean components. Such manifolds handle heterogeneous\ncurvature while still factorizing neatly into simpler components, making them\ncompelling embedding spaces for complex datasets. Our novel angular\nreformulation of DTs respects the geometry of the product manifold, yielding\nsplits that are geodesically convex, maximum-margin, and composable. In the\nspecial cases of single-component manifolds, our method simplifies to its\nEuclidean or hyperbolic counterparts, or introduces hyperspherical DT\nalgorithms, depending on the curvature. We benchmark our method on various\nclassification, regression, and link prediction tasks on synthetic data, graph\nembeddings, mixed-curvature variational autoencoder latent spaces, and\nempirical data. Compared to 7 other classifiers, product RFs ranked first on 25\nout of 57 benchmarks, and placed in the top 2 for 46 out of 57. This highlights\nthe value of product RFs as straightforward yet powerful new tools for data\nanalysis in product manifolds. Code for our paper is available at\nhttps://github.com/pchlenski/manify.\n","authors":["Philippe Chlenski","Quentin Chu","Raiyan R. Khan","Kaizhu Du","Antonio Khalil Moretti","Itsik Pe'er"],"pdf_url":"https://arxiv.org/pdf/2410.13879v2.pdf","comment":"27 pages, 11 figures. Submitted to ICML 2025"},{"id":"http://arxiv.org/abs/2502.09479v1","updated":"2025-02-13T16:43:32Z","published":"2025-02-13T16:43:32Z","title":"Assessing Generative AI value in a public sector context: evidence from\n  a field experiment","summary":"  The emergence of Generative AI (Gen AI) has motivated an interest in\nunderstanding how it could be used to enhance productivity across various\ntasks. We add to research results for the performance impact of Gen AI on\ncomplex knowledge-based tasks in a public sector setting. In a pre-registered\nexperiment, after establishing a baseline level of performance, we find mixed\nevidence for two types of composite tasks related to document understanding and\ndata analysis. For the Documents task, the treatment group using Gen AI had a\n17% improvement in answer quality scores (as judged by human evaluators) and a\n34% improvement in task completion time compared to a control group. For the\nData task, we find the Gen AI treatment group experienced a 12% reduction in\nquality scores and no significant difference in mean completion time compared\nto the control group. These results suggest that the benefits of Gen AI may be\ntask and potentially respondent dependent. We also discuss field notes and\nlessons learned, as well as supplementary insights from a post-trial survey and\nfeedback workshop with participants.\n","authors":["Trevor Fitzpatrick","Seamus Kelly","Patrick Carey","David Walsh","Ruairi Nugent"],"pdf_url":"https://arxiv.org/pdf/2502.09479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09477v1","updated":"2025-02-13T16:41:44Z","published":"2025-02-13T16:41:44Z","title":"DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation\n  Networks for Quantitative Nanomaterial Analysis through Differentiable\n  Rendering and Generative Modelling","summary":"  Nanomaterials exhibit distinctive properties governed by parameters such as\nsize, shape, and surface characteristics, which critically influence their\napplications and interactions across technological, biological, and\nenvironmental contexts. Accurate quantification and understanding of these\nmaterials are essential for advancing research and innovation. In this regard,\ndeep learning segmentation networks have emerged as powerful tools that enable\nautomated insights and replace subjective methods with precise quantitative\nanalysis. However, their efficacy depends on representative annotated datasets,\nwhich are challenging to obtain due to the costly imaging of nanoparticles and\nthe labor-intensive nature of manual annotations. To overcome these\nlimitations, we introduce DiffRenderGAN, a novel generative model designed to\nproduce annotated synthetic data. By integrating a differentiable renderer into\na Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes\ntextural rendering parameters to generate realistic, annotated nanoparticle\nimages from non-annotated real microscopy images. This approach reduces the\nneed for manual intervention and enhances segmentation performance compared to\nexisting synthetic data methods by generating diverse and realistic data.\nTested on multiple ion and electron microscopy cases, including titanium\ndioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),\nDiffRenderGAN bridges the gap between synthetic and real data, advancing the\nquantification and understanding of complex nanomaterial systems.\n","authors":["Dennis Possart","Leonid Mill","Florian Vollnhals","Tor Hildebrand","Peter Suter","Mathis Hoffmann","Jonas Utz","Daniel Augsburger","Mareike Thies","Mingxuan Wu","Fabian Wagner","George Sarau","Silke Christiansen","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2502.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16333v2","updated":"2025-02-13T16:41:13Z","published":"2024-10-19T15:42:49Z","title":"Conformal Predictive Portfolio Selection","summary":"  This study examines portfolio selection using predictive models for portfolio\nreturns. Portfolio selection is a fundamental task in finance, and a variety of\nmethods have been developed to achieve this goal. For instance, the\nmean-variance approach constructs portfolios by balancing the trade-off between\nthe mean and variance of asset returns, while the quantile-based approach\noptimizes portfolios by considering tail risk. These methods often depend on\ndistributional information estimated from historical data using predictive\nmodels, each of which carries its own uncertainty. To address this, we propose\na framework for predictive portfolio selection via conformal prediction ,\ncalled \\emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach\nforecasts future portfolio returns, computes the corresponding prediction\nintervals, and selects the portfolio of interest based on these intervals. The\nframework is flexible and can accommodate a wide range of predictive models,\nincluding autoregressive (AR) models, random forests, and neural networks. We\ndemonstrate the effectiveness of the CPPS framework by applying it to an AR\nmodel and validate its performance through empirical studies, showing that it\ndelivers superior returns compared to simpler strategies.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2410.16333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09473v1","updated":"2025-02-13T16:36:25Z","published":"2025-02-13T16:36:25Z","title":"Learning to Predict Global Atrial Fibrillation Dynamics from Sparse\n  Measurements","summary":"  Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all\ntreatment with limited success in persistent AF. This may be due to our\ninability to map the dynamics of AF with the limited resolution and coverage\nprovided by sequential contact mapping catheters, preventing effective patient\nphenotyping for personalised, targeted ablation. Here we introduce FibMap, a\ngraph recurrent neural network model that reconstructs global AF dynamics from\nsparse measurements. Trained and validated on 51 non-contact whole atria\nrecordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,\nachieving a 210% lower mean absolute error and an order of magnitude higher\nperformance in tracking phase singularities compared to baseline methods.\nClinical utility of FibMap is demonstrated on real-world contact mapping\nrecordings, achieving reconstruction fidelity comparable to non-contact\nmapping. FibMap's state-spaces and patient-specific parameters offer insights\nfor electrophenotyping AF. Integrating FibMap into clinical practice could\nenable personalised AF care and improve outcomes.\n","authors":["Alexander Jenkins","Andrea Cini","Joseph Barker","Alexander Sharp","Arunashis Sau","Varun Valentine","Srushti Valasang","Xinyang Li","Tom Wong","Timothy Betts","Danilo Mandic","Cesare Alippi","Fu Siong Ng"],"pdf_url":"https://arxiv.org/pdf/2502.09473v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.20440v2","updated":"2025-02-13T16:35:17Z","published":"2024-09-30T16:00:23Z","title":"Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits","summary":"  Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret\nfor adversarial as well as stochastic bandit problems and allow for a\nstreamlined analysis. Nonetheless, FTRL algorithms require the solution of an\noptimization problem in every iteration and are thus computationally\nchallenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve\ncomputational efficiency by perturbing the estimates of the rewards of the\narms, but their regret analysis is cumbersome. We propose a new FTPL algorithm\nthat generates optimal policies for both adversarial and stochastic multi-armed\nbandits. Like FTRL, our algorithm admits a unified regret analysis, and similar\nto FTPL, it offers low computational costs. Unlike existing FTPL algorithms\nthat rely on independent additive disturbances governed by a \\textit{known}\ndistribution, we allow for disturbances governed by an \\textit{ambiguous}\ndistribution that is only known to belong to a given set and propose a\nprinciple of optimism in the face of ambiguity. Consequently, our framework\ngeneralizes existing FTPL algorithms. It also encapsulates a broad range of\nFTRL methods as special cases, including several optimal ones, which appears to\nbe impossible with current FTPL methods. Finally, we use techniques from\ndiscrete choice theory to devise an efficient bisection algorithm for computing\nthe optimistic arm sampling probabilities. This algorithm is up to $10^4$ times\nfaster than standard FTRL algorithms that solve an optimization problem in\nevery iteration. Our results not only settle existing conjectures but also\nprovide new insights into the impact of perturbations by mapping FTRL to FTPL.\n","authors":["Mengmeng Li","Daniel Kuhn","Bahar Taşkesen"],"pdf_url":"https://arxiv.org/pdf/2409.20440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17163v2","updated":"2025-02-13T16:32:55Z","published":"2024-05-27T13:36:50Z","title":"Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep\n  Graph Networks","summary":"  The dynamics of information diffusion within graphs is a critical open issue\nthat heavily influences graph representation learning, especially when\nconsidering long-range propagation. This calls for principled approaches that\ncontrol and regulate the degree of propagation and dissipation of information\nthroughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian\nDeep Graph Networks, a novel framework that models neural information flow in\ngraphs by building on the laws of conservation of Hamiltonian dynamical\nsystems. We reconcile under a single theoretical and practical framework both\nnon-dissipative long-range propagation and non-conservative behaviors,\nintroducing tools from mechanical systems to gauge the equilibrium between the\ntwo components. Our approach can be applied to general message-passing\narchitectures, and it provides theoretical guarantees on information\nconservation in time. Empirical results prove the effectiveness of our\nport-Hamiltonian scheme in pushing simple graph convolutional architectures to\nstate-of-the-art performance in long-range benchmarks.\n","authors":["Simon Heilig","Alessio Gravina","Alessandro Trenta","Claudio Gallicchio","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2405.17163v2.pdf","comment":"Accepted at ICLR 2025 (https://openreview.net/forum?id=03EkqSCKuO)"},{"id":"http://arxiv.org/abs/2411.03263v2","updated":"2025-02-13T16:28:07Z","published":"2024-11-05T17:02:29Z","title":"Proxy-informed Bayesian transfer learning with unknown sources","summary":"  Generalization outside the scope of one's training data requires leveraging\nprior knowledge about the effects that transfer, and the effects that don't,\nbetween different data sources. Transfer learning is a framework for specifying\nand refining this knowledge about sets of source (training) and target\n(prediction) data. A challenging open problem is addressing the empirical\nphenomenon of negative transfer, whereby the transfer learner performs worse on\nthe target data after taking the source data into account than before. We first\nintroduce a Bayesian perspective on negative transfer, and then a method to\naddress it. The key insight from our formulation is that negative transfer can\nstem from misspecified prior information about non-transferable causes of the\nsource data. Our proposed method, proxy-informed robust method for\nprobabilistic transfer learning (PROMPT), does not require prior knowledge of\nthe source data (the data sources may be \"unknown\"). PROMPT is thus applicable\nwhen differences between tasks are unobserved, such as in the presence of\nlatent confounders. Moreover, the learner need not have access to observations\nin the target task (cannot \"fine-tune\"), and instead makes use of proxy\n(indirect) information. Our theoretical results show that the threat of\nnegative transfer does not depend on the informativeness of the proxy\ninformation, highlighting the usefulness of PROMPT in cases where only noisy\nindirect information, such as human feedback, is available.\n","authors":["Sabina J. Sloman","Julien Martinelli","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2411.03263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09445v1","updated":"2025-02-13T16:15:43Z","published":"2025-02-13T16:15:43Z","title":"A Differentiable Rank-Based Objective For Better Feature Learning","summary":"  In this paper, we leverage existing statistical methods to better understand\nfeature learning from data. We tackle this by modifying the model-free variable\nselection method, Feature Ordering by Conditional Independence (FOCI), which is\nintroduced in \\cite{azadkia2021simple}. While FOCI is based on a non-parametric\ncoefficient of conditional dependence, we introduce its parametric,\ndifferentiable approximation. With this approximate coefficient of correlation,\nwe present a new algorithm called difFOCI, which is applicable to a wider range\nof machine learning problems thanks to its differentiable nature and learnable\nparameters. We present difFOCI in three contexts: (1) as a variable selection\nmethod with baseline comparisons to FOCI, (2) as a trainable model parametrized\nwith a neural network, and (3) as a generic, widely applicable neural network\nregularizer, one that improves feature learning with better management of\nspurious correlations. We evaluate difFOCI on increasingly complex problems\nranging from basic variable selection in toy examples to saliency map\ncomparisons in convolutional networks. We then show how difFOCI can be\nincorporated in the context of fairness to facilitate classifications without\nrelying on sensitive data.\n","authors":["Krunoslav Lehman Pavasovic","David Lopez-Paz","Giulio Biroli","Levent Sagun"],"pdf_url":"https://arxiv.org/pdf/2502.09445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19082v2","updated":"2025-02-13T16:14:34Z","published":"2025-01-31T12:15:58Z","title":"A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration","summary":"  Distributed stochastic optimization algorithms can simultaneously process\nlarge-scale datasets, significantly accelerating model training. However, their\neffectiveness is often hindered by the sparsity of distributed networks and\ndata heterogeneity. In this paper, we propose a momentum-accelerated\ndistributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum\n(EDM), which mitigates the bias from data heterogeneity and incorporates\nmomentum techniques commonly used in deep learning to enhance convergence rate.\nOur theoretical analysis demonstrates that the EDM algorithm converges\nsub-linearly to the neighborhood of the optimal solution, the radius of which\nis irrespective of data heterogeneity, when applied to non-convex objective\nfunctions; under the Polyak-Lojasiewicz condition, which is a weaker assumption\nthan strong convexity, it converges linearly to the target region. Our analysis\ntechniques employed to handle momentum in complex distributed parameter update\nstructures yield a sufficiently tight convergence upper bound, offering a new\nperspective for the theoretical analysis of other momentum-based distributed\nalgorithms.\n","authors":["Yuchen Hu","Xi Chen","Weidong Liu","Xiaojun Mao"],"pdf_url":"https://arxiv.org/pdf/2501.19082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09443v1","updated":"2025-02-13T16:12:17Z","published":"2025-02-13T16:12:17Z","title":"Relational Conformal Prediction for Correlated Time Series","summary":"  We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.\n","authors":["Andrea Cini","Alexander Jenkins","Danilo Mandic","Cesare Alippi","Filippo Maria Bianchi"],"pdf_url":"https://arxiv.org/pdf/2502.09443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2410.11415v2","updated":"2025-02-13T16:02:42Z","published":"2024-10-15T09:02:55Z","title":"KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI","summary":"  A popular approach to neurosymbolic AI involves mapping logic formulas to\narithmetic circuits (computation graphs consisting of sums and products) and\npassing the outputs of a neural network through these circuits. This approach\nenforces symbolic constraints onto a neural network in a principled and\nend-to-end differentiable way. Unfortunately, arithmetic circuits are\nchallenging to run on modern AI accelerators as they exhibit a high degree of\nirregular sparsity. To address this limitation, we introduce knowledge layers\n(KLay), a new data structure to represent arithmetic circuits that can be\nefficiently parallelized on GPUs. Moreover, we contribute two algorithms used\nin the translation of traditional circuit representations to KLay and a further\nalgorithm that exploits parallelization opportunities during circuit\nevaluations. We empirically show that KLay achieves speedups of multiple orders\nof magnitude over the state of the art, thereby paving the way towards scaling\nneurosymbolic AI to larger real-world applications.\n","authors":["Jaron Maene","Vincent Derkinderen","Pedro Zuidberg Dos Martires"],"pdf_url":"https://arxiv.org/pdf/2410.11415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09432v1","updated":"2025-02-13T15:55:00Z","published":"2025-02-13T15:55:00Z","title":"Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes","summary":"  We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs.\n","authors":["Navdeep Kumar","Adarsh Gupta","Maxence Mohamed Elfatihi","Giorgia Ramponi","Kfir Yehuda Levy","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2502.09432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08097v3","updated":"2025-02-13T15:53:59Z","published":"2024-05-13T18:24:03Z","title":"A Galois theorem for machine learning: Functions on symmetric matrices\n  and point clouds via lightweight invariant features","summary":"  In this work, we present a mathematical formulation for machine learning of\n(1) functions on symmetric matrices that are invariant with respect to the\naction of permutations by conjugation, and (2) functions on point clouds that\nare invariant with respect to rotations, reflections, and permutations of the\npoints. To achieve this, we provide a general construction of generically\nseparating invariant features using ideas inspired by Galois theory. We\nconstruct $O(n^2)$ invariant features derived from generators for the field of\nrational functions on $n\\times n$ symmetric matrices that are invariant under\njoint permutations of rows and columns. We show that these invariant features\ncan separate all distinct orbits of symmetric matrices except for a measure\nzero set; such features can be used to universally approximate invariant\nfunctions on almost all weighted graphs. For point clouds in a fixed dimension,\nwe prove that the number of invariant features can be reduced, generically\nwithout losing expressivity, to $O(n)$, where $n$ is the number of points. We\ncombine these invariant features with DeepSets to learn functions on symmetric\nmatrices and point clouds with varying sizes. We empirically demonstrate the\nfeasibility of our approach on molecule property regression and point cloud\ndistance prediction.\n","authors":["Ben Blum-Smith","Ningyuan Huang","Marco Cuturi","Soledad Villar"],"pdf_url":"https://arxiv.org/pdf/2405.08097v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00315v3","updated":"2025-02-13T15:46:35Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.01706v2","updated":"2025-02-13T15:43:25Z","published":"2024-10-02T16:15:26Z","title":"Sable: a Performant, Efficient and Scalable Sequence Model for MARL","summary":"  As multi-agent reinforcement learning (MARL) progresses towards solving\nlarger and more complex problems, it becomes increasingly important that\nalgorithms exhibit the key properties of (1) strong performance, (2) memory\nefficiency and (3) scalability. In this work, we introduce Sable, a performant,\nmemory efficient and scalable sequence modeling approach to MARL. Sable works\nby adapting the retention mechanism in Retentive Networks to achieve\ncomputationally efficient processing of multi-agent observations with long\ncontext memory for temporal reasoning. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in a large number of diverse tasks\n(34 out of 45 tested). Furthermore, Sable maintains performance as we scale the\nnumber of agents, handling environments with more than a thousand agents while\nexhibiting a linear increase in memory usage. Finally, we conduct ablation\nstudies to isolate the source of Sable's performance gains and confirm its\nefficient computational memory usage.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09419v1","updated":"2025-02-13T15:42:44Z","published":"2025-02-13T15:42:44Z","title":"On multi-token prediction for efficient LLM inference","summary":"  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n","authors":["Somesh Mehra","Javier Alonso Garcia","Lukas Mauch"],"pdf_url":"https://arxiv.org/pdf/2502.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09417v1","updated":"2025-02-13T15:40:39Z","published":"2025-02-13T15:40:39Z","title":"A Survey of Reinforcement Learning for Optimization in Automation","summary":"  Reinforcement Learning (RL) has become a critical tool for optimization\nchallenges within automation, leading to significant advancements in several\nareas. This review article examines the current landscape of RL within\nautomation, with a particular focus on its roles in manufacturing, energy\nsystems, and robotics. It discusses state-of-the-art methods, major challenges,\nand upcoming avenues of research within each sector, highlighting RL's capacity\nto solve intricate optimization challenges. The paper reviews the advantages\nand constraints of RL-driven optimization methods in automation. It points out\nprevalent challenges encountered in RL optimization, including issues related\nto sample efficiency and scalability; safety and robustness; interpretability\nand trustworthiness; transfer learning and meta-learning; and real-world\ndeployment and integration. It further explores prospective strategies and\nfuture research pathways to navigate these challenges. Additionally, the survey\nincludes a comprehensive list of relevant research papers, making it an\nindispensable guide for scholars and practitioners keen on exploring this\ndomain.\n","authors":["Ahmad Farooq","Kamran Iqbal"],"pdf_url":"https://arxiv.org/pdf/2502.09417v1.pdf","comment":"8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International\n  Conference on Automation Science and Engineering (CASE) 2024"},{"id":"http://arxiv.org/abs/2410.08751v2","updated":"2025-02-13T15:36:19Z","published":"2024-10-11T12:10:51Z","title":"Zero-Shot Offline Imitation Learning via Optimal Transport","summary":"  Zero-shot imitation learning algorithms hold the promise of reproducing\nunseen behavior from as little as a single demonstration at test time. Existing\npractical approaches view the expert demonstration as a sequence of goals,\nenabling imitation with a high-level goal selector, and a low-level\ngoal-conditioned policy. However, this framework can suffer from myopic\nbehavior: the agent's immediate actions towards achieving individual goals may\nundermine long-term objectives. We introduce a novel method that mitigates this\nissue by directly optimizing the occupancy matching objective that is intrinsic\nto imitation learning. We propose to lift a goal-conditioned value function to\na distance between occupancies, which are in turn approximated via a learned\nworld model. The resulting method can learn from offline, suboptimal data, and\nis capable of non-myopic, zero-shot imitation, as we demonstrate in complex,\ncontinuous benchmarks.\n","authors":["Thomas Rupf","Marco Bagatella","Nico Gürtler","Jonas Frey","Georg Martius"],"pdf_url":"https://arxiv.org/pdf/2410.08751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2411.02280v2","updated":"2025-02-13T15:21:43Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.09396v1","updated":"2025-02-13T15:16:53Z","published":"2025-02-13T15:16:53Z","title":"A hierarchical approach for assessing the vulnerability of tree-based\n  classification models to membership inference attack","summary":"  Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy.\n","authors":["Richard J. Preen","Jim Smith"],"pdf_url":"https://arxiv.org/pdf/2502.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09395v1","updated":"2025-02-13T15:16:52Z","published":"2025-02-13T15:16:52Z","title":"Robot Pouring: Identifying Causes of Spillage and Selecting Alternative\n  Action Parameters Using Probabilistic Actual Causation","summary":"  In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a\nlarge variety of objects and goals. When confronted with an unexpected or\nunwanted outcome, we take corrective actions and try again until achieving the\ndesired result. The reasoning performed to identify a cause of the observed\noutcome and to select an appropriate corrective action is a crucial aspect of\nhuman reasoning for successful task execution. Central to this reasoning is the\nassumption that a factor is responsible for producing the observed outcome. In\nthis paper, we investigate the use of probabilistic actual causation to\ndetermine whether a factor is the cause of an observed undesired outcome.\nFurthermore, we show how the actual causation probabilities can be used to find\nalternative actions to change the outcome. We apply the probabilistic actual\ncausation analysis to a robot pouring task. When spillage occurs, the analysis\nindicates whether a task parameter is the cause and how it should be changed to\navoid spillage. The analysis requires a causal graph of the task and the\ncorresponding conditional probability distributions. To fulfill these\nrequirements, we perform a complete causal modeling procedure (i.e., task\nanalysis, definition of variables, determination of the causal graph structure,\nand estimation of conditional probability distributions) using data from a\nrealistic simulation of the robot pouring task, covering a large combinatorial\nspace of task parameters. Based on the results, we discuss the implications of\nthe variables' representation and how the alternative actions suggested by the\nactual causation analysis would compare to the alternative solutions proposed\nby a human observer. The practical use of the analysis of probabilistic actual\ncausation to select alternative action parameters is demonstrated.\n","authors":["Jaime Maldonado","Jonas Krumme","Christoph Zetzsche","Vanessa Didelez","Kerstin Schill"],"pdf_url":"https://arxiv.org/pdf/2502.09395v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.14441v2","updated":"2025-02-13T14:52:43Z","published":"2025-01-24T12:22:18Z","title":"Impact of Batch Normalization on Convolutional Network Representations","summary":"  Batch normalization (BatchNorm) is a popular layer normalization technique\nused when training deep neural networks. It has been shown to enhance the\ntraining speed and accuracy of deep learning models. However, the mechanics by\nwhich BatchNorm achieves these benefits is an active area of research, and\ndifferent perspectives have been proposed. In this paper, we investigate the\neffect of BatchNorm on the resulting hidden representations, that is, the\nvectors of activation values formed as samples are processed at each hidden\nlayer. Specifically, we consider the sparsity of these representations, as well\nas their implicit clustering -- the creation of groups of representations that\nare similar to some extent. We contrast image classification models trained\nwith and without batch normalization and highlight consistent differences\nobserved. These findings highlight that BatchNorm's effect on representational\nsparsity is not a significant factor affecting generalization, while the\nrepresentations of models trained with BatchNorm tend to show more advantageous\nclustering characteristics.\n","authors":["Hermanus L. Potgieter","Coenraad Mouton","Marelie H. Davel"],"pdf_url":"https://arxiv.org/pdf/2501.14441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09376v1","updated":"2025-02-13T14:45:11Z","published":"2025-02-13T14:45:11Z","title":"LoRA Training Provably Converges to a Low-Rank Global Minimum or It\n  Fails Loudly (But it Probably Won't Fail)","summary":"  Low-rank adaptation (LoRA) has become a standard approach for fine-tuning\nlarge foundation models. However, our theoretical understanding of LoRA remains\nlimited as prior analyses of LoRA's training dynamics either rely on\nlinearization arguments or consider highly simplified setups. In this work, we\nanalyze the LoRA loss landscape without such restrictive assumptions. We define\ntwo regimes: a ``special regime'', which includes idealized setups where\nlinearization arguments hold, and a ``generic regime'' representing more\nrealistic setups where linearization arguments do not hold. In the generic\nregime, we show that LoRA training converges to a global minimizer with low\nrank and small magnitude, or a qualitatively distinct solution with high rank\nand large magnitude. Finally, we argue that the zero-initialization and weight\ndecay in LoRA training induce an implicit bias toward the low-rank,\nsmall-magnitude region of the parameter space -- where global minima lie --\nthus shedding light on why LoRA training usually succeeds in finding global\nminima.\n","authors":["Junsu Kim","Jaeyeon Kim","Ernest K. Ryu"],"pdf_url":"https://arxiv.org/pdf/2502.09376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09374v1","updated":"2025-02-13T14:43:22Z","published":"2025-02-13T14:43:22Z","title":"Mitigating multiple single-event upsets during deep neural network\n  inference using fault-aware training","summary":"  Deep neural networks (DNNs) are increasingly used in safety-critical\napplications. Reliable fault analysis and mitigation are essential to ensure\ntheir functionality in harsh environments that contain high radiation levels.\nThis study analyses the impact of multiple single-bit single-event upsets in\nDNNs by performing fault injection at the level of a DNN model. Additionally, a\nfault aware training (FAT) methodology is proposed that improves the DNNs'\nrobustness to faults without any modification to the hardware. Experimental\nresults show that the FAT methodology improves the tolerance to faults up to a\nfactor 3.\n","authors":["Toon Vinck","Naïn Jonckers","Gert Dekkers","Jeffrey Prinzie","Peter Karsmakers"],"pdf_url":"https://arxiv.org/pdf/2502.09374v1.pdf","comment":"7 pages, 4 figures, Topical Workshop on Electronics for Particle\n  Physics"},{"id":"http://arxiv.org/abs/2502.07465v2","updated":"2025-02-13T14:38:24Z","published":"2025-02-11T11:16:59Z","title":"Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models","summary":"  This study uses deep-learning models to predict city partition crime counts\non specific days. It helps police enhance surveillance, gather intelligence,\nand proactively prevent crimes. We formulate crime count prediction as a\nspatiotemporal sequence challenge, where both input data and prediction targets\nare spatiotemporal sequences. In order to improve the accuracy of crime\nforecasting, we introduce a new model that combines Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a\ncomparative analysis to access the effects of various data sequences, including\nraw and binned data, on the prediction errors of four deep learning forecasting\nmodels. Directly inputting raw crime data into the forecasting model causes\nhigh prediction errors, making the model unsuitable for real - world use. The\nfindings indicate that the proposed CNN-LSTM model achieves optimal performance\nwhen crime data is categorized into 10 or 5 groups. Data binning can enhance\nforecasting model performance, but poorly defined intervals may reduce map\ngranularity. Compared to dividing into 5 bins, binning into 10 intervals\nstrikes an optimal balance, preserving data characteristics and surpassing raw\ndata in predictive modelling efficacy.\n","authors":["Li Mao","Wei Du","Shuo Wen","Qi Li","Tong Zhang","Wei Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.07465v2.pdf","comment":"The paper was submitted without the consent of all co-authors. The\n  content of the paper is incomplete and requires substantial additional work\n  before it can be considered a complete and coherent submission"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna Pîslar","Michiel A. Bakker","Michael Henry Tessler","Raphael Köster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09365v1","updated":"2025-02-13T14:33:02Z","published":"2025-02-13T14:33:02Z","title":"Simple Path Structural Encoding for Graph Transformers","summary":"  Graph transformers extend global self-attention to graph-structured data,\nachieving notable success in graph learning. Recently, random walk structural\nencoding (RWSE) has been found to further enhance their predictive power by\nencoding both structural and positional information into the edge\nrepresentation. However, RWSE cannot always distinguish between edges that\nbelong to different local graph patterns, which reduces its ability to capture\nthe full structural complexity of graphs. This work introduces Simple Path\nStructural Encoding (SPSE), a novel method that utilizes simple path counts for\nedge encoding. We show theoretically and experimentally that SPSE overcomes the\nlimitations of RWSE, providing a richer representation of graph structures,\nparticularly for capturing local cyclic patterns. To make SPSE computationally\ntractable, we propose an efficient approximate algorithm for simple path\ncounting. SPSE demonstrates significant performance improvements over RWSE on\nvarious benchmarks, including molecular and long-range graph datasets,\nachieving statistically significant gains in discriminative tasks. These\nresults pose SPSE as a powerful edge encoding alternative for enhancing the\nexpressivity of graph transformers.\n","authors":["Louis Airale","Antonio Longa","Mattia Rigon","Andrea Passerini","Roberto Passerone"],"pdf_url":"https://arxiv.org/pdf/2502.09365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09363v1","updated":"2025-02-13T14:31:49Z","published":"2025-02-13T14:31:49Z","title":"The Accuracy Cost of Weakness: A Theoretical Analysis of Fixed-Segment\n  Weak Labeling for Events in Time","summary":"  Accurate labels are critical for deriving robust machine learning models.\nLabels are used to train supervised learning models and to evaluate most\nmachine learning paradigms. In this paper, we model the accuracy and cost of a\ncommon weak labeling process where annotators assign presence or absence labels\nto fixed-length data segments for a given event class. The annotator labels a\nsegment as \"present\" if it sufficiently covers an event from that class, e.g.,\na birdsong sound event in audio data. We analyze how the segment length affects\nthe label accuracy and the required number of annotations, and compare this\nfixed-length labeling approach with an oracle method that uses the true event\nactivations to construct the segments. Furthermore, we quantify the gap between\nthese methods and verify that in most realistic scenarios the oracle method is\nbetter than the fixed-length labeling method in both accuracy and cost. Our\nfindings provide a theoretical justification for adaptive weak labeling\nstrategies that mimic the oracle process, and a foundation for optimizing weak\nlabeling processes in sequence labeling tasks.\n","authors":["John Martinsson","Olof Mogren","Tuomas Virtanen","Maria Sandsten"],"pdf_url":"https://arxiv.org/pdf/2502.09363v1.pdf","comment":"Submitted to TMLR"},{"id":"http://arxiv.org/abs/2411.17287v2","updated":"2025-02-13T14:31:24Z","published":"2024-11-26T10:19:16Z","title":"Privacy-Preserving Federated Unsupervised Domain Adaptation for\n  Regression on Small-Scale and High-Dimensional Biological Data","summary":"  Machine learning models often struggle with generalization in small,\nheterogeneous datasets due to domain shifts caused by variations in data\ncollection and population differences. This challenge is particularly\npronounced in biological data, where data is high-dimensional, small-scale, and\ndecentralized across institutions. While federated domain adaptation methods\n(FDA) aim to address these challenges, most existing approaches rely on deep\nlearning and focus on classification tasks, making them unsuitable for\nsmall-scale, high-dimensional applications. In this work, we propose freda, a\nprivacy-preserving federated method for unsupervised domain adaptation in\nregression tasks. Unlike deep learning-based FDA approaches, freda is the first\nmethod to enable the federated training of Gaussian Processes to model complex\nfeature relationships while ensuring complete data privacy through randomized\nencoding and secure aggregation. This allows for effective domain adaptation\nwithout direct access to raw data, making it well-suited for applications\ninvolving high-dimensional, heterogeneous datasets. We evaluate freda on the\nchallenging task of age prediction from DNA methylation data, demonstrating\nthat it achieves performance comparable to the centralized state-of-the-art\nmethod while preserving complete data privacy.\n","authors":["Cem Ata Baykara","Ali Burak Ünal","Nico Pfeifer","Mete Akgün"],"pdf_url":"https://arxiv.org/pdf/2411.17287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09352v1","updated":"2025-02-13T14:18:41Z","published":"2025-02-13T14:18:41Z","title":"Wasserstein distributional adversarial training for deep neural networks","summary":"  Design of adversarial attacks for deep neural networks, as well as methods of\nadversarial training against them, are subject of intense research. In this\npaper, we propose methods to train against distributional attack threats,\nextending the TRADES method used for pointwise attacks. Our approach leverages\nrecent contributions and relies on sensitivity analysis for Wasserstein\ndistributionally robust optimization problems. We introduce an efficient\nfine-tuning method which can be deployed on a previously trained model. We test\nour methods on a range of pre-trained models on RobustBench. These experimental\nresults demonstrate the additional training enhances Wasserstein distributional\nrobustness, while maintaining original levels of pointwise robustness, even for\nalready very successful networks. The improvements are less marked for models\npre-trained using huge synthetic datasets of 20-100M images. However,\nremarkably, sometimes our methods are still able to improve their performance\neven when trained using only the original training dataset (50k images).\n","authors":["Xingjian Bai","Guangyi He","Yifan Jiang","Jan Obloj"],"pdf_url":"https://arxiv.org/pdf/2502.09352v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.05366v2","updated":"2025-02-13T14:16:56Z","published":"2024-06-08T06:06:20Z","title":"Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator","summary":"  Risk-sensitive linear quadratic regulator is one of the most fundamental\nproblems in risk-sensitive optimal control. In this paper, we study online\nadaptive control of risk-sensitive linear quadratic regulator in the finite\nhorizon episodic setting. We propose a simple least-squares greedy algorithm\nand show that it achieves $\\widetilde{\\mathcal{O}}(\\log N)$ regret under a\nspecific identifiability assumption, where $N$ is the total number of episodes.\nIf the identifiability assumption is not satisfied, we propose incorporating\nexploration noise into the least-squares-based algorithm, resulting in an\nalgorithm with $\\widetilde{\\mathcal{O}}(\\sqrt{N})$ regret. To our best\nknowledge, this is the first set of regret bounds for episodic risk-sensitive\nlinear quadratic regulator. Our proof relies on perturbation analysis of\nless-standard Riccati equations for risk-sensitive linear quadratic control,\nand a delicate analysis of the loss in the risk-sensitive performance criterion\ndue to applying the suboptimal controller in the online learning process.\n","authors":["Wenhao Xu","Xuefeng Gao","Xuedong He"],"pdf_url":"https://arxiv.org/pdf/2406.05366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09346v1","updated":"2025-02-13T14:11:33Z","published":"2025-02-13T14:11:33Z","title":"Machine learning for modelling unstructured grid data in computational\n  physics: a review","summary":"  Unstructured grid data are essential for modelling complex geometries and\ndynamics in computational physics. Yet, their inherent irregularity presents\nsignificant challenges for conventional machine learning (ML) techniques. This\npaper provides a comprehensive review of advanced ML methodologies designed to\nhandle unstructured grid data in high-dimensional dynamical systems. Key\napproaches discussed include graph neural networks, transformer models with\nspatial attention mechanisms, interpolation-integrated ML methods, and meshless\ntechniques such as physics-informed neural networks. These methodologies have\nproven effective across diverse fields, including fluid dynamics and\nenvironmental simulations. This review is intended as a guidebook for\ncomputational scientists seeking to apply ML approaches to unstructured grid\ndata in their domains, as well as for ML researchers looking to address\nchallenges in computational physics. It places special focus on how ML methods\ncan overcome the inherent limitations of traditional numerical techniques and,\nconversely, how insights from computational physics can inform ML development.\nTo support benchmarking, this review also provides a summary of open-access\ndatasets of unstructured grid data in computational physics. Finally, emerging\ndirections such as generative models with unstructured data, reinforcement\nlearning for mesh generation, and hybrid physics-data-driven paradigms are\ndiscussed to inspire future advancements in this evolving field.\n","authors":["Sibo Cheng","Marc Bocquet","Weiping Ding","Tobias Sebastian Finn","Rui Fu","Jinlong Fu","Yike Guo","Eleda Johnson","Siyi Li","Che Liu","Eric Newton Moro","Jie Pan","Matthew Piggott","Cesar Quilodran","Prakhar Sharma","Kun Wang","Dunhui Xiao","Xiao Xue","Yong Zeng","Mingrui Zhang","Hao Zhou","Kewei Zhu","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2502.09346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04708v2","updated":"2025-02-13T14:06:51Z","published":"2024-11-07T13:45:26Z","title":"Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs","summary":"  Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the *multi-level nature* of\nthe graph modality, even though different chemistry tasks may benefit from\ndifferent feature levels. In this work, we first study the effect of feature\ngranularity and reveal that even reducing all GNN-generated feature tokens to a\nsingle one does not significantly impact model performance. We then investigate\nthe effect of various graph feature levels and demonstrate that both the\nquality of LLM-generated molecules and model performance across different tasks\ndepend on different graph feature levels. Therefore, we conclude with two key\ninsights: (1) current molecular-related multimodal LLMs lack a comprehensive\nunderstanding of graph features, and (2) static processing is not sufficient\nfor hierarchical graph feature. We share our findings in detail, with the hope\nof paving the way for the community to develop more advanced multimodal LLMs\nfor incorporating molecular graphs.\n","authors":["Chengxin Hu","Hao Li","Yihe Yuan","Jing Li","Ivor Tsang"],"pdf_url":"https://arxiv.org/pdf/2411.04708v2.pdf","comment":"9 pages, 4 tables, 1 figure, paper under review"},{"id":"http://arxiv.org/abs/2502.09341v1","updated":"2025-02-13T14:01:15Z","published":"2025-02-13T14:01:15Z","title":"Neural Spatiotemporal Point Processes: Trends and Challenges","summary":"  Spatiotemporal point processes (STPPs) are probabilistic models for events\noccurring in continuous space and time. Real-world event data often exhibit\nintricate dependencies and heterogeneous dynamics. By incorporating modern deep\nlearning techniques, STPPs can model these complexities more effectively than\ntraditional approaches. Consequently, the fusion of neural methods with STPPs\nhas become an active and rapidly evolving research area. In this review, we\ncategorize existing approaches, unify key design choices, and explain the\nchallenges of working with this data modality. We further highlight emerging\ntrends and diverse application domains. Finally, we identify open challenges\nand gaps in the literature.\n","authors":["Sumantrak Mukherjee","Mouad Elhamdi","George Mohler","David A. Selby","Yao Xie","Sebastian Vollmer","Gerrit Grossmann"],"pdf_url":"https://arxiv.org/pdf/2502.09341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09340v1","updated":"2025-02-13T14:00:55Z","published":"2025-02-13T14:00:55Z","title":"This looks like what? Challenges and Future Research Directions for\n  Part-Prototype Models","summary":"  The growing interest in eXplainable Artificial Intelligence (XAI) has\nprompted research into models with built-in interpretability, the most\nprominent of which are part-prototype models. Part-Prototype Models (PPMs) make\ndecisions by comparing an input image to a set of learned prototypes, providing\nhuman-understandable explanations in the form of ``this looks like that''.\nDespite their inherent interpretability, PPMS are not yet considered a valuable\nalternative to post-hoc models. In this survey, we investigate the reasons for\nthis and provide directions for future research. We analyze papers from 2019 to\n2024, and derive a taxonomy of the challenges that current PPMS face. Our\nanalysis shows that the open challenges are quite diverse. The main concern is\nthe quality and quantity of prototypes. Other concerns are the lack of\ngeneralization to a variety of tasks and contexts, and general methodological\nissues, including non-standardized evaluation. We provide ideas for future\nresearch in five broad directions: improving predictive performance, developing\nnovel architectures grounded in theory, establishing frameworks for human-AI\ncollaboration, aligning models with humans, and establishing metrics and\nbenchmarks for evaluation. We hope that this survey will stimulate research and\npromote intrinsically interpretable models for application domains. Our list of\nsurveyed papers is available at https://github.com/aix-group/ppm-survey.\n","authors":["Khawla Elhadri","Tomasz Michalski","Adam Wróbel","Jörg Schlötterer","Bartosz Zieliński","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2502.09340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07532v2","updated":"2025-02-13T13:55:08Z","published":"2025-02-11T13:15:16Z","title":"Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with\n  Diffusion","summary":"  Machine learning methods have been shown to be effective for weather\nforecasting, based on the speed and accuracy compared to traditional numerical\nmodels. While early efforts primarily concentrated on deterministic\npredictions, the field has increasingly shifted toward probabilistic\nforecasting to better capture the forecast uncertainty. Most machine\nlearning-based models have been designed for global-scale predictions, with\nonly limited work targeting regional or limited area forecasting, which allows\nmore specialized and flexible modeling for specific locations. This work\nintroduces Diffusion-LAM, a probabilistic limited area weather model leveraging\nconditional diffusion. By conditioning on boundary data from surrounding\nregions, our approach generates forecasts within a defined area. Experimental\nresults on the MEPS limited area dataset demonstrate the potential of\nDiffusion-LAM to deliver accurate probabilistic forecasts, highlighting its\npromise for limited-area weather prediction.\n","authors":["Erik Larsson","Joel Oskarsson","Tomas Landelius","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.07532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09335v1","updated":"2025-02-13T13:54:58Z","published":"2025-02-13T13:54:58Z","title":"Graph Diffusion Network for Drug-Gene Prediction","summary":"  Predicting drug-gene associations is crucial for drug development and disease\ntreatment. While graph neural networks (GNN) have shown effectiveness in this\ntask, they face challenges with data sparsity and efficient contrastive\nlearning implementation. We introduce a graph diffusion network for drug-gene\nprediction (GDNDGP), a framework that addresses these limitations through two\nkey innovations. First, it employs meta-path-based homogeneous graph learning\nto capture drug-drug and gene-gene relationships, ensuring similar entities\nshare embedding spaces. Second, it incorporates a parallel diffusion network\nthat generates hard negative samples during training, eliminating the need for\nexhaustive negative sample retrieval. Our model achieves superior performance\non the DGIdb 4.0 dataset and demonstrates strong generalization capability on\ntripartite drug-gene-disease networks. Results show significant improvements\nover existing methods in drug-gene prediction tasks, particularly in handling\ncomplex heterogeneous relationships. The source code is publicly available at\nhttps://github.com/csjywu1/GDNDGP.\n","authors":["Jiayang Wu","Wensheng Gan","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.09335v1.pdf","comment":"IEEE/ACM TCBB. 14 pages"},{"id":"http://arxiv.org/abs/2412.05000v2","updated":"2025-02-13T13:53:58Z","published":"2024-12-06T12:52:24Z","title":"Noise Matters: Diffusion Model-based Urban Mobility Generation with\n  Collaborative Noise Priors","summary":"  With global urbanization, the focus on sustainable cities has largely grown,\ndriving research into equity, resilience, and urban planning, which often\nrelies on mobility data. The rise of web-based apps and mobile devices has\nprovided valuable user data for mobility-related research. However, real-world\nmobility data is costly and raises privacy concerns. To protect privacy while\nretaining key features of real-world movement, the demand for synthetic data\nhas steadily increased. Recent advances in diffusion models have shown great\npotential for mobility trajectory generation due to their ability to model\nrandomness and uncertainty. However, existing approaches often directly apply\nidentically distributed (i.i.d.) noise sampling from image generation\ntechniques, which fail to account for the spatiotemporal correlations and\nsocial interactions that shape urban mobility patterns. In this paper, we\npropose CoDiffMob, a diffusion model for urban mobility generation with\ncollaborative noise priors, we emphasize the critical role of noise in\ndiffusion models for generating mobility data. By leveraging both individual\nmovement characteristics and population-wide dynamics, we construct novel\ncollaborative noise priors that provide richer and more informative guidance\nthroughout the generation process. Extensive experiments demonstrate the\nsuperiority of our method, with generated data accurately capturing both\nindividual preferences and collective patterns, achieving an improvement of\nover 32%. Furthermore, it can effectively replace web-derived mobility data to\nbetter support downstream applications, while safeguarding user privacy and\nfostering a more secure and ethical web. This highlights its tremendous\npotential for applications in sustainable city-related research. The code and\ndata are available at https://github.com/tsinghua-fib-lab/CoDiffMob.\n","authors":["Yuheng Zhang","Yuan Yuan","Jingtao Ding","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2412.05000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09332v1","updated":"2025-02-13T13:49:52Z","published":"2025-02-13T13:49:52Z","title":"Full Swap Regret and Discretized Calibration","summary":"  We study the problem of minimizing swap regret in structured normal-form\ngames. Players have a very large (potentially infinite) number of pure actions,\nbut each action has an embedding into $d$-dimensional space and payoffs are\ngiven by bilinear functions of these embeddings. We provide an efficient\nlearning algorithm for this setting that incurs at most\n$\\tilde{O}(T^{(d+1)/(d+3)})$ swap regret after $T$ rounds.\n  To achieve this, we introduce a new online learning problem we call\n\\emph{full swap regret minimization}. In this problem, a learner repeatedly\ntakes a (randomized) action in a bounded convex $d$-dimensional action set\n$\\mathcal{K}$ and then receives a loss from the adversary, with the goal of\nminimizing their regret with respect to the \\emph{worst-case} swap function\nmapping $\\mathcal{K}$ to $\\mathcal{K}$. For varied assumptions about the\nconvexity and smoothness of the loss functions, we design algorithms with full\nswap regret bounds ranging from $O(T^{d/(d+2)})$ to $O(T^{(d+1)/(d+2)})$.\n  Finally, we apply these tools to the problem of online forecasting to\nminimize calibration error, showing that several notions of calibration can be\nviewed as specific instances of full swap regret. In particular, we design\nefficient algorithms for online forecasting that guarantee at most $O(T^{1/3})$\n$\\ell_2$-calibration error and $O(\\max(\\sqrt{\\epsilon T}, T^{1/3}))$\n\\emph{discretized-calibration} error (when the forecaster is restricted to\npredicting multiples of $\\epsilon$).\n","authors":["Maxwell Fishelson","Robert Kleinberg","Princewill Okoroafor","Renato Paes Leme","Jon Schneider","Yifeng Teng"],"pdf_url":"https://arxiv.org/pdf/2502.09332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09329v1","updated":"2025-02-13T13:43:52Z","published":"2025-02-13T13:43:52Z","title":"Bayesian Optimization for Simultaneous Selection of Machine Learning\n  Algorithms and Hyperparameters on Shared Latent Space","summary":"  Selecting the optimal combination of a machine learning (ML) algorithm and\nits hyper-parameters is crucial for the development of high-performance ML\nsystems. However, since the combination of ML algorithms and hyper-parameters\nis enormous, the exhaustive validation requires a significant amount of time.\nMany existing studies use Bayesian optimization (BO) for accelerating the\nsearch. On the other hand, a significant difficulty is that, in general, there\nexists a different hyper-parameter space for each one of candidate ML\nalgorithms. BO-based approaches typically build a surrogate model independently\nfor each hyper-parameter space, by which sufficient observations are required\nfor all candidate ML algorithms. In this study, our proposed method embeds\ndifferent hyper-parameter spaces into a shared latent space, in which a\nsurrogate multi-task model for BO is estimated. This approach can share\ninformation of observations from different ML algorithms by which efficient\noptimization is expected with a smaller number of total observations. We\nfurther propose the pre-training of the latent space embedding with an\nadversarial regularization, and a ranking model for selecting an effective\npre-trained embedding for a given target dataset. Our empirical study\ndemonstrates effectiveness of the proposed method through datasets from OpenML.\n","authors":["Kazuki Ishikawa","Ryota Ozaki","Yohei Kanzaki","Ichiro Takeuchi","Masayuki Karasuyama"],"pdf_url":"https://arxiv.org/pdf/2502.09329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16839v3","updated":"2025-02-13T13:39:26Z","published":"2025-01-28T10:28:17Z","title":"Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans","summary":"  Among generative neural models, flow matching techniques stand out for their\nsimple applicability and good scaling properties. Here, velocity fields of\ncurves connecting a simple latent and a target distribution are learned. Then\nthe corresponding ordinary differential equation can be used to sample from a\ntarget distribution, starting in samples from the latent one. This paper\nreviews from a mathematical point of view different techniques to learn the\nvelocity fields of absolutely continuous curves in the Wasserstein geometry. We\nshow how the velocity fields can be characterized and learned via i) transport\nplans (couplings) between latent and target distributions, ii) Markov kernels\nand iii) stochastic processes, where the latter two include the coupling\napproach, but are in general broader. Besides this main goal, we show how flow\nmatching can be used for solving Bayesian inverse problems, where the\ndefinition of conditional Wasserstein distances plays a central role. Finally,\nwe briefly address continuous normalizing flows and score matching techniques,\nwhich approach the learning of velocity fields of curves from other directions.\n","authors":["Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2501.16839v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09324v1","updated":"2025-02-13T13:37:52Z","published":"2025-02-13T13:37:52Z","title":"Depth-Bounds for Neural Networks via the Braid Arrangement","summary":"  We contribute towards resolving the open question of how many hidden layers\nare required in ReLU networks for exactly representing all continuous and\npiecewise linear functions on $\\mathbb{R}^d$. While the question has been\nresolved in special cases, the best known lower bound in general is still 2. We\nfocus on neural networks that are compatible with certain polyhedral complexes,\nmore precisely with the braid fan. For such neural networks, we prove a\nnon-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to\nexactly represent the maximum of $d$ numbers. Additionally, under our\nassumption, we provide a combinatorial proof that 3 hidden layers are necessary\nto compute the maximum of 5 numbers; this had only been verified with an\nexcessive computation so far. Finally, we show that a natural generalization of\nthe best known upper bound to maxout networks is not tight, by demonstrating\nthat a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to\nrepresent the maximum of 7 numbers.\n","authors":["Moritz Grillo","Christoph Hertrich","Georg Loho"],"pdf_url":"https://arxiv.org/pdf/2502.09324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09319v1","updated":"2025-02-13T13:33:45Z","published":"2025-02-13T13:33:45Z","title":"Bridging Jensen Gap for Max-Min Group Fairness Optimization in\n  Recommendation","summary":"  Group max-min fairness (MMF) is commonly used in fairness-aware recommender\nsystems (RS) as an optimization objective, as it aims to protect marginalized\nitem groups and ensures a fair competition platform. However, our theoretical\nanalysis indicates that integrating MMF constraint violates the assumption of\nsample independence during optimization, causing the loss function to deviate\nfrom linear additivity. Such nonlinearity property introduces the Jensen gap\nbetween the model's convergence point and the optimal point if mini-batch\nsampling is applied. Both theoretical and empirical studies show that as the\nmini-batch size decreases and the group size increases, the Jensen gap will\nwiden accordingly. Some methods using heuristic re-weighting or debiasing\nstrategies have the potential to bridge the Jensen gap. However, they either\nlack theoretical guarantees or suffer from heavy computational costs. To\novercome these limitations, we first theoretically demonstrate that the\nMMF-constrained objective can be essentially reformulated as a group-weighted\noptimization objective. Then we present an efficient and effective algorithm\nnamed FairDual, which utilizes a dual optimization technique to minimize the\nJensen gap. Our theoretical analysis demonstrates that FairDual can achieve a\nsub-linear convergence rate to the globally optimal solution and the Jensen gap\ncan be well bounded under a mini-batch sampling strategy with random shuffle.\nExtensive experiments conducted using six large-scale RS backbone models on\nthree publicly available datasets demonstrate that FairDual outperforms all\nbaselines in terms of both accuracy and fairness. Our data and codes are shared\nat https://github.com/XuChen0427/FairDual.\n","authors":["Chen Xu","Yuxin Li","Wenjie Wang","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09319v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09318v1","updated":"2025-02-13T13:33:35Z","published":"2025-02-13T13:33:35Z","title":"SigGate: Enhancing Recurrent Neural Networks with Signature-Based Gating\n  Mechanisms","summary":"  In this paper, we propose a novel approach that enhances recurrent neural\nnetworks (RNNs) by incorporating path signatures into their gating mechanisms.\nOur method modifies both Long Short-Term Memory (LSTM) and Gated Recurrent Unit\n(GRU) architectures by replacing their forget and reset gates, respectively,\nwith learnable path signatures. These signatures, which capture the geometric\nfeatures of the entire path history, provide a richer context for controlling\ninformation flow through the network's memory. This modification allows the\nnetworks to make memory decisions based on the full historical context rather\nthan just the current input and state. Through experimental studies, we\ndemonstrate that our Signature-LSTM (SigLSTM) and Signature-GRU (SigGRU) models\noutperform their traditional counterparts across various sequential learning\ntasks. By leveraging path signatures in recurrent architectures, this method\noffers new opportunities to enhance performance in time series analysis and\nforecasting applications.\n","authors":["Rémi Genet","Hugo Inzirillo"],"pdf_url":"https://arxiv.org/pdf/2502.09318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04750v2","updated":"2025-02-13T13:33:19Z","published":"2025-02-07T08:33:28Z","title":"Tighter sparse variational Gaussian processes","summary":"  Sparse variational Gaussian process (GP) approximations based on inducing\npoints have become the de facto standard for scaling GPs to large datasets,\nowing to their theoretical elegance, computational efficiency, and ease of\nimplementation. This paper introduces a provably tighter variational\napproximation by relaxing the standard assumption that the conditional\napproximate posterior given the inducing points must match that in the prior.\nThe key innovation is to modify the conditional posterior to have smaller\nvariances than that of the prior at the training points. We derive the\ncollapsed bound for the regression case, describe how to use the proposed\napproximation in large data settings, and discuss its application to handle\northogonally structured inducing points and GP latent variable models.\nExtensive experiments on regression benchmarks, classification, and latent\nvariable models demonstrate that the proposed approximation consistently\nmatches or outperforms standard sparse variational GPs while maintaining the\nsame computational cost. An implementation will be made available in all\npopular GP packages.\n","authors":["Thang D. Bui","Matthew Ashman","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2502.04750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03345v3","updated":"2025-02-13T13:25:18Z","published":"2024-06-05T15:04:27Z","title":"Feature contamination: Neural networks learn uncorrelated features and\n  fail to generalize","summary":"  Learning representations that generalize under distribution shifts is\ncritical for building robust machine learning models. However, despite\nsignificant efforts in recent years, algorithmic advances in this direction\nhave been limited. In this work, we seek to understand the fundamental\ndifficulty of out-of-distribution generalization with deep neural networks. We\nfirst empirically show that perhaps surprisingly, even allowing a neural\nnetwork to explicitly fit the representations obtained from a teacher network\nthat can generalize out-of-distribution is insufficient for the generalization\nof the student network. Then, by a theoretical study of two-layer ReLU networks\noptimized by stochastic gradient descent (SGD) under a structured feature\nmodel, we identify a fundamental yet unexplored feature learning proclivity of\nneural networks, feature contamination: neural networks can learn uncorrelated\nfeatures together with predictive features, resulting in generalization failure\nunder distribution shifts. Notably, this mechanism essentially differs from the\nprevailing narrative in the literature that attributes the generalization\nfailure to spurious correlations. Overall, our results offer new insights into\nthe non-linear feature learning dynamics of neural networks and highlight the\nnecessity of considering inductive biases in out-of-distribution\ngeneralization.\n","authors":["Tianren Zhang","Chujie Zhao","Guanyu Chen","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.03345v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2502.09306v1","updated":"2025-02-13T13:18:30Z","published":"2025-02-13T13:18:30Z","title":"Non-asymptotic Analysis of Diffusion Annealed Langevin Monte Carlo for\n  Generative Modelling","summary":"  We investigate the theoretical properties of general diffusion\n(interpolation) paths and their Langevin Monte Carlo implementation, referred\nto as diffusion annealed Langevin Monte Carlo (DALMC), under weak conditions on\nthe data distribution. Specifically, we analyse and provide non-asymptotic\nerror bounds for the annealed Langevin dynamics where the path of distributions\nis defined as Gaussian convolutions of the data distribution as in diffusion\nmodels. We then extend our results to recently proposed heavy-tailed (Student's\nt) diffusion paths, demonstrating their theoretical properties for heavy-tailed\ndata distributions for the first time. Our analysis provides theoretical\nguarantees for a class of score-based generative models that interpolate\nbetween a simple distribution (Gaussian or Student's t) and the data\ndistribution in finite time. This approach offers a broader perspective\ncompared to standard score-based diffusion approaches, which are typically\nbased on a forward Ornstein-Uhlenbeck (OU) noising process.\n","authors":["Paula Cordero-Encinar","O. Deniz Akyildiz","Andrew B. Duncan"],"pdf_url":"https://arxiv.org/pdf/2502.09306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09303v1","updated":"2025-02-13T13:16:10Z","published":"2025-02-13T13:16:10Z","title":"Towards Seamless Hierarchical Federated Learning under Intermittent\n  Client Participation: A Stagewise Decision-Making Methodology","summary":"  Federated Learning (FL) offers a pioneering distributed learning paradigm\nthat enables devices/clients to build a shared global model. This global model\nis obtained through frequent model transmissions between clients and a central\nserver, which may cause high latency, energy consumption, and congestion over\nbackhaul links. To overcome these drawbacks, Hierarchical Federated Learning\n(HFL) has emerged, which organizes clients into multiple clusters and utilizes\nedge nodes (e.g., edge servers) for intermediate model aggregations between\nclients and the central server. Current research on HFL mainly focus on\nenhancing model accuracy, latency, and energy consumption in scenarios with a\nstable/fixed set of clients. However, addressing the dynamic availability of\nclients -- a critical aspect of real-world scenarios -- remains underexplored.\nThis study delves into optimizing client selection and client-to-edge\nassociations in HFL under intermittent client participation so as to minimize\noverall system costs (i.e., delay and energy), while achieving fast model\nconvergence. We unveil that achieving this goal involves solving a complex\nNP-hard problem. To tackle this, we propose a stagewise methodology that splits\nthe solution into two stages, referred to as Plan A and Plan B. Plan A focuses\non identifying long-term clients with high chance of participation in\nsubsequent model training rounds. Plan B serves as a backup, selecting\nalternative clients when long-term clients are unavailable during model\ntraining rounds. This stagewise methodology offers a fresh perspective on\nclient selection that can enhance both HFL and conventional FL via enabling\nlow-overhead decision-making processes. Through evaluations on MNIST and\nCIFAR-10 datasets, we show that our methodology outperforms existing benchmarks\nin terms of model accuracy and system costs.\n","authors":["Minghong Wu","Minghui Liwang","Yuhan Su","Li Li","Seyyedali Hosseinalipour","Xianbin Wang","Huaiyu Dai","Zhenzhen Jiao"],"pdf_url":"https://arxiv.org/pdf/2502.09303v1.pdf","comment":"20 pages, 8 figures,5 tables"},{"id":"http://arxiv.org/abs/2108.12113v3","updated":"2025-02-13T13:12:41Z","published":"2021-08-27T04:18:45Z","title":"A method of supervised learning from conflicting data with hidden\n  contexts","summary":"  Conventional supervised learning assumes a stable input-output relationship.\nHowever, this assumption fails in open-ended training settings where the\ninput-output relationship depends on hidden contexts. In this work, we\nformulate a more general supervised learning problem in which training data is\ndrawn from multiple unobservable domains, each potentially exhibiting distinct\ninput-output maps. This inherent conflict in data renders standard empirical\nrisk minimization training ineffective. To address this challenge, we propose a\nmethod LEAF that introduces an allocation function, which learns to assign\nconflicting data to different predictive models. We establish a connection\nbetween LEAF and a variant of the Expectation-Maximization algorithm, allowing\nus to derive an analytical expression for the allocation function. Finally, we\nprovide a theoretical analysis of LEAF and empirically validate its\neffectiveness on both synthetic and real-world tasks involving conflicting\ndata.\n","authors":["Tianren Zhang","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2108.12113v3.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09298v1","updated":"2025-02-13T13:12:16Z","published":"2025-02-13T13:12:16Z","title":"Convex Is Back: Solving Belief MDPs With Convexity-Informed Deep\n  Reinforcement Learning","summary":"  We present a novel method for Deep Reinforcement Learning (DRL),\nincorporating the convex property of the value function over the belief space\nin Partially Observable Markov Decision Processes (POMDPs). We introduce hard-\nand soft-enforced convexity as two different approaches, and compare their\nperformance against standard DRL on two well-known POMDP environments, namely\nthe Tiger and FieldVisionRockSample problems. Our findings show that including\nthe convexity feature can substantially increase performance of the agents, as\nwell as increase robustness over the hyperparameter space, especially when\ntesting on out-of-distribution domains. The source code for this work can be\nfound at https://github.com/Dakout/Convex_DRL.\n","authors":["Daniel Koutas","Daniel Hettegger","Kostas G. Papakonstantinou","Daniel Straub"],"pdf_url":"https://arxiv.org/pdf/2502.09298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09297v1","updated":"2025-02-13T13:11:54Z","published":"2025-02-13T13:11:54Z","title":"When do neural networks learn world models?","summary":"  Humans develop world models that capture the underlying generation process of\ndata. Whether neural networks can learn similar world models remains an open\nproblem. In this work, we provide the first theoretical results for this\nproblem, showing that in a multi-task setting, models with a low-degree bias\nprovably recover latent data-generating variables under mild assumptions --\neven if proxy tasks involve complex, non-linear functions of the latents.\nHowever, such recovery is also sensitive to model architecture. Our analysis\nleverages Boolean models of task solutions via the Fourier-Walsh transform and\nintroduces new techniques for analyzing invertible Boolean transforms, which\nmay be of independent interest. We illustrate the algorithmic implications of\nour results and connect them to related research areas, including\nself-supervised learning, out-of-distribution generalization, and the linear\nrepresentation hypothesis in large language models.\n","authors":["Tianren Zhang","Guanyu Chen","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09297v1.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.16106v2","updated":"2025-02-13T13:11:46Z","published":"2024-10-21T15:34:44Z","title":"Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation","summary":"  Statistical inference with finite-sample validity for the value function of a\ngiven policy in Markov decision processes (MDPs) is crucial for ensuring the\nreliability of reinforcement learning. Temporal Difference (TD) learning,\narguably the most widely used algorithm for policy evaluation, serves as a\nnatural framework for this purpose. In this paper, we study the consistency\nproperties of TD learning with Polyak-Ruppert averaging and linear function\napproximation, and obtain three significant improvements over existing results.\nFirst, we derive a novel sharp high-dimensional probability convergence\nguarantee that depends explicitly on the asymptotic variance and holds under\nweak conditions. We further establish refined high-dimensional Berry-Esseen\nbounds over the class of convex sets that guarantee faster rates than those in\nthe literature. Finally, we propose a plug-in estimator for the asymptotic\ncovariance matrix, designed for efficient online computation. These results\nenable the construction of confidence regions and simultaneous confidence\nintervals for the linear parameters of the value function, with guaranteed\nfinite-sample coverage. We demonstrate the applicability of our theoretical\nfindings through numerical experiments.\n","authors":["Weichen Wu","Gen Li","Yuting Wei","Alessandro Rinaldo"],"pdf_url":"https://arxiv.org/pdf/2410.16106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09291v1","updated":"2025-02-13T13:08:11Z","published":"2025-02-13T13:08:11Z","title":"Joint Attention Mechanism Learning to Facilitate Opto-physiological\n  Monitoring during Physical Activity","summary":"  Opto-physiological monitoring is a non-contact technique for measuring\ncardiac signals, i.e., photoplethysmography (PPG). Quality PPG signals directly\nlead to reliable physiological readings. However, PPG signal acquisition\nprocedures are often accompanied by spurious motion artefacts (MAs), especially\nduring low-to-high-intensity physical activity. This study proposes a practical\nadversarial learning approach for opto-physiological monitoring by using a\ngenerative adversarial network with an attention mechanism (AM-GAN) to model\nmotion noise and to allow MA removal. The AM-GAN learns an MA-resistant mapping\nfrom raw and noisy signals to clear PPG signals in an adversarial manner,\nguided by an attention mechanism to directly translate the motion reference of\ntriaxial acceleration to the MAs appearing in the raw signal. The AM-GAN was\nexperimented with three various protocols engaged with 39 subjects in various\nphysical activities. The average absolute error for heart rate (HR) derived\nfrom the MA-free PPG signal via the AM-GAN, is 1.81 beats/min for the IEEE-SPC\ndataset and 3.86 beats/min for the PPGDalia dataset. The same procedure applied\nto an in-house LU dataset resulted in average absolute errors for HR and\nrespiratory rate (RR) of less than 1.37 beats/min and 2.49 breaths/min,\nrespectively. The study demonstrates the robustness and resilience of AM-GAN,\nparticularly during low-to-high-intensity physical activities.\n","authors":["Xiaoyu Zheng","Sijung Hu","Vincent Dwyer","Mahsa Derakhshani","Laura Barrett"],"pdf_url":"https://arxiv.org/pdf/2502.09291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09290v1","updated":"2025-02-13T13:06:56Z","published":"2025-02-13T13:06:56Z","title":"Dynamic Rolling Horizon Optimization for Network-Constrained V2X Value\n  Stacking of Electric Vehicles Under Uncertainties","summary":"  Electric vehicle (EV) coordination can provide significant benefits through\nvehicle-to-everything (V2X) by interacting with the grid, buildings, and other\nEVs. This work aims to develop a V2X value-stacking framework, including\nvehicle-to-building (V2B), vehicle-to-grid (V2G), and energy trading, to\nmaximize economic benefits for residential communities while maintaining\ndistribution voltage. This work also seeks to quantify the impact of prediction\nerrors related to building load, renewable energy, and EV arrivals. A dynamic\nrolling-horizon optimization (RHO) method is employed to leverage multiple\nrevenue streams and maximize the potential of EV coordination. To address\nenergy uncertainties, including hourly local building load, local photovoltaic\n(PV) generation, and EV arrivals, this work develops a Transformer-based\nforecasting model named Gated Recurrent Units-Encoder-Temporal Fusion Decoder\n(GRU-EN-TFD). The simulation results, using real data from Australia's National\nElectricity Market, and the Independent System Operators in New England and New\nYork in the US, reveal that V2X value stacking can significantly reduce energy\ncosts. The proposed GRU-EN-TFD model outperforms the benchmark forecast model.\nUncertainties in EV arrivals have a more substantial impact on value-stacking\nperformance, highlighting the significance of its accurate forecast. This work\nprovides new insights into the dynamic interactions among residential\ncommunities, unlocking the full potential of EV batteries.\n","authors":["Canchen Jiang","Ariel Liebman","Bo Jie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09290v1.pdf","comment":"21 pages, accepted by Renewable Energy"},{"id":"http://arxiv.org/abs/2502.09287v1","updated":"2025-02-13T13:01:46Z","published":"2025-02-13T13:01:46Z","title":"An Uncertainty Principle for Linear Recurrent Neural Networks","summary":"  We consider linear recurrent neural networks, which have become a key\nbuilding block of sequence modeling due to their ability for stable and\neffective long-range modeling. In this paper, we aim at characterizing this\nability on a simple but core copy task, whose goal is to build a linear filter\nof order $S$ that approximates the filter that looks $K$ time steps in the past\n(which we refer to as the shift-$K$ filter), where $K$ is larger than $S$.\nUsing classical signal models and quadratic cost, we fully characterize the\nproblem by providing lower bounds of approximation, as well as explicit filters\nthat achieve this lower bound up to constants. The optimal performance\nhighlights an uncertainty principle: the optimal filter has to average values\naround the $K$-th time step in the past with a range~(width) that is\nproportional to $K/S$.\n","authors":["Alexandre François","Antonio Orvieto","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2502.09287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07115v2","updated":"2025-02-13T12:54:36Z","published":"2025-02-10T23:11:44Z","title":"Online Scheduling for LLM Inference with KV Cache Constraints","summary":"  Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.\n","authors":["Patrick Jaillet","Jiashuo Jiang","Chara Podimata","Zijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09282v1","updated":"2025-02-13T12:54:13Z","published":"2025-02-13T12:54:13Z","title":"FE-LWS: Refined Image-Text Representations via Decoder Stacking and\n  Fused Encodings for Remote Sensing Image Captioning","summary":"  Remote sensing image captioning aims to generate descriptive text from remote\nsensing images, typically employing an encoder-decoder framework. In this\nsetup, a convolutional neural network (CNN) extracts feature representations\nfrom the input image, which then guide the decoder in a sequence-to-sequence\ncaption generation process. Although much research has focused on refining the\ndecoder, the quality of image representations from the encoder remains crucial\nfor accurate captioning. This paper introduces a novel approach that integrates\nfeatures from two distinct CNN based encoders, capturing complementary\ninformation to enhance caption generation. Additionally, we propose a weighted\naveraging technique to combine the outputs of all GRUs in the stacked decoder.\nFurthermore, a comparison-based beam search strategy is incorporated to refine\ncaption selection. The results demonstrate that our fusion-based approach,\nalong with the enhanced stacked decoder, significantly outperforms both the\ntransformer-based state-of-the-art model and other LSTM-based baselines.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.09282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09795v4","updated":"2025-02-13T12:35:53Z","published":"2024-10-13T10:48:22Z","title":"WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for\n  Molecular Ground-State Conformation Prediction","summary":"  Predicting molecular ground-state conformation (i.e., energy-minimized\nconformation) is crucial for many chemical applications such as molecular\ndocking and property prediction. Classic energy-based simulation is\ntime-consuming when solving this problem while existing learning-based methods\nhave advantages in computational efficiency but sacrifice accuracy and\ninterpretability. In this work, we propose a novel and effective method to\nbridge the energy-based simulation and the learning-based strategy, which\ndesigns and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called\nWGFormer, for molecular ground-state conformation prediction. Specifically, our\nmethod tackles this task within an auto-encoding framework, which encodes\nlow-quality conformations by the proposed WGFormer and decodes corresponding\nground-state conformations by an MLP. The architecture of WGFormer corresponds\nto Wasserstein gradient flows -- it optimizes molecular conformations by\nminimizing an energy function defined on the latent mixture models of atoms,\nthereby significantly improving performance and interpretability. Extensive\nexperiments show that our method consistently outperforms state-of-the-art\ncompetitors, providing a new and insightful paradigm to predict molecular\nground-state conformation.\n","authors":["Fanmeng Wang","Minjie Cheng","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.09795v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09271v1","updated":"2025-02-13T12:33:39Z","published":"2025-02-13T12:33:39Z","title":"LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via\n  Subgraph Injection","summary":"  Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nmodeling data with graph structures, yet recent research reveals their\nsusceptibility to adversarial attacks. Traditional attack methodologies, which\nrely on manipulating the original graph or adding links to artificially created\nnodes, often prove impractical in real-world settings. This paper introduces a\nnovel adversarial scenario involving the injection of an isolated subgraph to\ndeceive both the link recommender and the node classifier within a GNN system.\nSpecifically, the link recommender is mislead to propose links between targeted\nvictim nodes and the subgraph, encouraging users to unintentionally establish\nconnections and that would degrade the node classification accuracy, thereby\nfacilitating a successful attack. To address this, we present the LiSA\nframework, which employs a dual surrogate model and bi-level optimization to\nsimultaneously meet two adversarial objectives. Extensive experiments on\nreal-world datasets demonstrate the effectiveness of our method.\n","authors":["Wenlun Zhang","Enyan Dai","Kentaro Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2502.09271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17460v3","updated":"2025-02-13T12:32:45Z","published":"2024-01-30T21:46:09Z","title":"Rendering Wireless Environments Useful for Gradient Estimators: A\n  Zero-Order Stochastic Federated Learning Method","summary":"  Cross-device federated learning (FL) is a growing machine learning setting\nwhereby multiple edge devices collaborate to train a model without disclosing\ntheir raw data. With the great number of mobile devices participating in more\nFL applications via the wireless environment, the practical implementation of\nthese applications will be hindered due to the limited uplink capacity of\ndevices, causing critical bottlenecks. In this work, we propose a novel doubly\ncommunication-efficient zero-order (ZO) method with a one-point gradient\nestimator that replaces communicating long vectors with scalar values and that\nharnesses the nature of the wireless communication channel, overcoming the need\nto know the channel state coefficient. It is the first method that includes the\nwireless channel in the learning algorithm itself instead of wasting resources\nto analyze it and remove its impact. We then offer a thorough analysis of the\nproposed zero-order federated learning (ZOFL) framework and prove that our\nmethod converges \\textit{almost surely}, which is a novel result in nonconvex\nZO optimization. We further prove a convergence rate of\n$O(\\frac{1}{\\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate the\npotential of our algorithm with experimental results.\n","authors":["Elissa Mhanna","Mohamad Assaad"],"pdf_url":"https://arxiv.org/pdf/2401.17460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09268v1","updated":"2025-02-13T12:29:50Z","published":"2025-02-13T12:29:50Z","title":"GEVRM: Goal-Expressive Video Generation Model For Robust Visual\n  Manipulation","summary":"  With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks.\n","authors":["Hongyin Zhang","Pengxiang Ding","Shangke Lyu","Ying Peng","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09268v1.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09263v1","updated":"2025-02-13T12:24:23Z","published":"2025-02-13T12:24:23Z","title":"Unlocking the Potential of Classic GNNs for Graph-level Tasks: Simple\n  Architectures Meet Excellence","summary":"  Message-passing Graph Neural Networks (GNNs) are often criticized for their\nlimited expressiveness, issues like over-smoothing and over-squashing, and\nchallenges in capturing long-range dependencies, while Graph Transformers (GTs)\nare considered superior due to their global attention mechanisms. Literature\nfrequently suggests that GTs outperform GNNs, particularly in graph-level tasks\nsuch as graph classification and regression. In this study, we explore the\nuntapped potential of GNNs through an enhanced framework, GNN+, which\nintegrates six widely used techniques: edge feature integration, normalization,\ndropout, residual connections, feed-forward networks, and positional encoding,\nto effectively tackle graph-level tasks. We conduct a systematic evaluation of\nthree classic GNNs, namely GCN, GIN, and GatedGCN, enhanced by the GNN+\nframework across 14 well-known graph-level datasets. Our results show that,\ncontrary to the prevailing belief, classic GNNs excel in graph-level tasks,\nsecuring top three rankings across all datasets and achieving first place in\neight, while also demonstrating greater efficiency than GTs. This highlights\nthe potential of simple GNN architectures, challenging the belief that complex\nmechanisms in GTs are essential for superior graph-level performance.\n","authors":["Yuankai Luo","Lei Shi","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.09266v5","updated":"2025-02-13T12:15:30Z","published":"2021-05-19T17:22:47Z","title":"Copyright in Generative Deep Learning","summary":"  Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques, which have seen a formidable\ndevelopment and remarkable refinement in the very recent years. Given the\ninherent characteristics of these techniques, a series of novel legal problems\narise. In this article, we consider a set of key questions in the area of\ngenerative deep learning for the arts, including the following: is it possible\nto use copyrighted works as training set for generative models? How do we\nlegally store their copies in order to perform the training process? Who (if\nsomeone) will own the copyright on the generated data? We try to answer these\nquestions considering the law in force in both the United States of America and\nthe European Union, and potential future alternatives. We then extend our\nanalysis to code generation, which is an emerging area of generative deep\nlearning. Finally, we also formulate a set of practical guidelines for artists\nand developers working on deep learning generated art, as well as some policy\nsuggestions for policymakers.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2105.09266v5.pdf","comment":"Published in Data & Policy at\n  https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E"},{"id":"http://arxiv.org/abs/2502.09257v1","updated":"2025-02-13T12:13:25Z","published":"2025-02-13T12:13:25Z","title":"Bandit Multiclass List Classification","summary":"  We study the problem of multiclass list classification with (semi-)bandit\nfeedback, where input examples are mapped into subsets of size $m$ of a\ncollection of $K$ possible labels, and the feedback consists of the predicted\nlabels which lie in the set of true labels of the given example. Our main\nresult is for the $(\\varepsilon,\\delta)$-PAC variant of the problem for which\nwe design an algorithm that returns an $\\varepsilon$-optimal hypothesis with\nhigh probability using a sample complexity of $O \\big( (\\mathrm{poly}(K/m) + sm\n/ \\varepsilon^2) \\log (|H|/\\delta) \\big)$ where $H$ is the underlying (finite)\nhypothesis class and $s$ is an upper bound on the number of true labels for a\ngiven example. This bound improves upon known bounds for combinatorial\nsemi-bandits whenever $s \\ll K$. Moreover, in the regime where $s = O(1)$ the\nleading terms in our bound match the corresponding full-information rates,\nimplying that bandit feedback essentially comes at no cost. Our PAC learning\nalgorithm is also computationally efficient given access to an ERM oracle for\n$H$. Additionally, we consider the regret minimization setting where data can\nbe generated adversarially, and establish a regret bound of $\\widetilde O(|H| +\n\\sqrt{smT \\log |H|})$. Our results generalize and extend those of Erez et al.\n(2024) who consider the simpler single-label setting corresponding to $s=m=1$,\nand in fact hold for the more general contextual combinatorial semi-bandit\nproblem with $s$-sparse rewards.\n","authors":["Liad Erez","Tomer Koren"],"pdf_url":"https://arxiv.org/pdf/2502.09257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.02726v7","updated":"2025-02-13T12:12:28Z","published":"2021-04-06T18:00:06Z","title":"Creativity and Machine Learning: A Survey","summary":"  There is a growing interest in the area of machine learning and creativity.\nThis survey presents an overview of the history and the state of the art of\ncomputational creativity theories, key machine learning techniques (including\ngenerative deep learning), and corresponding automatic evaluation methods.\nAfter presenting a critical discussion of the key contributions in this area,\nwe outline the current research challenges and emerging opportunities in this\nfield.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2104.02726v7.pdf","comment":"Published in ACM Computing Surveys at\n  https://dl.acm.org/doi/10.1145/3664595"},{"id":"http://arxiv.org/abs/2502.07005v3","updated":"2025-02-13T12:11:58Z","published":"2025-02-10T20:10:25Z","title":"Geometry-aware RL for Manipulation of Varying Shapes and Deformable\n  Objects","summary":"  Manipulating objects with varying geometries and deformable objects is a\nmajor challenge in robotics. Tasks such as insertion with different objects or\ncloth hanging require precise control and effective modelling of complex\ndynamics. In this work, we frame this problem through the lens of a\nheterogeneous graph that comprises smaller sub-graphs, such as actuators and\nobjects, accompanied by different edge types describing their interactions.\nThis graph representation serves as a unified structure for both rigid and\ndeformable objects tasks, and can be extended further to tasks comprising\nmultiple actuators. To evaluate this setup, we present a novel and challenging\nreinforcement learning benchmark, including rigid insertion of diverse objects,\nas well as rope and cloth manipulation with multiple end-effectors. These tasks\npresent a large search space, as both the initial and target configurations are\nuniformly sampled in 3D space. To address this issue, we propose a novel\ngraph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi),\nutilizing $SE(3)$ equivariant message passing networks as the main backbone to\nexploit the geometric symmetry. In addition, by modeling explicit\nheterogeneity, HEPi can outperform Transformer-based and non-heterogeneous\nequivariant policies in terms of average returns, sample efficiency, and\ngeneralization to unseen objects.\n","authors":["Tai Hoang","Huy Le","Philipp Becker","Vien Anh Ngo","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2502.07005v3.pdf","comment":"Accepted at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.09254v1","updated":"2025-02-13T12:10:05Z","published":"2025-02-13T12:10:05Z","title":"AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection","summary":"  Graph anomaly detection (GAD) aims to identify abnormal nodes that differ\nfrom the majority of the nodes in a graph, which has been attracting\nsignificant attention in recent years. Existing generalist graph models have\nachieved remarkable success in different graph tasks but struggle to generalize\nto the GAD task. This limitation arises from their difficulty in learning\ngeneralized knowledge for capturing the inherently infrequent, irregular and\nheterogeneous abnormality patterns in graphs from different domains. To address\nthis challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model\nthat supports zero-shot inference and few-shot prompt tuning for GAD in diverse\ngraph datasets. One key insight is that graph-agnostic representations for\nnormal and abnormal classes are required to support effective zero/few-shot GAD\nacross different graphs. Motivated by this, AnomalyGFM is pre-trained to align\ndata-independent, learnable normal and abnormal class prototypes with node\nrepresentation residuals (i.e., representation deviation of a node from its\nneighbors). The residual features essentially project the node information into\na unified feature space where we can effectively measure the abnormality of\nnodes from different graphs in a consistent way. This provides a driving force\nfor the learning of graph-agnostic, discriminative prototypes for the normal\nand abnormal classes, which can be used to enable zero-shot GAD on new graphs,\nincluding very large-scale graphs. If there are few-shot labeled normal nodes\navailable in the new graphs, AnomalyGFM can further support prompt tuning to\nleverage these nodes for better adaptation. Comprehensive experiments on 11\nwidely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM\nsignificantly outperforms state-of-the-art competing methods under both zero-\nand few-shot GAD settings.\n","authors":["Hezhe Qiao","Chaoxi Niu","Ling Chen","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2502.09254v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09252v1","updated":"2025-02-13T12:09:17Z","published":"2025-02-13T12:09:17Z","title":"On the Importance of Embedding Norms in Self-Supervised Learning","summary":"  Self-supervised learning (SSL) allows training data representations without a\nsupervised signal and has become an important paradigm in machine learning.\nMost SSL methods employ the cosine similarity between embedding vectors and\nhence effectively embed data on a hypersphere. While this seemingly implies\nthat embedding norms cannot play any role in SSL, a few recent works have\nsuggested that embedding norms have properties related to network convergence\nand confidence. In this paper, we resolve this apparent contradiction and\nsystematically establish the embedding norm's role in SSL training. Using\ntheoretical analysis, simulations, and experiments, we show that embedding\nnorms (i) govern SSL convergence rates and (ii) encode network confidence, with\nsmaller norms corresponding to unexpected samples. Additionally, we show that\nmanipulating embedding norms can have large effects on convergence speed. Our\nfindings demonstrate that SSL embedding norms are integral to understanding and\noptimizing network behavior.\n","authors":["Andrew Draganov","Sharvaree Vadgama","Sebastian Damrich","Jan Niklas Böhm","Lucas Maes","Dmitry Kobak","Erik Bekkers"],"pdf_url":"https://arxiv.org/pdf/2502.09252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07477v2","updated":"2025-02-13T12:06:33Z","published":"2024-12-10T12:50:25Z","title":"Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution\n  Simulations for Time-Efficient Fine-Resolution Policy Learning","summary":"  In earthwork and construction, excavators often encounter large rocks mixed\nwith various soil conditions, requiring skilled operators. This paper presents\na framework for achieving autonomous excavation using reinforcement learning\n(RL) through a rock excavation simulator. In the simulation, resolution can be\ndefined by the particle size/number in the whole soil space. Fine-resolution\nsimulations closely mimic real-world behavior but demand significant\ncalculation time and challenging sample collection, while coarse-resolution\nsimulations enable faster sample collection but deviate from real-world\nbehavior. To combine the advantages of both resolutions, we explore using\npolicies developed in coarse-resolution simulations for pre-training in\nfine-resolution simulations. To this end, we propose a novel policy learning\nframework called Progressive-Resolution Policy Distillation (PRPD), which\nprogressively transfers policies through some middle-resolution simulations\nwith conservative policy transfer to avoid domain gaps that could lead to\npolicy transfer failure. Validation in a rock excavation simulator and nine\nreal-world rock environments demonstrated that PRPD reduced sampling time to\nless than 1/7 while maintaining task success rates comparable to those achieved\nthrough policy learning in a fine-resolution simulation.\n","authors":["Yuki Kadokawa","Hirotaka Tahara","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2412.07477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09245v1","updated":"2025-02-13T12:00:50Z","published":"2025-02-13T12:00:50Z","title":"You Do Not Fully Utilize Transformer's Representation Capacity","summary":"  In contrast to RNNs, which compress previous tokens into a single hidden\nstate, Transformers can attend to all previous tokens directly. However,\nstandard Transformers only use representations from the immediately preceding\nlayer. In this paper, we show that this design choice causes representation\ncollapse and leads to suboptimal performance. To address this issue, we\nintroduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that\npreserves the model's overall memory footprint while expanding its\nrepresentational capacity by allowing access to hidden states from earlier\nlayers. Through extensive experiments across various architectures and\ndifferent lookup mechanisms, we demonstrate consistent performance improvements\non a wide range of tasks. Moreover, our analysis of the learned representation\ndynamics and our exploration of depthwise circuits reveal how LIMe integrates\ninformation across layers, pointing to promising directions for future\nresearch.\n","authors":["Gleb Gerasimov","Yaroslav Aksenov","Nikita Balagansky","Viacheslav Sinii","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2502.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18458v4","updated":"2025-02-13T11:59:20Z","published":"2024-05-28T17:27:20Z","title":"Asymmetrical estimator for training encapsulated deep photonic neural\n  networks","summary":"  Photonic neural networks (PNNs) are fast in-propagation and high bandwidth\nparadigms that aim to popularize reproducible NN acceleration with higher\nefficiency and lower cost. However, the training of PNN is known to be\nchallenging, where the device-to-device and system-to-system variations create\nimperfect knowledge of the PNN. Despite backpropagation (BP)-based training\nalgorithms being the industry standard for their robustness, generality, and\nfast gradient convergence for digital training, existing PNN-BP methods rely\nheavily on accurate intermediate state extraction or extensive computational\nresources for deep PNNs (DPNNs). The truncated photonic signal propagation and\nthe computation overhead bottleneck DPNN's operation efficiency and increase\nsystem construction cost. Here, we introduce the asymmetrical training (AsyT)\nmethod, tailored for encapsulated DPNNs, where the signal is preserved in the\nanalogue photonic domain for the entire structure. AsyT offers a lightweight\nsolution for DPNNs with minimum readouts, fast and energy-efficient operation,\nand minimum system footprint. AsyT's ease of operation, error tolerance, and\ngenerality aim to promote PNN acceleration in a widened operational scenario\ndespite the fabrication variations and imperfect controls. We demonstrated AsyT\nfor encapsulated DPNN with integrated photonic chips, repeatably enhancing the\nperformance from in-silico BP for different network structures and datasets.\n","authors":["Yizhi Wang","Minjia Chen","Chunhui Yao","Jie Ma","Ting Yan","Richard Penty","Qixiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.18458v4.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09219v1","updated":"2025-02-13T11:50:04Z","published":"2025-02-13T11:50:04Z","title":"Abduction of Domain Relationships from Data for VQA","summary":"  In this paper, we study the problem of visual question answering (VQA) where\nthe image and query are represented by ASP programs that lack domain data. We\nprovide an approach that is orthogonal and complementary to existing knowledge\naugmentation techniques where we abduce domain relationships of image\nconstructs from past examples. After framing the abduction problem, we provide\na baseline approach, and an implementation that significantly improves the\naccuracy of query answering yet requires few examples.\n","authors":["Al Mehdi Saadat Chowdhury","Paulo Shakarian","Gerardo I. Simari"],"pdf_url":"https://arxiv.org/pdf/2502.09219v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09213v1","updated":"2025-02-13T11:48:46Z","published":"2025-02-13T11:48:46Z","title":"Neuro-Symbolic Contrastive Learning for Cross-domain Inference","summary":"  Pre-trained language models (PLMs) have made significant advances in natural\nlanguage inference (NLI) tasks, however their sensitivity to textual\nperturbations and dependence on large datasets indicate an over-reliance on\nshallow heuristics. In contrast, inductive logic programming (ILP) excels at\ninferring logical relationships across diverse, sparse and limited datasets,\nbut its discrete nature requires the inputs to be precisely specified, which\nlimits their application. This paper proposes a bridge between the two\napproaches: neuro-symbolic contrastive learning. This allows for smooth and\ndifferentiable optimisation that improves logical accuracy across an otherwise\ndiscrete, noisy, and sparse topological space of logical functions. We show\nthat abstract logical relationships can be effectively embedded within a\nneuro-symbolic paradigm, by representing data as logic programs and sets of\nlogic rules. The embedding space captures highly varied textual information\nwith similar semantic logical relations, but can also separate similar textual\nrelations that have dissimilar logical relations. Experimental results\ndemonstrate that our approach significantly improves the inference capabilities\nof the models in terms of generalisation and reasoning.\n","authors":["Mingyue Liu","Ryo Ueda","Zhen Wan","Katsumi Inoue","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2502.09213v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2410.13914v5","updated":"2025-02-13T11:47:51Z","published":"2024-10-17T03:08:28Z","title":"Exogenous Matching: Learning Good Proposals for Tractable Counterfactual\n  Estimation","summary":"  We propose an importance sampling method for tractable and efficient\nestimation of counterfactual expressions in general settings, named Exogenous\nMatching. By minimizing a common upper bound of counterfactual estimators, we\ntransform the variance minimization problem into a conditional distribution\nlearning problem, enabling its integration with existing conditional\ndistribution modeling approaches. We validate the theoretical results through\nexperiments under various types and settings of Structural Causal Models (SCMs)\nand demonstrate the outperformance on counterfactual estimation tasks compared\nto other existing importance sampling methods. We also explore the impact of\ninjecting structural prior knowledge (counterfactual Markov boundaries) on the\nresults. Finally, we apply this method to identifiable proxy SCMs and\ndemonstrate the unbiasedness of the estimates, empirically illustrating the\napplicability of the method to practical scenarios.\n","authors":["Yikang Chen","Dehui Du","Lili Tian"],"pdf_url":"https://arxiv.org/pdf/2410.13914v5.pdf","comment":"51 pages, 15 figures. Accepted at NeurIPS 2024, see\n  https://papers.nips.cc/paper_files/paper/2024/hash/ee94bf235482e4c1f689c04c81656dbf-Abstract-Conference.html"},{"id":"http://arxiv.org/abs/2407.06447v2","updated":"2025-02-13T11:46:41Z","published":"2024-07-08T23:11:47Z","title":"Geospatial Trajectory Generation via Efficient Abduction: Deployment for\n  Independent Testing","summary":"  The ability to generate artificial human movement patterns while meeting\nlocation and time constraints is an important problem in the security\ncommunity, particularly as it enables the study of the analog problem of\ndetecting such patterns while maintaining privacy. We frame this problem as an\ninstance of abduction guided by a novel parsimony function represented as an\naggregate truth value over an annotated logic program. This approach has the\nadded benefit of affording explainability to an analyst user. By showing that\nany subset of such a program can provide a lower bound on this parsimony\nrequirement, we are able to abduce movement trajectories efficiently through an\ninformed (i.e., A*) search. We describe how our implementation was enhanced\nwith the application of multiple techniques in order to be scaled and\nintegrated with a cloud-based software stack that included bottom-up rule\nlearning, geolocated knowledge graph retrieval/management, and interfaces with\ngovernment systems for independently conducted government-run tests for which\nwe provide results. We also report on our own experiments showing that we not\nonly provide exact results but also scale to very large scenarios and provide\nrealistic agent trajectories that can go undetected by machine learning anomaly\ndetectors.\n","authors":["Divyagna Bavikadi","Dyuman Aditya","Devendra Parkar","Paulo Shakarian","Graham Mueller","Chad Parvis","Gerardo I. Simari"],"pdf_url":"https://arxiv.org/pdf/2407.06447v2.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09203v1","updated":"2025-02-13T11:43:43Z","published":"2025-02-13T11:43:43Z","title":"Revisiting Euclidean Alignment for Transfer Learning in EEG-Based\n  Brain-Computer Interfaces","summary":"  Due to the non-stationarity and large individual differences of EEG signals,\nEEG-based brain-computer interfaces (BCIs) usually need subject-specific\ncalibration to tailor the decoding algorithm for each new subject, which is\ntime-consuming and user-unfriendly, hindering their real-world applications.\nTransfer learning (TL) has been extensively used to expedite the calibration,\nby making use of EEG data from other subjects/sessions. An important\nconsideration in TL for EEG-based BCIs is to reduce the data distribution\ndiscrepancies among different subjects/session, to avoid negative transfer.\nEuclidean alignment (EA) was proposed in 2020 to address this challenge.\nNumerous experiments from 10 different BCI paradigms demonstrated its\neffectiveness and efficiency. This paper revisits the EA, explaining its\nprocedure and correct usage, introducing its applications and extensions, and\npointing out potential new research directions. It should be very helpful to\nBCI researchers, especially those who are working on EEG signal decoding.\n","authors":["Dongrui Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06913v2","updated":"2025-02-13T11:42:53Z","published":"2025-02-10T09:26:57Z","title":"A Simple yet Effective DDG Predictor is An Unsupervised Antibody\n  Optimizer and Explainer","summary":"  The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.\n","authors":["Lirong Wu","Yunfan Liu","Haitao Lin","Yufei Huang","Guojiang Zhao","Zhifeng Gao","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2502.06913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09198v1","updated":"2025-02-13T11:37:55Z","published":"2025-02-13T11:37:55Z","title":"Understanding High-Dimensional Bayesian Optimization","summary":"  Recent work reported that simple Bayesian optimization methods perform well\nfor high-dimensional real-world tasks, seemingly contradicting prior work and\ntribal knowledge. This paper investigates the 'why'. We identify fundamental\nchallenges that arise in high-dimensional Bayesian optimization and explain why\nrecent methods succeed. Our analysis shows that vanishing gradients caused by\nGaussian process initialization schemes play a major role in the failures of\nhigh-dimensional Bayesian optimization and that methods that promote local\nsearch behaviors are better suited for the task. We find that maximum\nlikelihood estimation of Gaussian process length scales suffices for\nstate-of-the-art performance. Based on this, we propose a simple variant of\nmaximum likelihood estimation called MSR that leverages these findings to\nachieve state-of-the-art performance on a comprehensive set of real-world\napplications. We also present targeted experiments to illustrate and confirm\nour findings.\n","authors":["Leonard Papenmeier","Matthias Poloczek","Luigi Nardi"],"pdf_url":"https://arxiv.org/pdf/2502.09198v1.pdf","comment":"19 pages, 20 figures"},{"id":"http://arxiv.org/abs/2502.09193v1","updated":"2025-02-13T11:33:17Z","published":"2025-02-13T11:33:17Z","title":"Generalizability through Explainability: Countering Overfitting with\n  Counterfactual Examples","summary":"  Overfitting is a well-known issue in machine learning that occurs when a\nmodel struggles to generalize its predictions to new, unseen data beyond the\nscope of its training set. Traditional techniques to mitigate overfitting\ninclude early stopping, data augmentation, and regularization. In this work, we\ndemonstrate that the degree of overfitting of a trained model is correlated\nwith the ability to generate counterfactual examples. The higher the\noverfitting, the easier it will be to find a valid counterfactual example for a\nrandomly chosen input data point. Therefore, we introduce CF-Reg, a novel\nregularization term in the training loss that controls overfitting by ensuring\nenough margin between each instance and its corresponding counterfactual.\nExperiments conducted across multiple datasets and models show that our\ncounterfactual regularizer generally outperforms existing regularization\ntechniques.\n","authors":["Flavio Giorgi","Fabiano Veglianti","Fabrizio Silvestri","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2502.09193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11619v3","updated":"2025-02-13T11:10:54Z","published":"2024-08-21T13:46:58Z","title":"Data-driven Modeling of Combined Sewer Systems for Urban Sustainability:\n  An Empirical Evaluation","summary":"  Climate change poses complex challenges, with extreme weather events becoming\nincreasingly frequent and difficult to model. Examples include the dynamics of\nCombined Sewer Systems (CSS). Overburdened CSS during heavy rainfall will\noverflow untreated wastewater into surface water bodies. Classical approaches\nto modeling the impact of extreme rainfall events rely on physical simulations,\nwhich are particularly challenging to create for large urban infrastructures.\nDeep Learning (DL) models offer a cost-effective alternative for modeling the\ncomplex dynamics of sewer systems. In this study, we present a comprehensive\nempirical evaluation of several state-of-the-art DL time series models for\npredicting sewer system dynamics in a large urban infrastructure, utilizing\nthree years of measurement data. We especially investigate the potential of DL\nmodels to maintain predictive precision during network outages by comparing\nglobal models, which have access to all variables within the sewer system, and\nlocal models, which are limited to data from a restricted set of local sensors.\nOur findings demonstrate that DL models can accurately predict the dynamics of\nsewer system load, even under network outage conditions. These results suggest\nthat DL models can effectively aid in balancing the load redistribution in CSS,\nthereby enhancing the sustainability and resilience of urban infrastructures.\n","authors":["Vipin Singh","Tianheng Ling","Teodor Chiaburu","Felix Biessmann"],"pdf_url":"https://arxiv.org/pdf/2408.11619v3.pdf","comment":"8 pages, 4 figures, accepted at 2nd Workshop on 'Public Interest AI'\n  co-located with 47th German Conference on Artificial Intelligence, Wuerzburg\n  23rd September 2024"},{"id":"http://arxiv.org/abs/2404.18573v2","updated":"2025-02-13T11:09:19Z","published":"2024-04-29T10:28:28Z","title":"Predicting Safety Misbehaviours in Autonomous Driving Systems using\n  Uncertainty Quantification","summary":"  The automated real-time recognition of unexpected situations plays a crucial\nrole in the safety of autonomous vehicles, especially in unsupported and\nunpredictable scenarios. This paper evaluates different Bayesian uncertainty\nquantification methods from the deep learning domain for the anticipatory\ntesting of safety-critical misbehaviours during system-level simulation-based\ntesting. Specifically, we compute uncertainty scores as the vehicle executes,\nfollowing the intuition that high uncertainty scores are indicative of\nunsupported runtime conditions that can be used to distinguish safe from\nfailure-inducing driving behaviors. In our study, we conducted an evaluation of\nthe effectiveness and computational overhead associated with two Bayesian\nuncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for\nmisbehaviour avoidance. Overall, for three benchmarks from the Udacity\nsimulator comprising both out-of-distribution and unsafe conditions introduced\nvia mutation testing, both methods successfully detected a high number of\nout-of-bounds episodes providing early warnings several seconds in advance,\noutperforming two state-of-the-art misbehaviour prediction methods based on\nautoencoders and attention maps in terms of effectiveness and efficiency.\nNotably, Deep Ensembles detected most misbehaviours without any false alarms\nand did so even when employing a relatively small number of models, making them\ncomputationally feasible for real-time detection. Our findings suggest that\nincorporating uncertainty quantification methods is a viable approach for\nbuilding fail-safe mechanisms in deep neural network-based autonomous vehicles.\n","authors":["Ruben Grewal","Paolo Tonella","Andrea Stocco"],"pdf_url":"https://arxiv.org/pdf/2404.18573v2.pdf","comment":"In proceedings of the 17th IEEE International Conference on Software\n  Testing, Verification and Validation 2024 (ICST '24)"},{"id":"http://arxiv.org/abs/2502.09173v1","updated":"2025-02-13T10:57:25Z","published":"2025-02-13T10:57:25Z","title":"Two-Stage Representation Learning for Analyzing Movement Behavior\n  Dynamics in People Living with Dementia","summary":"  In remote healthcare monitoring, time series representation learning reveals\ncritical patient behavior patterns from high-frequency data. This study\nanalyzes home activity data from individuals living with dementia by proposing\na two-stage, self-supervised learning approach tailored to uncover low-rank\nstructures. The first stage converts time-series activities into text sequences\nencoded by a pre-trained language model, providing a rich, high-dimensional\nlatent state space using a PageRank-based method. This PageRank vector captures\nlatent state transitions, effectively compressing complex behaviour data into a\nsuccinct form that enhances interpretability. This low-rank representation not\nonly enhances model interpretability but also facilitates clustering and\ntransition analysis, revealing key behavioral patterns correlated with\nclinicalmetrics such as MMSE and ADAS-COG scores. Our findings demonstrate the\nframework's potential in supporting cognitive status prediction, personalized\ncare interventions, and large-scale health monitoring.\n","authors":["Jin Cui","Alexander Capstick","Payam Barnaghi","Gregory Scott"],"pdf_url":"https://arxiv.org/pdf/2502.09173v1.pdf","comment":"AAAI 2025 Workshop on Large Language Models and Generative AI for\n  Health"},{"id":"http://arxiv.org/abs/2502.09172v1","updated":"2025-02-13T10:56:58Z","published":"2025-02-13T10:56:58Z","title":"LOB-Bench: Benchmarking Generative AI for Finance - an Application to\n  Limit Order Book Data","summary":"  While financial data presents one of the most challenging and interesting\nsequence modelling tasks due to high noise, heavy tails, and strategic\ninteractions, progress in this area has been hindered by the lack of consensus\non quantitative evaluation paradigms. To address this, we present LOB-Bench, a\nbenchmark, implemented in python, designed to evaluate the quality and realism\nof generative message-by-order data for limit order books (LOB) in the LOBSTER\nformat. Our framework measures distributional differences in conditional and\nunconditional statistics between generated and real LOB data, supporting\nflexible multivariate statistical evaluation. The benchmark also includes\nfeatures commonly used LOB statistics such as spread, order book volumes, order\nimbalance, and message inter-arrival times, along with scores from a trained\ndiscriminator network. Lastly, LOB-Bench contains \"market impact metrics\", i.e.\nthe cross-correlations and price response functions for specific events in the\ndata. We benchmark generative autoregressive state-space models, a (C)GAN, as\nwell as a parametric LOB model and find that the autoregressive GenAI approach\nbeats traditional model classes.\n","authors":["Peer Nagy","Sascha Frey","Kang Li","Bidipta Sarkar","Svitlana Vyetrenko","Stefan Zohren","Ani Calinescu","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2502.09172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09164v1","updated":"2025-02-13T10:48:11Z","published":"2025-02-13T10:48:11Z","title":"E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization","summary":"  We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.\n","authors":["Trung X. Pham","Zhang Kang","Ji Woo Hong","Xuran Zheng","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2502.09164v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.14630v2","updated":"2025-02-13T10:43:50Z","published":"2024-10-18T17:30:20Z","title":"On the Regularization of Learnable Embeddings for Time Series\n  Forecasting","summary":"  In forecasting multiple time series, accounting for the individual features\nof each sequence can be challenging. To address this, modern deep learning\nmethods for time series analysis combine a shared (global) model with local\nlayers, specific to each time series, often implemented as learnable\nembeddings. Ideally, these local embeddings should encode meaningful\nrepresentations of the unique dynamics of each sequence. However, when these\nare learned end-to-end as parameters of a forecasting model, they may end up\nacting as mere sequence identifiers. Shared processing blocks may then become\nreliant on such identifiers, limiting their transferability to new contexts. In\nthis paper, we address this issue by investigating methods to regularize the\nlearning of local learnable embeddings for time series processing.\nSpecifically, we perform the first extensive empirical study on the subject and\nshow how such regularizations consistently improve performance in widely\nadopted architectures. Furthermore, we show that methods attempting to prevent\nthe co-adaptation of local and global parameters by means of embeddings\nperturbation are particularly effective in this context. In this regard, we\ninclude in the comparison several perturbation-based regularization methods,\ngoing as far as periodically resetting the embeddings during training. The\nobtained results provide an important contribution to understanding the\ninterplay between learnable local parameters and shared processing layers: a\nkey challenge in modern time series processing models and a step toward\ndeveloping effective foundation models for time series.\n","authors":["Luca Butera","Giovanni De Felice","Andrea Cini","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2410.14630v2.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2502.09152v1","updated":"2025-02-13T10:29:31Z","published":"2025-02-13T10:29:31Z","title":"Vertical Federated Continual Learning via Evolving Prototype Knowledge","summary":"  Vertical Federated Learning (VFL) has garnered significant attention as a\nprivacy-preserving machine learning framework for sample-aligned feature\nfederation. However, traditional VFL approaches do not address the challenges\nof class and feature continual learning, resulting in catastrophic forgetting\nof knowledge from previous tasks. To address the above challenge, we propose a\nnovel vertical federated continual learning method, named Vertical Federated\nContinual Learning via Evolving Prototype Knowledge (V-LETO), which primarily\nfacilitates the transfer of knowledge from previous tasks through the evolution\nof prototypes. Specifically, we propose an evolving prototype knowledge method,\nenabling the global model to retain both previous and current task knowledge.\nFurthermore, we introduce a model optimization technique that mitigates the\nforgetting of previous task knowledge by restricting updates to specific\nparameters of the local model, thereby enhancing overall performance. Extensive\nexperiments conducted in both CIL and FIL settings demonstrate that our method,\nV-LETO, outperforms the other state-of-the-art methods. For example, our method\noutperforms the state-of-the-art method by 10.39% and 35.15% for CIL and FIL\ntasks, respectively. Our code is available at\nhttps://anonymous.4open.science/r/V-LETO-0108/README.md.\n","authors":["Shuo Wang","Keke Gai","Jing Yu","Liehuang Zhu","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05279v2","updated":"2025-02-13T10:28:42Z","published":"2025-01-09T14:43:08Z","title":"Learning convolution operators on compact Abelian groups","summary":"  We consider the problem of learning convolution operators associated to\ncompact Abelian groups. We study a regularization-based approach and provide\ncorresponding learning guarantees, discussing natural regularity condition on\nthe convolution kernel. More precisely, we assume the convolution kernel is a\nfunction in a translation invariant Hilbert space and analyze a natural ridge\nregression (RR) estimator. Building on existing results for RR, we characterize\nthe accuracy of the estimator in terms of finite sample bounds. Interestingly,\nregularity assumptions which are classical in the analysis of RR, have a novel\nand natural interpretation in terms of space/frequency localization.\nTheoretical results are illustrated by numerical simulations.\n","authors":["Emilia Magnani","Ernesto De Vito","Philipp Hennig","Lorenzo Rosasco"],"pdf_url":"https://arxiv.org/pdf/2501.05279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09151v1","updated":"2025-02-13T10:27:30Z","published":"2025-02-13T10:27:30Z","title":"Regularization can make diffusion models more efficient","summary":"  Diffusion models are one of the key architectures of generative AI. Their\nmain drawback, however, is the computational costs. This study indicates that\nthe concept of sparsity, well known especially in statistics, can provide a\npathway to more efficient diffusion pipelines. Our mathematical guarantees\nprove that sparsity can reduce the input dimension's influence on the\ncomputational complexity to that of a much smaller intrinsic dimension of the\ndata. Our empirical findings confirm that inducing sparsity can indeed lead to\nbetter samples at a lower cost.\n","authors":["Mahsa Taheri","Johannes Lederer"],"pdf_url":"https://arxiv.org/pdf/2502.09151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15421v2","updated":"2025-02-13T10:26:45Z","published":"2024-05-24T10:36:23Z","title":"Model-free reinforcement learning with noisy actions for automated\n  experimental control in optics","summary":"  Setting up and controlling optical systems is often a challenging and tedious\ntask. The high number of degrees of freedom to control mirrors, lenses, or\nphases of light makes automatic control challenging, especially when the\ncomplexity of the system cannot be adequately modeled due to noise or\nnon-linearities. Here, we show that reinforcement learning (RL) can overcome\nthese challenges when coupling laser light into an optical fiber, using a\nmodel-free RL approach that trains directly on the experiment without\npre-training. By utilizing the sample-efficient algorithms Soft Actor-Critic\n(SAC) or Truncated Quantile Critics (TQC), our agent learns to couple with 90%\nefficiency, comparable to the human expert. We demonstrate that direct training\non an experiment can replace extensive system modeling. Our result exemplifies\nRL's potential to tackle problems in optics, paving the way for more complex\napplications where full noise modeling is not feasible.\n","authors":["Lea Richtmann","Viktoria-S. Schmiesing","Dennis Wilken","Jan Heine","Aaron Tranter","Avishek Anand","Tobias J. Osborne","Michèle Heurs"],"pdf_url":"https://arxiv.org/pdf/2405.15421v2.pdf","comment":"10 pages + 12 pages appendices, 2 + 12 figures"},{"id":"http://arxiv.org/abs/2502.09150v1","updated":"2025-02-13T10:25:52Z","published":"2025-02-13T10:25:52Z","title":"Shortcut Learning Susceptibility in Vision Classifiers","summary":"  Shortcut learning, where machine learning models exploit spurious\ncorrelations in data instead of capturing meaningful features, poses a\nsignificant challenge to building robust and generalizable models. This\nphenomenon is prevalent across various machine learning applications, including\nvision, natural language processing, and speech recognition, where models may\nfind unintended cues that minimize training loss but fail to capture the\nunderlying structure of the data. Vision classifiers such as Convolutional\nNeural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers\n(ViTs) leverage distinct architectural principles to process spatial and\nstructural information, making them differently susceptible to shortcut\nlearning. In this study, we systematically evaluate these architectures by\nintroducing deliberate shortcuts into the dataset that are positionally\ncorrelated with class labels, creating a controlled setup to assess whether\nmodels rely on these artificial cues or learn actual distinguishing features.\nWe perform both quantitative evaluation by training on the shortcut-modified\ndataset and testing them on two different test sets -- one containing the same\nshortcuts and another without them -- to determine the extent of reliance on\nshortcuts. Additionally, qualitative evaluation is performed by using network\ninversion-based reconstruction techniques to analyze what the models\ninternalize in their weights, aiming to reconstruct the training data as\nperceived by the classifiers. We evaluate shortcut learning behavior across\nmultiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and\nCIFAR-10, to compare the susceptibility of different vision classifier\narchitectures to shortcut reliance and assess their varying degrees of\nsensitivity to spurious correlations.\n","authors":["Pirzada Suhail","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2502.09150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09143v1","updated":"2025-02-13T10:18:44Z","published":"2025-02-13T10:18:44Z","title":"Feature-based Graph Attention Networks Improve Online Continual Learning","summary":"  Online continual learning for image classification is crucial for models to\nadapt to new data while retaining knowledge of previously learned tasks. This\ncapability is essential to address real-world challenges involving dynamic\nenvironments and evolving data distributions. Traditional approaches\npredominantly employ Convolutional Neural Networks, which are limited to\nprocessing images as grids and primarily capture local patterns rather than\nrelational information. Although the emergence of transformer architectures has\nimproved the ability to capture relationships, these models often require\nsignificantly larger resources. In this paper, we present a novel online\ncontinual learning framework based on Graph Attention Networks (GATs), which\neffectively capture contextual relationships and dynamically update the\ntask-specific representation via learned attention weights. Our approach\nutilizes a pre-trained feature extractor to convert images into graphs using\nhierarchical feature maps, representing information at varying levels of\ngranularity. These graphs are then processed by a GAT and incorporate an\nenhanced global pooling strategy to improve classification performance for\ncontinual learning. In addition, we propose the rehearsal memory duplication\ntechnique that improves the representation of the previous tasks while\nmaintaining the memory budget. Comprehensive evaluations on benchmark datasets,\nincluding SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the\nsuperiority of our method compared to the state-of-the-art methods.\n","authors":["Adjovi Sim","Zhengkui Wang","Aik Beng Ng","Shalini De Mello","Simon See","Wonmin Byeon"],"pdf_url":"https://arxiv.org/pdf/2502.09143v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.09140v1","updated":"2025-02-13T10:15:16Z","published":"2025-02-13T10:15:16Z","title":"Replay-free Online Continual Learning with Self-Supervised MultiPatches","summary":"  Online Continual Learning (OCL) methods train a model on a non-stationary\ndata stream where only a few examples are available at a time, often leveraging\nreplay strategies. However, usage of replay is sometimes forbidden, especially\nin applications with strict privacy regulations. Therefore, we propose\nContinual MultiPatches (CMP), an effective plug-in for existing OCL\nself-supervised learning strategies that avoids the use of replay samples. CMP\ngenerates multiple patches from a single example and projects them into a\nshared feature space, where patches coming from the same example are pushed\ntogether without collapsing into a single point. CMP surpasses replay and other\nSSL-based strategies on OCL streams, challenging the role of replay as a go-to\nsolution for self-supervised OCL.\n","authors":["Giacomo Cignoni","Andrea Cossu","Alex Gomez-Villa","Joost van de Weijer","Antonio Carta"],"pdf_url":"https://arxiv.org/pdf/2502.09140v1.pdf","comment":"Accepted at ESANN 2025"},{"id":"http://arxiv.org/abs/2502.09137v1","updated":"2025-02-13T10:13:20Z","published":"2025-02-13T10:13:20Z","title":"Trust Me, I Know the Way: Predictive Uncertainty in the Presence of\n  Shortcut Learning","summary":"  The correct way to quantify predictive uncertainty in neural networks remains\na topic of active discussion. In particular, it is unclear whether the\nstate-of-the art entropy decomposition leads to a meaningful representation of\nmodel, or epistemic, uncertainty (EU) in the light of a debate that pits\nignorance against disagreement perspectives. We aim to reconcile the\nconflicting viewpoints by arguing that both are valid but arise from different\nlearning situations. Notably, we show that the presence of shortcuts is\ndecisive for EU manifesting as disagreement.\n","authors":["Lisa Wimmer","Bernd Bischl","Ludwig Bothmann"],"pdf_url":"https://arxiv.org/pdf/2502.09137v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.09135v1","updated":"2025-02-13T10:11:36Z","published":"2025-02-13T10:11:36Z","title":"Interpreting and Steering Protein Language Models through Sparse\n  Autoencoders","summary":"  The rapid advancements in transformer-based language models have\nrevolutionized natural language processing, yet understanding the internal\nmechanisms of these models remains a significant challenge. This paper explores\nthe application of sparse autoencoders (SAE) to interpret the internal\nrepresentations of protein language models, specifically focusing on the ESM-2\n8M parameter model. By performing a statistical analysis on each latent\ncomponent's relevance to distinct protein annotations, we identify potential\ninterpretations linked to various protein characteristics, including\ntransmembrane regions, binding sites, and specialized motifs.\n  We then leverage these insights to guide sequence generation, shortlisting\nthe relevant latent components that can steer the model towards desired targets\nsuch as zinc finger domains. This work contributes to the emerging field of\nmechanistic interpretability in biological sequence models, offering new\nperspectives on model steering for sequence design.\n","authors":["Edith Natalia Villegas Garcia","Alessio Ansuini"],"pdf_url":"https://arxiv.org/pdf/2502.09135v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09130v1","updated":"2025-02-13T10:07:35Z","published":"2025-02-13T10:07:35Z","title":"Finite-Time Analysis of Discrete-Time Stochastic Interpolants","summary":"  The stochastic interpolant framework offers a powerful approach for\nconstructing generative models based on ordinary differential equations (ODEs)\nor stochastic differential equations (SDEs) to transform arbitrary data\ndistributions. However, prior analyses of this framework have primarily focused\non the continuous-time setting, assuming a perfect solution of the underlying\nequations. In this work, we present the first discrete-time analysis of the\nstochastic interpolant framework, where we introduce an innovative\ndiscrete-time sampler and derive a finite-time upper bound on its distribution\nestimation error. Our result provides a novel quantification of how different\nfactors, including the distance between source and target distributions and\nestimation accuracy, affect the convergence rate and also offers a new\nprincipled way to design efficient schedules for convergence acceleration.\nFinally, numerical experiments are conducted on the discrete-time sampler to\ncorroborate our theoretical findings.\n","authors":["Yuhao Liu","Yu Chen","Rui Hu","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2502.09130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09128v1","updated":"2025-02-13T10:05:44Z","published":"2025-02-13T10:05:44Z","title":"A Novel Dialect-Aware Framework for the Classification of Arabic\n  Dialects and Emotions","summary":"  Arabic is one of the oldest languages still in use today. As a result,\nseveral Arabic-speaking regions have developed dialects that are unique to\nthem. Dialect and emotion recognition have various uses in Arabic text\nanalysis, such as determining an online customer's origin based on their\ncomments. Furthermore, intelligent chatbots that are aware of a user's emotions\ncan respond appropriately to the user. Current research in emotion detection in\nthe Arabic language lacks awareness of how emotions are exhibited in different\ndialects, which motivates the work found in this study. This research addresses\nthe problems of dialect and emotion classification in Arabic. Specifically,\nthis is achieved by building a novel framework that can identify and predict\nArabic dialects and emotions from a given text. The framework consists of three\nmodules: A text-preprocessing module, a classification module, and a clustering\nmodule with the novel capability of building new dialect-aware emotion\nlexicons. The proposed framework generated a new emotional lexicon for\ndifferent dialects. It achieved an accuracy of 88.9% in classifying Arabic\ndialects, which outperforms the state-of-the-art results by 6.45 percentage\npoints. Furthermore, the framework achieved 89.1-79% accuracy in detecting\nemotions in the Egyptian and Gulf dialects, respectively.\n","authors":["Nasser A Alsadhan"],"pdf_url":"https://arxiv.org/pdf/2502.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v3","updated":"2025-02-13T10:00:58Z","published":"2024-10-11T15:50:31Z","title":"An Overview of Prototype Formulations for Interpretable Deep Learning","summary":"  Prototypical part networks offer interpretable alternatives to black-box deep\nlearning models. However, many of these networks rely on Euclidean prototypes,\nwhich may limit their flexibility. This work provides a comprehensive overview\nof various prototype formulations. Experiments conducted on the CUB-200-2011,\nStanford Cars, and Oxford Flowers datasets demonstrate the effectiveness and\nversatility of these different formulations.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v3.pdf","comment":"Equal Contribution of M.X.Li and K.F.Rudolf"},{"id":"http://arxiv.org/abs/2303.03388v3","updated":"2025-02-13T10:00:15Z","published":"2023-03-03T07:09:17Z","title":"Multi-modal Multi-kernel Graph Learning for Autism Prediction and\n  Biomarker Discovery","summary":"  Due to its complexity, graph learning-based multi-modal integration and\nclassification is one of the most challenging obstacles for disease prediction.\nTo effectively offset the negative impact between modalities in the process of\nmulti-modal integration and extract heterogeneous information from graphs, we\npropose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning).\nFor the problem of negative impact between modalities, we propose a multi-modal\ngraph embedding module to construct a multi-modal graph. Different from\nconventional methods that manually construct static graphs for all modalities,\neach modality generates a separate graph by adaptive learning, where a function\ngraph and a supervision graph are introduced for optimization during the\nmulti-graph fusion embedding process. We then propose a multi-kernel graph\nlearning module to extract heterogeneous information from the multi-modal\ngraph. The information in the multi-modal graph at different levels is\naggregated by convolutional kernels with different receptive field sizes,\nfollowed by generating a cross-kernel discovery tensor for disease prediction.\nOur method is evaluated on the benchmark Autism Brain Imaging Data Exchange\n(ABIDE) dataset and outperforms the state-of-the-art methods. In addition,\ndiscriminative brain regions associated with autism are identified by our\nmodel, providing guidance for the study of autism pathology.\n","authors":["Jin Liu","Junbin Mao","Hanhe Lin","Hulin Kuang","Shirui Pan","Xusheng Wu","Shan Xie","Fei Liu","Yi Pan"],"pdf_url":"https://arxiv.org/pdf/2303.03388v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09624v2","updated":"2025-02-13T09:57:50Z","published":"2024-06-13T23:19:48Z","title":"DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational\n  Fluid Dynamics Simulations and Deep Learning Benchmarks","summary":"  We present DrivAerNet++, the largest and most comprehensive multimodal\ndataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car\ndesigns modeled with high-fidelity computational fluid dynamics (CFD)\nsimulations. The dataset includes diverse car configurations such as fastback,\nnotchback, and estateback, with different underbody and wheel designs to\nrepresent both internal combustion engines and electric vehicles. Each entry in\nthe dataset features detailed 3D meshes, parametric models, aerodynamic\ncoefficients, and extensive flow and surface field data, along with segmented\nparts for car classification and point cloud data. This dataset supports a wide\narray of machine learning applications including data-driven design\noptimization, generative modeling, surrogate model training, CFD simulation\nacceleration, and geometric classification. With more than 39 TB of publicly\navailable engineering data, DrivAerNet++ fills a significant gap in available\nresources, providing high-quality, diverse data to enhance model training,\npromote generalization, and accelerate automotive design processes. Along with\nrigorous dataset validation, we also provide ML benchmarking results on the\ntask of aerodynamic drag prediction, showcasing the breadth of applications\nsupported by our dataset. This dataset is set to significantly impact\nautomotive design and broader engineering disciplines by fostering innovation\nand improving the fidelity of aerodynamic evaluations. Dataset and code\navailable at: https://github.com/Mohamedelrefaie/DrivAerNet.\n","authors":["Mohamed Elrefaie","Florin Morar","Angela Dai","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2406.09624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09122v1","updated":"2025-02-13T09:57:25Z","published":"2025-02-13T09:57:25Z","title":"Improving Deep Regression with Tightness","summary":"  For deep regression, preserving the ordinality of the targets with respect to\nthe feature representation improves performance across various tasks. However,\na theoretical explanation for the benefits of ordinality is still lacking. This\nwork reveals that preserving ordinality reduces the conditional entropy\n$H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our\nfindings reveal that typical regression losses do little to reduce $H(Z|Y)$,\neven though it is vital for generalization performance. With this motivation,\nwe introduce an optimal transport-based regularizer to preserve the similarity\nrelationships of targets in the feature space to reduce $H(Z|Y)$. Additionally,\nwe introduce a simple yet efficient strategy of duplicating the regressor\ntargets, also with the aim of reducing $H(Z|Y)$. Experiments on three\nreal-world regression tasks verify the effectiveness of our strategies to\nimprove deep regression. Code:\nhttps://github.com/needylove/Regression_tightness.\n","authors":["Shihao Zhang","Yuguang Yan","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.09122v1.pdf","comment":"ICLR 2025, Code: https://github.com/needylove/Regression_tightness"},{"id":"http://arxiv.org/abs/2404.03713v2","updated":"2025-02-13T09:48:12Z","published":"2024-04-04T17:46:20Z","title":"Explaining Explainability: Recommendations for Effective Use of Concept\n  Activation Vectors","summary":"  Concept-based explanations translate the internal representations of deep\nlearning models into a language that humans are familiar with: concepts. One\npopular method for finding concepts is Concept Activation Vectors (CAVs), which\nare learnt using a probe dataset of concept exemplars. In this work, we\ninvestigate three properties of CAVs: (1) inconsistency across layers, (2)\nentanglement with other concepts, and (3) spatial dependency. Each property\nprovides both challenges and opportunities in interpreting models. We introduce\ntools designed to detect the presence of these properties, provide insight into\nhow each property can lead to misleading explanations, and provide\nrecommendations to mitigate their impact. To demonstrate practical\napplications, we apply our recommendations to a melanoma classification task,\nshowing how entanglement can lead to uninterpretable results and that the\nchoice of negative probe set can have a substantial impact on the meaning of a\nCAV. Further, we show that understanding these properties can be used to our\nadvantage. For example, we introduce spatially dependent CAVs to test if a\nmodel is translation invariant with respect to a specific concept and class.\nOur experiments are performed on natural images (ImageNet), skin lesions (ISIC\n2019), and a new synthetic dataset, Elements. Elements is designed to capture a\nknown ground truth relationship between concepts and classes. We release this\ndataset to facilitate further research in understanding and evaluating\ninterpretability methods.\n","authors":["Angus Nicolson","Lisa Schut","J. Alison Noble","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2404.03713v2.pdf","comment":"Accepted by Transactions on Machine Learning Research (02/2025)"},{"id":"http://arxiv.org/abs/2502.08644v2","updated":"2025-02-13T09:48:02Z","published":"2025-02-12T18:58:34Z","title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and\n  learning in neural networks","summary":"  The brain can rapidly adapt to new contexts and learn from limited data, a\ncoveted characteristic that artificial intelligence algorithms have struggled\nto mimic. Inspired by oscillatory rhythms of the mechanical structures of\nneural cells, we developed a learning paradigm that is based on oscillations in\nlink strengths and associates learning with the coordination of these\noscillations. We find that this paradigm yields rapid adaptation and learning\nin artificial neural networks. Link oscillations can rapidly change\ncoordination, endowing the network with the ability to sense subtle context\nchanges in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture\ncapable of predicting dynamics in multiple contexts. Oscillations also allow\nthe network to extrapolate dynamics to never-seen-before contexts. These\ncapabilities make our learning paradigm a powerful starting point for novel\nmodels of learning and cognition. Furthermore, learning through link\ncoordination is agnostic to the specifics of the neural network architecture,\nhence our study opens the door for introducing rapid adaptation and learning\ncapabilities into leading AI models.\n","authors":["Hoony Kang","Wolfgang Losert"],"pdf_url":"https://arxiv.org/pdf/2502.08644v2.pdf","comment":"13 pages, 3 figures v.2 comments: Updated email, updated typo on\n  p.11: h -> h^2 for RMSE"},{"id":"http://arxiv.org/abs/2501.19334v2","updated":"2025-02-13T09:47:38Z","published":"2025-01-31T17:34:53Z","title":"The Value of Prediction in Identifying the Worst-Off","summary":"  Machine learning is increasingly used in government programs to identify and\nsupport the most vulnerable individuals, prioritizing assistance for those at\ngreatest risk over optimizing aggregate outcomes. This paper examines the\nwelfare impacts of prediction in equity-driven contexts, and how they compare\nto other policy levers, such as expanding bureaucratic capacity. Through\nmathematical models and a real-world case study on long-term unemployment\namongst German residents, we develop a comprehensive understanding of the\nrelative effectiveness of prediction in surfacing the worst-off. Our findings\nprovide clear analytical frameworks and practical, data-driven tools that\nempower policymakers to make principled decisions when designing these systems.\n","authors":["Unai Fischer-Abaigar","Christoph Kern","Juan Carlos Perdomo"],"pdf_url":"https://arxiv.org/pdf/2501.19334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02308v2","updated":"2025-02-13T09:38:00Z","published":"2025-02-04T13:24:28Z","title":"Real-Time Operator Takeover for Visuomotor Diffusion Policy Training","summary":"  We present a Real-Time Operator Takeover (RTOT) paradigm enabling operators\nto seamlessly take control of a live visuomotor diffusion policy, guiding the\nsystem back into desirable states or reinforcing specific demonstrations. We\npresent new insights in using the Mahalonobis distance to automatically\nidentify undesirable states. Once the operator has intervened and redirected\nthe system, the control is seamlessly returned to the policy, which resumes\ngenerating actions until further intervention is required. We demonstrate that\nincorporating the targeted takeover demonstrations significantly improves\npolicy performance compared to training solely with an equivalent number of,\nbut longer, initial demonstrations. We provide an in-depth analysis of using\nthe Mahalanobis distance to detect out-of-distribution states, illustrating its\nutility for identifying critical failure points during execution. Supporting\nmaterials, including videos of initial and takeover demonstrations and all rice\nscooping experiments, are available on the project website:\nhttps://operator-takeover.github.io/\n","authors":["Nils Ingelhag","Jesper Munkeby","Michael C. Welle","Marco Moletta","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2502.02308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06099v2","updated":"2025-02-13T09:35:44Z","published":"2024-06-10T08:34:13Z","title":"Sequential Binary Classification for Intrusion Detection","summary":"  Network Intrusion Detection Systems (IDS) have become increasingly important\nas networks become more vulnerable to new and sophisticated attacks. Machine\nLearning (ML)-based IDS are increasingly seen as the most effective approach to\nhandle this issue. However, IDS datasets suffer from high class imbalance,\nwhich impacts the performance of standard ML models. Different from existing\ndata-driven techniques to handling class imbalance, this paper explores a\nstructural approach to handling class imbalance in multi-class classification\n(MCC) problems. The proposed approach - Sequential Binary Classification (SBC),\nis a hierarchical cascade of (regular) binary classifiers. Experiments on\nbenchmark IDS datasets demonstrate that the structural approach to handling\nclass-imbalance, as exemplified by SBC, is a viable approach to handling the\nissue.\n","authors":["Shrihari Vasudevan","Ishan Chokshi","Raaghul Ranganathan","Nachiappan Sundaram"],"pdf_url":"https://arxiv.org/pdf/2406.06099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09106v1","updated":"2025-02-13T09:29:04Z","published":"2025-02-13T09:29:04Z","title":"Scaling Law for Stochastic Gradient Descent in Quadratically\n  Parameterized Linear Regression","summary":"  In machine learning, the scaling law describes how the model performance\nimproves with the model and data size scaling up. From a learning theory\nperspective, this class of results establishes upper and lower generalization\nbounds for a specific learning algorithm. Here, the exact algorithm running\nusing a specific model parameterization often offers a crucial implicit\nregularization effect, leading to good generalization. To characterize the\nscaling law, previous theoretical studies mainly focus on linear models,\nwhereas, feature learning, a notable process that contributes to the remarkable\nempirical success of neural networks, is regretfully vacant. This paper studies\nthe scaling law over a linear regression with the model being quadratically\nparameterized. We consider infinitely dimensional data and slope ground truth,\nboth signals exhibiting certain power-law decay rates. We study convergence\nrates for Stochastic Gradient Descent and demonstrate the learning rates for\nvariables will automatically adapt to the ground truth. As a result, in the\ncanonical linear regression, we provide explicit separations for generalization\ncurves between SGD with and without feature learning, and the\ninformation-theoretical lower bound that is agnostic to parametrization method\nand the algorithm. Our analysis for decaying ground truth provides a new\ncharacterization for the learning dynamic of the model.\n","authors":["Shihong Ding","Haihan Zhang","Hanzhen Zhao","Cong Fang"],"pdf_url":"https://arxiv.org/pdf/2502.09106v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.07538v2","updated":"2025-02-13T14:21:12Z","published":"2025-02-11T13:24:38Z","title":"Visual-based spatial audio generation system for multi-speaker\n  environments","summary":"  In multimedia applications such as films and video games, spatial audio\ntechniques are widely employed to enhance user experiences by simulating 3D\nsound: transforming mono audio into binaural formats. However, this process is\noften complex and labor-intensive for sound designers, requiring precise\nsynchronization of audio with the spatial positions of visual components. To\naddress these challenges, we propose a visual-based spatial audio generation\nsystem - an automated system that integrates face detection YOLOv8 for object\ndetection, monocular depth estimation, and spatial audio techniques. Notably,\nthe system operates without requiring additional binaural dataset training. The\nproposed system is evaluated against existing Spatial Audio generation system\nusing objective metrics. Experimental results demonstrate that our method\nsignificantly improves spatial consistency between audio and video, enhances\nspeech quality, and performs robustly in multi-speaker scenarios. By\nstreamlining the audio-visual alignment process, the proposed system enables\nsound engineers to achieve high-quality results efficiently, making it a\nvaluable tool for professionals in multimedia production.\n","authors":["Xiaojing Liu","Ogulcan Gurelli","Yan Wang","Joshua Reiss"],"pdf_url":"https://arxiv.org/pdf/2502.07538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14755v2","updated":"2025-02-13T03:15:41Z","published":"2024-04-23T05:36:33Z","title":"SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework\n  with Interactive Vision-Language Models","summary":"  With the continuous advancement of vision language models (VLMs) technology,\nremarkable research achievements have emerged in the dermatology field, the\nfourth most prevalent human disease category. However, despite these\nadvancements, VLM still faces explainable problems to user in diagnosis due to\nthe inherent complexity of dermatological conditions, existing tools offer\nrelatively limited support for user comprehension. We propose SkinGEN, a\ndiagnosis-to-generation framework that leverages the stable diffusion(SD) model\nto generate reference demonstrations from diagnosis results provided by VLM,\nthereby enhancing the visual explainability for users. Through extensive\nexperiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for\nskin condition image generation. We conduct a user study with 32 participants\nevaluating both the system performance and explainability. Results demonstrate\nthat SkinGEN significantly improves users' comprehension of VLM predictions and\nfosters increased trust in the diagnostic process. This work paves the way for\nmore transparent and user-centric VLM applications in dermatology and beyond.\n","authors":["Bo Lin","Yingjing Xu","Xuanwen Bao","Zhou Zhao","Zhouyang Wang","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2404.14755v2.pdf","comment":null}]},"2025-02-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.06453v2","updated":"2025-02-12T23:16:27Z","published":"2025-02-10T13:31:46Z","title":"MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations","summary":"  Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.\n","authors":["Kaixuan Huang","Jiacheng Guo","Zihao Li","Xiang Ji","Jiawei Ge","Wenzhe Li","Yingqing Guo","Tianle Cai","Hui Yuan","Runzhe Wang","Yue Wu","Ming Yin","Shange Tang","Yangsibo Huang","Chi Jin","Xinyun Chen","Chiyuan Zhang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06453v2.pdf","comment":"v2: fix bugs in Fig. 1"},{"id":"http://arxiv.org/abs/2406.11278v3","updated":"2025-02-12T23:08:21Z","published":"2024-06-17T07:30:40Z","title":"Do Not Design, Learn: A Trainable Scoring Function for Uncertainty\n  Estimation in Generative LLMs","summary":"  Uncertainty estimation (UE) of generative large language models (LLMs) is\ncrucial for evaluating the reliability of generated sequences. A significant\nsubset of UE methods utilize token probabilities to assess uncertainty,\naggregating multiple token probabilities into a single UE score using a scoring\nfunction. Existing scoring functions for probability-based UE, such as\nlength-normalized scoring and semantic contribution-based weighting, are\ndesigned to solve certain aspects of the problem but exhibit limitations,\nincluding the inability to handle biased probabilities and complex semantic\ndependencies between tokens. To address these issues, in this work, we propose\nLearnable Response Scoring (LARS) function, a novel scoring function that\nleverages supervised data to capture complex dependencies between tokens and\nprobabilities, thereby producing more reliable and calibrated response scores\nin computing the uncertainty of LLM generations. Our comprehensive experiments\nacross question-answering and arithmetical reasoning tasks with various\ndatasets demonstrate that LARS significantly outperforms existing scoring\nfunctions, achieving improvements of up to 16\\% AUROC score.\n","authors":["Duygu Nur Yaldiz","Yavuz Faruk Bakman","Baturalp Buyukates","Chenyang Tao","Anil Ramakrishna","Dimitrios Dimitriadis","Jieyu Zhao","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2406.11278v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08826v1","updated":"2025-02-12T22:33:41Z","published":"2025-02-12T22:33:41Z","title":"Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.\n","authors":["Mohammad Mahdi Abootorabi","Amirhosein Zobeiri","Mahdi Dehghani","Mohammadali Mohammadkhani","Bardia Mohammadi","Omid Ghahroodi","Mahdieh Soleymani Baghshah","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2502.08826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08825v1","updated":"2025-02-12T22:30:18Z","published":"2025-02-12T22:30:18Z","title":"Examining and Adapting Time for Multilingual Classification via Mixture\n  of Temporal Experts","summary":"  Time is implicitly embedded in classification process: classifiers are\nusually built on existing data while to be applied on future data whose\ndistributions (e.g., label and token) may change. However, existing\nstate-of-the-art classification models merely consider the temporal variations\nand primarily focus on English corpora, which leaves temporal studies less\nexplored, let alone under multilingual settings. In this study, we fill the gap\nby treating time as domains (e.g., 2024 vs. 2025), examining temporal effects,\nand developing a domain adaptation framework to generalize classifiers over\ntime on multiple languages. Our framework proposes Mixture of Temporal Experts\n(MoTE) to leverage both semantic and data distributional shifts to learn and\nadapt temporal trends into classification models. Our analysis shows\nclassification performance varies over time across different languages, and we\nexperimentally demonstrate that MoTE can enhance classifier generalizability\nover temporal data shifts. Our study provides analytic insights and addresses\nthe need for time-aware models that perform robustly in multilingual scenarios.\n","authors":["Weisi Liu","Guangzeng Han","Xiaolei Huang"],"pdf_url":"https://arxiv.org/pdf/2502.08825v1.pdf","comment":"accept to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.04397v2","updated":"2025-02-12T22:26:50Z","published":"2025-02-06T06:58:09Z","title":"Multimodal Medical Code Tokenizer","summary":"  Foundation models trained on patient electronic health records (EHRs) require\ntokenizing medical data into sequences of discrete vocabulary items. Existing\ntokenizers treat medical codes from EHRs as isolated textual tokens. However,\neach medical code is defined by its textual description, its position in\nontological hierarchies, and its relationships to other codes, such as disease\nco-occurrences and drug-treatment associations. Medical vocabularies contain\nmore than 600,000 codes with critical information for clinical reasoning. We\nintroduce MedTok, a multimodal medical code tokenizer that uses the text\ndescriptions and relational context of codes. MedTok processes text using a\nlanguage model encoder and encodes the relational structure with a graph\nencoder. It then quantizes both modalities into a unified token space,\npreserving modality-specific and cross-modality information. We integrate\nMedTok into five EHR models and evaluate it on operational and clinical tasks\nacross in-patient and out-patient datasets, including outcome prediction,\ndiagnosis classification, drug recommendation, and risk stratification.\nSwapping standard EHR tokenizers with MedTok improves AUPRC across all EHR\nmodels, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with\nthe largest gains in drug recommendation. Beyond EHR modeling, we demonstrate\nusing MedTok tokenizer with medical QA systems. Our results demonstrate the\npotential of MedTok as a unified tokenizer for medical codes, improving\ntokenization for medical foundation models.\n","authors":["Xiaorui Su","Shvat Messica","Yepeng Huang","Ruth Johnson","Lukas Fesser","Shanghua Gao","Faryad Sahneh","Marinka Zitnik"],"pdf_url":"https://arxiv.org/pdf/2502.04397v2.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2502.08820v1","updated":"2025-02-12T22:18:34Z","published":"2025-02-12T22:18:34Z","title":"Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CALM: A Unified Conversational Agentic Language Model","summary":"  Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and\nCALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.\n","authors":["Emre Can Acikgoz","Jeremiah Greer","Akul Datta","Ze Yang","William Zeng","Oussama Elachqar","Emmanouil Koukoumidis","Dilek Hakkani-Tür","Gokhan Tur"],"pdf_url":"https://arxiv.org/pdf/2502.08820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08818v1","updated":"2025-02-12T22:11:07Z","published":"2025-02-12T22:11:07Z","title":"Lexical Manifold Reconfiguration in Large Language Models: A Novel\n  Architectural Approach for Contextual Modulation","summary":"  Contextual adaptation in token embeddings plays a central role in determining\nhow well language models maintain coherence and retain semantic relationships\nover extended text sequences. Static embeddings often impose constraints on\nlexical flexibility, leading to suboptimal performance when faced with complex\nsentence structures or domain-specific terminology shifts. To address this\nlimitation, a structured approach was developed for dynamically reconfiguring\ntoken embeddings through continuous geometric transformations, ensuring that\nrepresentations evolved in response to evolving discourse structures. A\nmanifold-based transformation mechanism was integrated to regulate lexical\npositioning, allowing embeddings to undergo controlled shifts while preserving\nlinguistic relationships across varying textual contexts. Empirical evaluations\ndemonstrated that embedding reconfiguration contributed to reductions in\nperplexity, improved lexical coherence, and enhanced sentence-level continuity,\nparticularly in structured and domain-adaptive text generation tasks.\nComparative analyses of embedding drift indicated that dynamically restructured\nrepresentations maintained stronger contextual consistency, reducing\nmisalignment in token dependencies while preserving fluency in language\nmodeling outputs. Computational overhead assessments confirmed that while\ntraining complexity increased due to the iterative refinement of embeddings,\ninference remained efficient, ensuring practical feasibility for real-time\ngeneration. Evaluations across multiple datasets further demonstrated that\ndynamically modulated embeddings exhibited broader lexical diversity, reducing\nrepetitive token patterns and enabling a more adaptable representation learning\nprocess.\n","authors":["Koinis Vassilis","Godfrey Milbourne","Harriet Featherstone","Xanthe Peverell","Yorick Bletchley","Zachary Montford"],"pdf_url":"https://arxiv.org/pdf/2502.08818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09298v4","updated":"2025-02-12T21:57:06Z","published":"2024-07-12T14:31:05Z","title":"Transformer Layers as Painters","summary":"  Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.\n","authors":["Qi Sun","Marc Pickett","Aakash Kumar Nain","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2407.09298v4.pdf","comment":"13 pages total, including references and appendices"},{"id":"http://arxiv.org/abs/2407.12101v2","updated":"2025-02-12T21:48:22Z","published":"2024-07-16T18:09:21Z","title":"Better RAG using Relevant Information Gain","summary":"  A common way to extend the memory of large language models (LLMs) is by\nretrieval augmented generation (RAG), which inserts text retrieved from a\nlarger memory into an LLM's context window. However, the context window is\ntypically limited to several thousand tokens, which limits the number of\nretrieved passages that can inform a model's response. For this reason, it's\nimportant to avoid occupying context window space with redundant information by\nensuring a degree of diversity among retrieved passages. At the same time, the\ninformation should also be relevant to the current task. Most prior methods\nthat encourage diversity among retrieved results, such as Maximal Marginal\nRelevance (MMR), do so by incorporating an objective that explicitly trades off\ndiversity and relevance. We propose a novel simple optimization metric based on\nrelevant information gain, a probabilistic measure of the total information\nrelevant to a query for a set of retrieved results. By optimizing this metric,\ndiversity organically emerges from our system. When used as a drop-in\nreplacement for the retrieval component of a RAG system, this method yields\nstate-of-the-art performance on question answering tasks from the Retrieval\nAugmented Generation Benchmark (RGB), outperforming existing metrics that\ndirectly optimize for relevance and diversity.\n","authors":["Marc Pickett","Jeremy Hartman","Ayan Kumar Bhowmick","Raquib-ul Alam","Aditya Vempaty"],"pdf_url":"https://arxiv.org/pdf/2407.12101v2.pdf","comment":"4 page paper submitted to EMNLP"},{"id":"http://arxiv.org/abs/2502.08796v1","updated":"2025-02-12T21:19:30Z","published":"2025-02-12T21:19:30Z","title":"A Systematic Review on the Evaluation of Large Language Models in Theory\n  of Mind Tasks","summary":"  In recent years, evaluating the Theory of Mind (ToM) capabilities of large\nlanguage models (LLMs) has received significant attention within the research\ncommunity. As the field rapidly evolves, navigating the diverse approaches and\nmethodologies has become increasingly complex. This systematic review\nsynthesizes current efforts to assess LLMs' ability to perform ToM tasks, an\nessential aspect of human cognition involving the attribution of mental states\nto oneself and others. Despite notable advancements, the proficiency of LLMs in\nToM remains a contentious issue. By categorizing benchmarks and tasks through a\ntaxonomy rooted in cognitive science, this review critically examines\nevaluation techniques, prompting strategies, and the inherent limitations of\nLLMs in replicating human-like mental state reasoning. A recurring theme in the\nliterature reveals that while LLMs demonstrate emerging competence in ToM\ntasks, significant gaps persist in their emulation of human cognitive\nabilities.\n","authors":["Karahan Sarıtaş","Kıvanç Tezören","Yavuz Durmazkeser"],"pdf_url":"https://arxiv.org/pdf/2502.08796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08788v1","updated":"2025-02-12T21:01:10Z","published":"2025-02-12T21:01:10Z","title":"If Multi-Agent Debate is the Answer, What is the Question?","summary":"  Multi-agent debate (MAD) has emerged as a promising approach to enhance the\nfactual accuracy and reasoning quality of large language models (LLMs) by\nengaging multiple agents in iterative discussions during inference. Despite its\npotential, we argue that current MAD research suffers from critical\nshortcomings in evaluation practices, including limited dataset overlap and\ninconsistent baselines, raising significant concerns about generalizability.\nCorrespondingly, this paper presents a systematic evaluation of five\nrepresentative MAD methods across nine benchmarks using four foundational\nmodels. Surprisingly, our findings reveal that MAD methods fail to reliably\noutperform simple single-agent baselines such as Chain-of-Thought and\nSelf-Consistency, even when consuming additional inference-time computation.\nFrom our analysis, we found that model heterogeneity can significantly improve\nMAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the\noutput from heterogeneous foundation models, which boosts the performance of\ncurrent MAD frameworks. Finally, we outline potential directions for advancing\nMAD, aiming to spark a broader conversation and inspire future work in this\narea.\n","authors":["Hangfan Zhang","Zhiyao Cui","Xinrun Wang","Qiaosheng Zhang","Zhen Wang","Dinghao Wu","Shuyue Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08788v1.pdf","comment":"This position paper takes a critical view of the status quo of MAD\n  research, and outline multiple potential directions to improve MAD"},{"id":"http://arxiv.org/abs/2502.08777v1","updated":"2025-02-12T20:39:01Z","published":"2025-02-12T20:39:01Z","title":"Zero-Shot Belief: A Hard Problem for LLMs","summary":"  We present two LLM-based approaches to zero-shot source-and-target belief\nprediction on FactBank: a unified system that identifies events, sources, and\nbelief labels in a single pass, and a hybrid approach that uses a fine-tuned\nDeBERTa tagger for event detection. We show that multiple open-sourced,\nclosed-source, and reasoning-based LLMs struggle with the task. Using the\nhybrid approach, we achieve new state-of-the-art results on FactBank and offer\na detailed error analysis. Our approach is then tested on the Italian belief\ncorpus ModaFact.\n","authors":["John Murzaku","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2502.08777v1.pdf","comment":"Submitted to ACL 2025"},{"id":"http://arxiv.org/abs/2502.08773v1","updated":"2025-02-12T20:30:28Z","published":"2025-02-12T20:30:28Z","title":"Universal Model Routing for Efficient LLM Inference","summary":"  Large language models' significant advances in capabilities are accompanied\nby significant increases in inference costs. Model routing is a simple\ntechnique for reducing inference cost, wherein one maintains a pool of\ncandidate LLMs, and learns to route each prompt to the smallest feasible LLM.\nExisting works focus on learning a router for a fixed pool of LLMs. In this\npaper, we consider the problem of dynamic routing, where new, previously\nunobserved LLMs are available at test time. We propose a new approach to this\nproblem that relies on representing each LLM as a feature vector, derived based\non predictions on a set of representative prompts. Based on this, we detail two\neffective strategies, relying on cluster-based routing and a learned cluster\nmap respectively. We prove that these strategies are estimates of a\ntheoretically optimal routing rule, and provide an excess risk bound to\nquantify their errors. Experiments on a range of public benchmarks show the\neffectiveness of the proposed strategies in routing amongst more than 30 unseen\nLLMs.\n","authors":["Wittawat Jitkrittum","Harikrishna Narasimhan","Ankit Singh Rawat","Jeevesh Juneja","Zifeng Wang","Chen-Yu Lee","Pradeep Shenoy","Rina Panigrahy","Aditya Krishna Menon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.08773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02362v3","updated":"2025-02-12T20:21:55Z","published":"2025-02-04T14:44:58Z","title":"Premise-Augmented Reasoning Chains Improve Error Identification in Math\n  reasoning with LLMs","summary":"  Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations.\n","authors":["Sagnik Mukherjee","Abhinav Chinta","Takyoung Kim","Tarun Anoop Sharma","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2502.02362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08767v1","updated":"2025-02-12T20:13:56Z","published":"2025-02-12T20:13:56Z","title":"SelfElicit: Your Language Model Secretly Knows Where is the Relevant\n  Evidence","summary":"  Providing Language Models (LMs) with relevant evidence in the context (either\nvia retrieval or user-provided) can significantly improve their ability to\nprovide factually correct grounded responses. However, recent studies have\nfound that LMs often struggle to fully comprehend and utilize key evidence from\nthe context, especially when it contains noise and irrelevant information - an\nissue common in real-world scenarios. To address this, we propose SelfElicit,\nan inference-time approach that helps LMs focus on key contextual evidence\nthrough self-guided explicit highlighting. By leveraging the inherent\nevidence-finding capabilities of LMs using the attention scores of deeper\nlayers, our method automatically identifies and emphasizes key evidence within\nthe input context, facilitating more accurate and factually grounded responses\nwithout additional training or iterative prompting. We demonstrate that\nSelfElicit brings consistent and significant improvement on multiple\nevidence-based QA tasks for various LM families while maintaining computational\nefficiency. Our code and documentation are available at\nhttps://github.com/ZhiningLiu1998/SelfElicit.\n","authors":["Zhining Liu","Rana Ali Amjad","Ravinarayana Adkathimar","Tianxin Wei","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2502.08767v1.pdf","comment":"16 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2502.08745v1","updated":"2025-02-12T19:35:28Z","published":"2025-02-12T19:35:28Z","title":"IHEval: Evaluating Language Models on Following the Instruction\n  Hierarchy","summary":"  The instruction hierarchy, which establishes a priority order from system\nmessages to user messages, conversation history, and tool outputs, is essential\nfor ensuring consistent and safe behavior in language models (LMs). Despite its\nimportance, this topic receives limited attention, and there is a lack of\ncomprehensive benchmarks for evaluating models' ability to follow the\ninstruction hierarchy. We bridge this gap by introducing IHEval, a novel\nbenchmark comprising 3,538 examples across nine tasks, covering cases where\ninstructions in different priorities either align or conflict. Our evaluation\nof popular LMs highlights their struggle to recognize instruction priorities.\nAll evaluated models experience a sharp performance decline when facing\nconflicting instructions, compared to their original instruction-following\nperformance. Moreover, the most competitive open-source model only achieves 48%\naccuracy in resolving such conflicts. Our results underscore the need for\ntargeted optimization in the future development of LMs.\n","authors":["Zhihan Zhang","Shiyang Li","Zixuan Zhang","Xin Liu","Haoming Jiang","Xianfeng Tang","Yifan Gao","Zheng Li","Haodong Wang","Zhaoxuan Tan","Yichuan Li","Qingyu Yin","Bing Yin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.08745v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.08744v1","updated":"2025-02-12T19:35:15Z","published":"2025-02-12T19:35:15Z","title":"Are Expressions for Music Emotions the Same Across Cultures?","summary":"  Music evokes profound emotions, yet the universality of emotional descriptors\nacross languages remains debated. A key challenge in cross-cultural research on\nmusic emotion is biased stimulus selection and manual curation of taxonomies,\npredominantly relying on Western music and languages. To address this, we\npropose a balanced experimental design with nine online experiments in Brazil,\nthe US, and South Korea, involving N=672 participants. First, we sample a\nbalanced set of popular music from these countries. Using an open-ended tagging\npipeline, we then gather emotion terms to create culture-specific taxonomies.\nFinally, using these bottom-up taxonomies, participants rate emotions of each\nsong. This allows us to map emotional similarities within and across cultures.\nResults show consistency in high arousal, high valence emotions but greater\nvariability in others. Notably, machine translations were often inadequate to\ncapture music-specific meanings. These findings together highlight the need for\na domain-sensitive, open-ended, bottom-up emotion elicitation approach to\nreduce cultural biases in emotion research.\n","authors":["Elif Celen","Pol van Rijn","Harin Lee","Nori Jacoby"],"pdf_url":"https://arxiv.org/pdf/2502.08744v1.pdf","comment":"Submitted to CogSci"},{"id":"http://arxiv.org/abs/2408.11081v2","updated":"2025-02-12T19:34:51Z","published":"2024-08-20T11:19:06Z","title":"What can Large Language Models Capture about Code Functional\n  Equivalence?","summary":"  Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress\nin learning rich representations of the structure and syntax of code,\nsuccessfully using it to generate or classify code fragments. At the same time,\nunderstanding if they are able to do so because they capture code semantics,\nand how well, is still an open question. In this paper, we tackle this problem\nby introducing SeqCoBench, a benchmark for systematically assessing how\nCode-LLMs can capture code functional equivalence. SeqCoBench contains over 20\ncode transformations that either preserve or alter the semantics of Python\nprograms. We conduct extensive evaluations in different settings, including\nzero-shot and parameter-efficient finetuning methods on state-of-the-art\n(Code)-LLMs to see if they can discern semantically equivalent or different\npairs of programs in SeqCoBench. We find that the performance gap between these\nLLMs and classical match-based retrieval scores is minimal, with both\napproaches showing a concerning lack of depth in understanding code semantics.\n","authors":["Nickil Maveli","Antonio Vergari","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2408.11081v2.pdf","comment":"Accepted to Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2305.03092v3","updated":"2025-02-12T19:30:14Z","published":"2023-05-04T18:15:45Z","title":"Curating corpora with classifiers: A case study of clean energy\n  sentiment online","summary":"  Well curated, large-scale corpora of social media posts containing broad\npublic opinion offer an alternative data source to complement traditional\nsurveys. While surveys are effective at collecting representative samples and\nare capable of achieving high accuracy, they can be both expensive to run and\nlag public opinion by days or weeks. Both of these drawbacks could be overcome\nwith a real-time, high volume data stream and fast analysis pipeline. A central\nchallenge in orchestrating such a data pipeline is devising an effective method\nfor rapidly selecting the best corpus of relevant documents for analysis.\nQuerying with keywords alone often includes irrelevant documents that are not\neasily disambiguated with bag-of-words natural language processing methods.\nHere, we explore methods of corpus curation to filter irrelevant tweets using\npre-trained transformer-based models, fine-tuned for our binary classification\ntask on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95.\nThe low cost and high performance of fine-tuning such a model suggests that our\napproach could be of broad benefit as a pre-processing step for social media\ndatasets with uncertain corpus boundaries.\n","authors":["Michael V. Arnold","Peter Sheridan Dodds","Christopher M. Danforth"],"pdf_url":"https://arxiv.org/pdf/2305.03092v3.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08638v1","updated":"2025-02-12T18:54:37Z","published":"2025-02-12T18:54:37Z","title":"Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples","summary":"  The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations.\n","authors":["Andrianos Michail","Simon Clematide","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2502.08638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04689v2","updated":"2025-02-12T18:36:24Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v2.pdf","comment":"20 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04354v3","updated":"2025-02-12T18:24:34Z","published":"2023-11-07T21:27:17Z","title":"Uncovering Intermediate Variables in Transformers using Circuit Probing","summary":"  Neural network models have achieved high performance on a wide variety of\ncomplex tasks, but the algorithms that they implement are notoriously difficult\nto interpret. It is often necessary to hypothesize intermediate variables\ninvolved in a network's computation in order to understand these algorithms.\nFor example, does a language model depend on particular syntactic properties\nwhen generating a sentence? Yet, existing analysis tools make it difficult to\ntest hypotheses of this type. We propose a new analysis technique - circuit\nprobing - that automatically uncovers low-level circuits that compute\nhypothesized intermediate variables. This enables causal analysis through\ntargeted ablation at the level of model parameters. We apply this method to\nmodels trained on simple arithmetic tasks, demonstrating its effectiveness at\n(1) deciphering the algorithms that models have learned, (2) revealing modular\nstructure within a model, and (3) tracking the development of circuits over\ntraining. Across these three experiments we demonstrate that circuit probing\ncombines and extends the capabilities of existing methods, providing one\nunified approach for a variety of analyses. Finally, we demonstrate circuit\nprobing on a real-world use case: uncovering circuits that are responsible for\nsubject-verb agreement and reflexive anaphora in GPT2-Small and Medium.\n","authors":["Michael A. Lepori","Thomas Serre","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2311.04354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15537v3","updated":"2025-02-12T17:59:14Z","published":"2024-02-23T04:52:08Z","title":"Evaluating the Performance of ChatGPT for Spam Email Detection","summary":"  Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.\n","authors":["Shijing Si","Yuwei Wu","Le Tang","Yugui Zhang","Jedrek Wosik","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2402.15537v3.pdf","comment":"12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)"},{"id":"http://arxiv.org/abs/2502.08606v1","updated":"2025-02-12T17:52:47Z","published":"2025-02-12T17:52:47Z","title":"Distillation Scaling Laws","summary":"  We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.\n","authors":["Dan Busbridge","Amitis Shidani","Floris Weers","Jason Ramapuram","Etai Littwin","Russ Webb"],"pdf_url":"https://arxiv.org/pdf/2502.08606v1.pdf","comment":"67 pages, 54 figures, 13 tables"},{"id":"http://arxiv.org/abs/2502.08599v1","updated":"2025-02-12T17:38:27Z","published":"2025-02-12T17:38:27Z","title":"SPeCtrum: A Grounded Framework for Multidimensional Identity\n  Representation in LLM-Based Agent","summary":"  Existing methods for simulating individual identities often oversimplify\nhuman complexity, which may lead to incomplete or flattened representations. To\naddress this, we introduce SPeCtrum, a grounded framework for constructing\nauthentic LLM agent personas by incorporating an individual's multidimensional\nself-concept. SPeCtrum integrates three core components: Social Identity (S),\nPersonal Identity (P), and Personal Life Context (C), each contributing\ndistinct yet interconnected aspects of identity. To evaluate SPeCtrum's\neffectiveness in identity representation, we conducted automated and human\nevaluations. Automated evaluations using popular drama characters showed that\nPersonal Life Context (C)-derived from short essays on preferences and daily\nroutines-modeled characters' identities more effectively than Social Identity\n(S) and Personal Identity (P) alone and performed comparably to the full SPC\ncombination. In contrast, human evaluations involving real-world individuals\nfound that the full SPC combination provided a more comprehensive self-concept\nrepresentation than C alone. Our findings suggest that while C alone may\nsuffice for basic identity simulation, integrating S, P, and C enhances the\nauthenticity and accuracy of real-world identity representation. Overall,\nSPeCtrum offers a structured approach for simulating individuals in LLM agents,\nenabling more personalized human-AI interactions and improving the realism of\nsimulation-based behavioral studies.\n","authors":["Keyeun Lee","Seo Hyeong Kim","Seolhee Lee","Jinsu Eun","Yena Ko","Hayeon Jeon","Esther Hehsun Kim","Seonghye Cho","Soeun Yang","Eun-mee Kim","Hajin Lim"],"pdf_url":"https://arxiv.org/pdf/2502.08599v1.pdf","comment":"21 pages, 8 figures, 5 tables, Accepted in NAACL2025 Main"},{"id":"http://arxiv.org/abs/2407.07313v3","updated":"2025-02-12T17:20:56Z","published":"2024-07-10T02:20:19Z","title":"ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models","summary":"  The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics -- Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) -- suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.\n","authors":["Benjamin G. Ascoli","Yasoda Sai Ram Kandikonda","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2407.07313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08561v1","updated":"2025-02-12T16:49:52Z","published":"2025-02-12T16:49:52Z","title":"Quality-Aware Decoding: Unifying Quality Estimation and Decoding","summary":"  An emerging research direction in NMT involves the use of Quality Estimation\n(QE) models, which have demonstrated high correlations with human judgment and\ncan enhance translations through Quality-Aware Decoding. Although several\napproaches have been proposed based on sampling multiple candidate\ntranslations, none have integrated these models directly into the decoding\nprocess. In this paper, we address this by proposing a novel token-level QE\nmodel capable of reliably scoring partial translations. We build a\nuni-directional QE model for this, as decoder models are inherently trained and\nefficient on partial sequences. We then present a decoding strategy that\nintegrates the QE model for Quality-Aware decoding and demonstrate that the\ntranslation quality improves when compared to the N-best list re-ranking with\nstate-of-the-art QE models (upto $1.39$ XCOMET-XXL $\\uparrow$). Finally, we\nshow that our approach provides significant benefits in document translation\ntasks, where the quality of N-best lists is typically suboptimal.\n","authors":["Sai Koneru","Matthias Huck","Miriam Exel","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2502.08561v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2501.04686v3","updated":"2025-02-12T16:49:50Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Yiyao Yu","Xinzhe Ni","Zicheng Lin","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v3.pdf","comment":"Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08550v1","updated":"2025-02-12T16:31:21Z","published":"2025-02-12T16:31:21Z","title":"LLMs can implicitly learn from mistakes in-context","summary":"  Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning.\n","authors":["Lisa Alazraki","Maximilian Mozes","Jon Ander Campos","Yi Chern Tan","Marek Rei","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2502.08550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v2","updated":"2025-02-12T16:25:44Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08524v1","updated":"2025-02-12T16:00:11Z","published":"2025-02-12T16:00:11Z","title":"LLM Pretraining with Continuous Concepts","summary":"  Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.\n","authors":["Jihoon Tack","Jack Lanchantin","Jane Yu","Andrew Cohen","Ilia Kulikov","Janice Lan","Shibo Hao","Yuandong Tian","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2502.08524v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.08845v1","updated":"2025-02-12T23:32:09Z","published":"2025-02-12T23:32:09Z","title":"Optimal Dataset Size for Recommender Systems: Evaluating Algorithms'\n  Performance via Downsampling","summary":"  This thesis investigates dataset downsampling as a strategy to optimize\nenergy efficiency in recommender systems while maintaining competitive\nperformance. With increasing dataset sizes posing computational and\nenvironmental challenges, this study explores the trade-offs between energy\nefficiency and recommendation quality in Green Recommender Systems, which aim\nto reduce environmental impact. By applying two downsampling approaches to\nseven datasets, 12 algorithms, and two levels of core pruning, the research\ndemonstrates significant reductions in runtime and carbon emissions. For\nexample, a 30% downsampling portion can reduce runtime by 52% compared to the\nfull dataset, leading to a carbon emission reduction of up to 51.02 KgCO2e\nduring the training of a single algorithm on a single dataset. The analysis\nreveals that algorithm performance under different downsampling portions\ndepends on factors like dataset characteristics, algorithm complexity, and the\nspecific downsampling configuration (scenario dependent). Some algorithms,\nwhich showed lower nDCG@10 scores compared to higher-performing ones, exhibited\nlower sensitivity to the amount of training data, offering greater potential\nfor efficiency in lower downsampling portions. On average, these algorithms\nretained 81% of full-size performance using only 50% of the training set. In\ncertain downsampling configurations, where more users were progressively\nincluded while keeping the test set size fixed, they even showed higher nDCG@10\nscores than when using the full dataset. These findings highlight the\nfeasibility of balancing sustainability and effectiveness, providing insights\nfor designing energy-efficient recommender systems and promoting sustainable AI\npractices.\n","authors":["Ardalan Arabzadeh","Joeran Beel","Tobias Vente"],"pdf_url":"https://arxiv.org/pdf/2502.08845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08826v1","updated":"2025-02-12T22:33:41Z","published":"2025-02-12T22:33:41Z","title":"Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.\n","authors":["Mohammad Mahdi Abootorabi","Amirhosein Zobeiri","Mahdi Dehghani","Mohammadali Mohammadkhani","Bardia Mohammadi","Omid Ghahroodi","Mahdieh Soleymani Baghshah","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2502.08826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18870v2","updated":"2025-02-12T21:37:54Z","published":"2024-10-24T15:57:17Z","title":"End-to-end Training for Recommendation with Language-based User Profiles","summary":"  There is a growing interest in natural language-based user profiles for\nrecommender systems, which aims to enhance transparency and scrutability\ncompared with embedding-based methods. Existing studies primarily generate\nthese profiles using zero-shot inference from large language models (LLMs), but\ntheir quality remains insufficient, leading to suboptimal recommendation\nperformance. In this paper, we introduce LangPTune, the first end-to-end\ntraining framework to optimize LLM-generated user profiles. Our method\nsignificantly outperforms zero-shot approaches by explicitly training the LLM\nfor the recommendation objective. Through extensive evaluations across diverse\ntraining configurations and benchmarks, we demonstrate that LangPTune not only\nsurpasses zero-shot baselines but can also matches the performance of\nstate-of-the-art embedding-based methods. Finally, we investigate whether the\ntraining procedure preserves the interpretability of these profiles compared to\nzero-shot inference through both GPT-4 simulations and crowdworker user\nstudies. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune.\n","authors":["Zhaolin Gao","Joyce Zhou","Yijia Dai","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2410.18870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20163v3","updated":"2025-02-12T16:49:56Z","published":"2024-12-28T14:27:45Z","title":"Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems","summary":"  The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.\n","authors":["Minhye Jeon","Seokho Ahn","Young-Duk Seo"],"pdf_url":"https://arxiv.org/pdf/2412.20163v3.pdf","comment":"Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025"},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08496v1","updated":"2025-02-12T15:31:16Z","published":"2025-02-12T15:31:16Z","title":"Fine-Tuning Topics through Weighting Aspect Keywords","summary":"  Topic modeling often requires examining topics from multiple perspectives to\nuncover hidden patterns, especially in less explored areas. This paper presents\nan approach to address this need, utilizing weighted keywords from various\naspects derived from a domain knowledge. The research method starts with\nstandard topic modeling. Then, it adds a process consisting of four key steps.\nFirst, it defines keywords for each aspect. Second, it gives weights to these\nkeywords based on their relevance. Third, it calculates relevance scores for\naspect-weighted keywords and topic keywords to create aspect-topic models.\nFourth, it uses these scores to tune relevant new documents. Finally, the\ngenerated topic models are interpreted and validated. The findings show that\ntop-scoring documents are more likely to be about the same aspect of a topic.\nThis highlights the model's effectiveness in finding the related documents to\nthe aspects.\n","authors":["Ali Nazari","Michael Weiss"],"pdf_url":"https://arxiv.org/pdf/2502.08496v1.pdf","comment":"17 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2502.08346v1","updated":"2025-02-12T12:13:51Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08326v1","updated":"2025-02-12T11:48:15Z","published":"2025-02-12T11:48:15Z","title":"Model-Free Counterfactual Subset Selection at Scale","summary":"  Ensuring transparency in AI decision-making requires interpretable\nexplanations, particularly at the instance level. Counterfactual explanations\nare a powerful tool for this purpose, but existing techniques frequently depend\non synthetic examples, introducing biases from unrealistic assumptions, flawed\nmodels, or skewed data. Many methods also assume full dataset availability, an\nimpractical constraint in real-time environments where data flows continuously.\nIn contrast, streaming explanations offer adaptive, real-time insights without\nrequiring persistent storage of the entire dataset. This work introduces a\nscalable, model-free approach to selecting diverse and relevant counterfactual\nexamples directly from observed data. Our algorithm operates efficiently in\nstreaming settings, maintaining $O(\\log k)$ update complexity per item while\nensuring high-quality counterfactual selection. Empirical evaluations on both\nreal-world and synthetic datasets demonstrate superior performance over\nbaseline methods, with robust behavior even under adversarial conditions.\n","authors":["Minh Hieu Nguyen","Viet Hung Doan","Anh Tuan Nguyen","Jun Jo","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.08326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08309v1","updated":"2025-02-12T11:23:46Z","published":"2025-02-12T11:23:46Z","title":"Unlocking Scaling Law in Industrial Recommendation Systems with a\n  Three-step Paradigm based Large User Model","summary":"  Recent advancements in autoregressive Large Language Models (LLMs) have\nachieved significant milestones, largely attributed to their scalability, often\nreferred to as the \"scaling law\". Inspired by these achievements, there has\nbeen a growing interest in adapting LLMs for Recommendation Systems (RecSys) by\nreformulating RecSys tasks into generative problems. However, these End-to-End\nGenerative Recommendation (E2E-GR) methods tend to prioritize idealized goals,\noften at the expense of the practical advantages offered by traditional Deep\nLearning based Recommendation Models (DLRMs) in terms of in features,\narchitecture, and practices. This disparity between idealized goals and\npractical needs introduces several challenges and limitations, locking the\nscaling law in industrial RecSys. In this paper, we introduce a large user\nmodel (LUM) that addresses these limitations through a three-step paradigm,\ndesigned to meet the stringent requirements of industrial settings while\nunlocking the potential for scalable recommendations. Our extensive\nexperimental evaluations demonstrate that LUM outperforms both state-of-the-art\nDLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with\nperformance improvements observed as the model scales up to 7 billion\nparameters. Additionally, we have successfully deployed LUM in an industrial\napplication, where it achieved significant gains in an A/B test, further\nvalidating its effectiveness and practicality.\n","authors":["Bencheng Yan","Shilei Liu","Zhiyuan Zeng","Zihao Wang","Yizhen Zhang","Yujin Yuan","Langming Liu","Jiaqi Liu","Di Wang","Wenbo Su","Wang Pengjie","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.08309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02231v3","updated":"2025-02-12T10:53:30Z","published":"2024-03-04T17:21:19Z","title":"CODE-ACCORD: A Corpus of building regulatory data for rule generation\n  towards automatic compliance checking","summary":"  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and\nConstruction (AEC) sector necessitates automating the interpretation of\nbuilding regulations to achieve its full potential. Converting textual rules\ninto machine-readable formats is challenging due to the complexities of natural\nlanguage and the scarcity of resources for advanced Machine Learning (ML).\nAddressing these challenges, we introduce CODE-ACCORD, a dataset of 862\nsentences from the building regulations of England and Finland. Only the\nself-contained sentences, which express complete rules without needing\nadditional context, were considered as they are essential for ACC. Each\nsentence was manually annotated with entities and relations by a team of 12\nannotators to facilitate machine-readable rule generation, followed by careful\ncuration to ensure accuracy. The final dataset comprises 4,297 entities and\n4,329 relations across various categories, serving as a robust ground truth.\nCODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,\nincluding text classification, entity recognition, and relation extraction. It\nenables applying recent trends, such as deep neural networks and large language\nmodels, to ACC.\n","authors":["Hansi Hettiarachchi","Amna Dridi","Mohamed Medhat Gaber","Pouyan Parsafard","Nicoleta Bocaneala","Katja Breitenfelder","Gonçal Costa","Maria Hedblom","Mihaela Juganaru-Mathieu","Thamer Mecharnia","Sumee Park","He Tan","Abdel-Rahman H. Tawil","Edlira Vakaj"],"pdf_url":"https://arxiv.org/pdf/2403.02231v3.pdf","comment":"This is a preprint of an article submitted to the Scientific Data\n  Journal"},{"id":"http://arxiv.org/abs/2502.08277v1","updated":"2025-02-12T10:31:45Z","published":"2025-02-12T10:31:45Z","title":"ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion\n  Rate Modeling","summary":"  Post-click conversion rate (CVR) estimation is a vital task in many\nrecommender systems of revenue businesses, e.g., e-commerce and advertising. In\na perspective of sample, a typical CVR positive sample usually goes through a\nfunnel of exposure to click to conversion. For lack of post-event labels for\nun-clicked samples, CVR learning task commonly only utilizes clicked samples,\nrather than all exposed samples as for click-through rate (CTR) learning task.\nHowever, during online inference, CVR and CTR are estimated on the same assumed\nexposure space, which leads to a inconsistency of sample space between training\nand inference, i.e., sample selection bias (SSB). To alleviate SSB, previous\nwisdom proposes to design novel auxiliary tasks to enable the CVR learning on\nun-click training samples, such as CTCVR and counterfactual CVR, etc. Although\nalleviating SSB to some extent, none of them pay attention to the\ndiscrimination between ambiguous negative samples (un-clicked) and factual\nnegative samples (clicked but un-converted) during modelling, which makes CVR\nmodel lacks robustness. To full this gap, we propose a novel ChorusCVR model to\nrealize debiased CVR learning in entire-space.\n","authors":["Wei Cheng","Yucheng Lu","Boyang Xia","Jiangxia Cao","Kuan Xu","Mingxing Wen","Wei Jiang","Jiaming Zhang","Zhaojie Liu","Kun Gai","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08277v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.08271v1","updated":"2025-02-12T10:24:22Z","published":"2025-02-12T10:24:22Z","title":"MoLoRec: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation","summary":"  Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm enhances domain-specific\nrecommendation tasks with parameter-efficient fine-tuning techniques, in order\nto improve models under the warm recommendation scenarios. While most previous\nworks treat these two paradigms separately, we argue that they have\ncomplementary advantages, and combining them together would be helpful.\n  To that end, in this paper, we propose a generalizable and efficient\nLLM-based recommendation framework MoLoRec. Our approach starts by\nparameter-efficient fine-tuning a domain-general module with general\nrecommendation instruction data, to align LLM with recommendation knowledge.\nThen, given users' behavior of a specific domain, we construct a\ndomain-specific instruction dataset and apply efficient fine-tuning to the\npre-trained LLM. After that, we provide approaches to integrate the above\ndomain-general part and domain-specific part with parameters mixture. Please\nnote that, MoLoRec is efficient with plug and play, as the domain-general\nmodule is trained only once, and any domain-specific plug-in can be efficiently\nmerged with only domain-specific fine-tuning. Extensive experiments on multiple\ndatasets under both warm and cold-start recommendation scenarios validate the\neffectiveness and generality of the proposed MoLoRec.\n","authors":["Min Hou","Chenxi Bai","Le Wu","Hao Liu","Kun Zhang","Kai Zhang","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08205v1","updated":"2025-02-12T08:35:10Z","published":"2025-02-12T08:35:10Z","title":"Wisdom of the Crowds in Forecasting: Forecast Summarization for\n  Supporting Future Event Prediction","summary":"  Future Event Prediction (FEP) is an essential activity whose demand and\napplication range across multiple domains. While traditional methods like\nsimulations, predictive and time-series forecasting have demonstrated promising\noutcomes, their application in forecasting complex events is not entirely\nreliable due to the inability of numerical data to accurately capture the\nsemantic information related to events. One forecasting way is to gather and\naggregate collective opinions on the future to make predictions as cumulative\nperspectives carry the potential to help estimating the likelihood of upcoming\nevents. In this work, we organize the existing research and frameworks that aim\nto support future event prediction based on crowd wisdom through aggregating\nindividual forecasts. We discuss the challenges involved, available datasets,\nas well as the scope of improvement and future research directions for this\ntask. We also introduce a novel data model to represent individual forecast\nstatements.\n","authors":["Anisha Saha","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03307v3","updated":"2025-02-12T08:16:44Z","published":"2025-02-05T16:08:05Z","title":"Intent Alignment between Interaction and Language Spaces for\n  Recommendation","summary":"  Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.\n","authors":["Yu Wang","Lei Sang","Yi Zhang","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.03307v3.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.05806v2","updated":"2025-02-12T07:38:08Z","published":"2024-10-08T08:39:15Z","title":"A Parameter Update Balancing Algorithm for Multi-task Ranking Models in\n  Recommendation Systems","summary":"  Multi-task ranking models have become essential for modern real-world\nrecommendation systems. While most recommendation researches focus on designing\nsophisticated models for specific scenarios, achieving performance improvement\nfor multi-task ranking models across various scenarios still remains a\nsignificant challenge. Training all tasks naively can result in inconsistent\nlearning, highlighting the need for the development of multi-task optimization\n(MTO) methods to tackle this challenge. Conventional methods assume that the\noptimal joint gradient on shared parameters leads to optimal parameter updates.\nHowever, the actual update on model parameters may deviates significantly from\ngradients when using momentum based optimizers such as Adam, and we design and\nexecute statistical experiments to support the observation. In this paper, we\npropose a novel Parameter Update Balancing algorithm for multi-task\noptimization, denoted as PUB. In contrast to traditional MTO method which are\nbased on gradient level tasks fusion or loss level tasks fusion, PUB is the\nfirst work to optimize multiple tasks through parameter update balancing.\nComprehensive experiments on benchmark multi-task ranking datasets demonstrate\nthat PUB consistently improves several multi-task backbones and achieves\nstate-of-the-art performance. Additionally, experiments on benchmark computer\nvision datasets show the great potential of PUB in various multi-task learning\nscenarios. Furthermore, we deployed our method for an industrial evaluation on\nthe real-world commercial platform, HUAWEI AppGallery, where PUB significantly\nenhances the online multi-task ranking model, efficiently managing the primary\ntraffic of a crucial channel.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2410.05806v2.pdf","comment":"Accepted by ICDM'24"},{"id":"http://arxiv.org/abs/2502.08161v1","updated":"2025-02-12T07:05:59Z","published":"2025-02-12T07:05:59Z","title":"MixDec Sampling: A Soft Link-based Sampling Method of Graph Neural\n  Network for Recommendation","summary":"  Graph neural networks have been widely used in recent recommender systems,\nwhere negative sampling plays an important role. Existing negative sampling\nmethods restrict the relationship between nodes as either hard positive pairs\nor hard negative pairs. This leads to the loss of structural information, and\nlacks the mechanism to generate positive pairs for nodes with few neighbors. To\novercome limitations, we propose a novel soft link-based sampling method,\nnamely MixDec Sampling, which consists of Mixup Sampling module and Decay\nSampling module. The Mixup Sampling augments node features by synthesizing new\nnodes and soft links, which provides sufficient number of samples for nodes\nwith few neighbors. The Decay Sampling strengthens the digestion of graph\nstructure information by generating soft links for node embedding learning. To\nthe best of our knowledge, we are the first to model sampling relationships\nbetween nodes by soft links in GNN-based recommender systems. Extensive\nexperiments demonstrate that the proposed MixDec Sampling can significantly and\nconsistently improve the recommendation performance of several representative\nGNN-based models on various recommendation benchmarks.\n","authors":["Xiangjin Xie","Yuxin Chen","Ruipeng Wang","Kai Ouyang","Zihan Zhang","Hai-Tao Zheng","Buyue Qian","Hansen Zheng","Bo Hu","Chengxiang Zhuo","Zang Li"],"pdf_url":"https://arxiv.org/pdf/2502.08161v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.00784v2","updated":"2025-02-12T06:34:11Z","published":"2024-10-17T06:44:18Z","title":"FIRE: Fact-checking with Iterative Retrieval and Verification","summary":"  Fact-checking long-form text is challenging, and it is therefore common\npractice to break it down into multiple atomic claims. The typical approach to\nfact-checking these atomic claims involves retrieving a fixed number of pieces\nof evidence, followed by a verification step. However, this method is usually\nnot cost-effective, as it underutilizes the verification model's internal\nknowledge of the claim and fails to replicate the iterative reasoning process\nin human search strategies. To address these limitations, we propose FIRE, a\nnovel agent-based framework that integrates evidence retrieval and claim\nverification in an iterative manner. Specifically, FIRE employs a unified\nmechanism to decide whether to provide a final answer or generate a subsequent\nsearch query, based on its confidence in the current judgment. We compare FIRE\nwith other strong fact-checking frameworks and find that it achieves slightly\nbetter performance while reducing large language model (LLM) costs by an\naverage of 7.6 times and search costs by 16.5 times. These results indicate\nthat FIRE holds promise for application in large-scale fact-checking\noperations. Our code is available at https://github.com/mbzuai-nlp/fire.git.\n","authors":["Zhuohan Xie","Rui Xing","Yuxia Wang","Jiahui Geng","Hasan Iqbal","Dhruv Sahnan","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2411.00784v2.pdf","comment":"4 figures, 8 tables, accepted to Findings of NAACL"},{"id":"http://arxiv.org/abs/2502.08132v1","updated":"2025-02-12T05:28:08Z","published":"2025-02-12T05:28:08Z","title":"SS4Rec: Continuous-Time Sequential Recommendation with State Space\n  Models","summary":"  Sequential recommendation is a key area in the field of recommendation\nsystems aiming to model user interest based on historical interaction sequences\nwith irregular intervals. While previous recurrent neural network-based and\nattention-based approaches have achieved significant results, they have\nlimitations in capturing system continuity due to the discrete characteristics.\nIn the context of continuous-time modeling, state space model (SSM) offers a\npotential solution, as it can effectively capture the dynamic evolution of user\ninterest over time. However, existing SSM-based approaches ignore the impact of\nirregular time intervals within historical user interactions, making it\ndifficult to model complexed user-item transitions in sequences. To address\nthis issue, we propose a hybrid SSM-based model called SS4Rec for\ncontinuous-time sequential recommendation. SS4Rec integrates a time-aware SSM\nto handle irregular time intervals and a relation-aware SSM to model contextual\ndependencies, enabling it to infer user interest from both temporal and\nsequential perspectives. In the training process, the time-aware SSM and the\nrelation-aware SSM are discretized by variable stepsizes according to user\ninteraction time intervals and input data, respectively. This helps capture the\ncontinuous dependency from irregular time intervals and provides time-specific\npersonalized recommendations. Experimental studies on five benchmark datasets\ndemonstrate the superiority and effectiveness of SS4Rec.\n","authors":["Wei Xiao","Huiying Wang","Qifeng Zhou","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15005v3","updated":"2025-02-12T04:50:07Z","published":"2024-12-19T16:20:42Z","title":"DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start\n  Cross-Domain Recommendation","summary":"  Recommender systems are widely used in various real-world applications, but\nthey often encounter the persistent challenge of the user cold-start problem.\nCross-domain recommendation (CDR), which leverages user interactions from one\ndomain to improve prediction performance in another, has emerged as a promising\nsolution. However, users with similar preferences in the source domain may\nexhibit different interests in the target domain. Therefore, directly\ntransferring embeddings may introduce irrelevant source-domain collaborative\ninformation. In this paper, we propose a novel graph-based disentangled\ncontrastive learning framework to capture fine-grained user intent and filter\nout irrelevant collaborative information, thereby avoiding negative transfer.\nSpecifically, for each domain, we use a multi-channel graph encoder to capture\ndiverse user intents. We then construct the affinity graph in the embedding\nspace and perform multi-step random walks to capture high-order user similarity\nrelationships. Treating one domain as the target, we propose a disentangled\nintent-wise contrastive learning approach, guided by user similarity, to refine\nthe bridging of user intents across domains. Extensive experiments on four\nbenchmark CDR datasets demonstrate that DisCo consistently outperforms existing\nstate-of-the-art baselines, thereby validating the effectiveness of both DisCo\nand its components.\n","authors":["Hourun Li","Yifan Wang","Zhiping Xiao","Jia Yang","Changling Zhou","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2412.15005v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2501.17630v2","updated":"2025-02-12T04:28:04Z","published":"2025-01-29T13:08:17Z","title":"Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation","summary":"  Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025\n","authors":["Wonbin Kweon","Sanghwan Jang","SeongKu Kang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2501.17630v2.pdf","comment":"WWW 2025"},{"id":"http://arxiv.org/abs/2408.09380v4","updated":"2025-02-12T04:00:41Z","published":"2024-08-18T06:41:46Z","title":"ELASTIC: Efficient Linear Attention for Sequential Interest Compression","summary":"  State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available.\n","authors":["Jiaxin Deng","Shiyao Wang","Song Lu","Yinfeng Li","Xinchen Luo","Yuanjun Liu","Peixing Xu","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.09380v4.pdf","comment":"We hereby withdraw this paper from arXiv due to incomplete\n  experiments. Upon further review, we have determined that additional\n  experimental work is necessary to fully validate our findings and conclusions"},{"id":"http://arxiv.org/abs/2502.08071v1","updated":"2025-02-12T02:24:26Z","published":"2025-02-12T02:24:26Z","title":"Collaborative Filtering Meets Spectrum Shift: Connecting User-Item\n  Interaction with Graph-Structured Side Information","summary":"  Graph Neural Network (GNN) has demonstrated their superiority in\ncollaborative filtering, where the user-item (U-I) interaction bipartite graph\nserves as the fundamental data format. However, when graph-structured side\ninformation (e.g., multimodal similarity graphs or social networks) is\nintegrated into the U-I bipartite graph, existing graph collaborative filtering\nmethods fall short of achieving satisfactory performance. We quantitatively\nanalyze this problem from a spectral perspective. Recall that a bipartite graph\npossesses a full spectrum within the range of [-1, 1], with the highest\nfrequency exactly achievable at -1 and the lowest frequency at 1; however, we\nobserve as more side information is incorporated, the highest frequency of the\naugmented adjacency matrix progressively shifts rightward. This spectrum shift\nphenomenon has caused previous approaches built for the full spectrum [-1, 1]\nto assign mismatched importance to different frequencies. To this end, we\npropose Spectrum Shift Correction (dubbed SSC), incorporating shifting and\nscaling factors to enable spectral GNNs to adapt to the shifted spectrum.\nUnlike previous paradigms of leveraging side information, which necessitate\ntailored designs for diverse data types, SSC directly connects traditional\ngraph collaborative filtering with any graph-structured side information.\nExperiments on social and multimodal recommendation demonstrate the\neffectiveness of SSC, achieving relative improvements of up to 23% without\nincurring any additional computational overhead.\n","authors":["Yunhang He","Cong Xu","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07219v2","updated":"2025-02-12T01:58:22Z","published":"2025-02-11T03:25:42Z","title":"DOGR: Leveraging Document-Oriented Contrastive Learning in Generative\n  Retrieval","summary":"  Generative retrieval constitutes an innovative approach in information\nretrieval, leveraging generative language models (LM) to generate a ranked list\nof document identifiers (docid) for a given query. It simplifies the retrieval\npipeline by replacing the large external index with model parameters. However,\nexisting works merely learned the relationship between queries and document\nidentifiers, which is unable to directly represent the relevance between\nqueries and documents. To address the above problem, we propose a novel and\ngeneral generative retrieval framework, namely Leveraging Document-Oriented\nContrastive Learning in Generative Retrieval (DOGR), which leverages\ncontrastive learning to improve generative retrieval tasks. It adopts a\ntwo-stage learning strategy that captures the relationship between queries and\ndocuments comprehensively through direct interactions. Furthermore, negative\nsampling methods and corresponding contrastive learning objectives are\nimplemented to enhance the learning of semantic representations, thereby\npromoting a thorough comprehension of the relationship between queries and\ndocuments. Experimental results demonstrate that DOGR achieves state-of-the-art\nperformance compared to existing generative retrieval methods on two public\nbenchmark datasets. Further experiments have shown that our framework is\ngenerally effective for common identifier construction techniques.\n","authors":["Penghao Lu","Xin Dong","Yuansheng Zhou","Lei Cheng","Chuan Yuan","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2502.07219v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.21144v4","updated":"2025-02-12T19:20:49Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods. Our code is\npublicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v4.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC\n  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC"},{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19702v2","updated":"2025-02-12T16:47:30Z","published":"2024-10-25T17:19:55Z","title":"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n","authors":["Xiangyu Zeng","Kunchang Li","Chenting Wang","Xinhao Li","Tianxiang Jiang","Ziang Yan","Songze Li","Yansong Shi","Zhengrong Yue","Yi Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19702v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08513v1","updated":"2025-02-12T15:46:47Z","published":"2025-02-12T15:46:47Z","title":"\"You'll Be Alice Adventuring in Wonderland!\" Processes, Challenges, and\n  Opportunities of Creating Animated Virtual Reality Stories","summary":"  Animated virtual reality (VR) stories, combining the presence of VR and the\nartistry of computer animation, offer a compelling way to deliver messages and\nevoke emotions. Motivated by the growing demand for immersive narrative\nexperiences, more creators are creating animated VR stories. However, a\nholistic understanding of their creation processes and challenges involved in\ncrafting these stories is still limited. Based on semi-structured interviews\nwith 21 animated VR story creators, we identify ten common stages in their\nend-to-end creation processes, ranging from idea generation to evaluation,\nwhich form diverse workflows that are story-driven or visual-driven.\nAdditionally, we highlight nine unique issues that arise during the creation\nprocess, such as a lack of reference material for multi-element plots, the\nabsence of specific functionalities for story integration, and inadequate\nsupport for audience evaluation. We compare the creation of animated VR stories\nto general XR applications and distill several future research opportunities.\n","authors":["Lin-Ping Yuan","Feilin Han","Liwenhan Xie","Junjie Zhang","Jian Zhao","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2502.08513v1.pdf","comment":"Conditionally accepted to the ACM Conference on Human Factors in\n  Computing Systems (CHI'25)"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2407.14093v3","updated":"2025-02-12T08:49:34Z","published":"2024-07-19T07:57:48Z","title":"Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models","summary":"  Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.\n","authors":["Qiong Wu","Zhaoxi Ke","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.14093v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v2","updated":"2025-02-12T07:35:56Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v2","updated":"2025-02-12T04:00:14Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v2.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.08674v1","updated":"2025-02-12T03:32:28Z","published":"2025-02-12T03:32:28Z","title":"COutfitGAN: Learning to Synthesize Compatible Outfits Supervised by\n  Silhouette Masks and Fashion Styles","summary":"  How to recommend outfits has gained considerable attention in both academia\nand industry in recent years. Many studies have been carried out regarding\nfashion compatibility learning, to determine whether the fashion items in an\noutfit are compatible or not. These methods mainly focus on evaluating the\ncompatibility of existing outfits and rarely consider applying such knowledge\nto 'design' new fashion items. We propose the new task of generating\ncomplementary and compatible fashion items based on an arbitrary number of\ngiven fashion items. In particular, given some fashion items that can make up\nan outfit, the aim of this paper is to synthesize photo-realistic images of\nother, complementary, fashion items that are compatible with the given ones. To\nachieve this, we propose an outfit generation framework, referred to as\nCOutfitGAN, which includes a pyramid style extractor, an outfit generator, a\nUNet-based real/fake discriminator, and a collocation discriminator. To train\nand evaluate this framework, we collected a large-scale fashion outfit dataset\nwith over 200K outfits and 800K fashion items from the Internet. Extensive\nexperiments show that COutfitGAN outperforms other baselines in terms of\nsimilarity, authenticity, and compatibility measurements.\n","authors":["Dongliang Zhou","Haijun Zhang","Qun Li","Jianghong Ma","Xiaofei Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08674v1.pdf","comment":"This paper was accepted by IEEE TMM"}]},"2025-02-11T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.07971v1","updated":"2025-02-11T21:35:13Z","published":"2025-02-11T21:35:13Z","title":"ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval","summary":"  Document retrieval is a core component of question-answering systems, as it\nenables conditioning answer generation on new and large-scale corpora. While\neffective, the standard practice of encoding documents into high-dimensional\nembeddings for similarity search entails large memory and compute footprints,\nand also makes it hard to inspect the inner workings of the system. In this\npaper, we propose a tree-based method for organizing and representing reference\ndocuments at various granular levels, which offers the flexibility to balance\ncost and utility, and eases the inspection of the corpus content and retrieval\noperations. Our method, called ReTreever, jointly learns a routing function per\ninternal node of a binary tree such that query and reference documents are\nassigned to similar tree branches, hence directly optimizing for retrieval\nperformance. Our evaluations show that ReTreever generally preserves full\nrepresentation accuracy. Its hierarchical structure further provides strong\ncoarse representations and enhances transparency by indirectly learning\nmeaningful semantic groupings. Among hierarchical retrieval methods, ReTreever\nachieves the best retrieval accuracy at the lowest latency, proving that this\nfamily of techniques can be viable in practical applications.\n","authors":["Shubham Gupta","Zichao Li","Tianyi Chen","Cem Subakan","Siva Reddy","Perouz Taslakian","Valentina Zantedeschi"],"pdf_url":"https://arxiv.org/pdf/2502.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03993v2","updated":"2025-02-11T21:10:32Z","published":"2024-03-06T19:08:28Z","title":"Personalized Negative Reservoir for Incremental Learning in Recommender\n  Systems","summary":"  Recommender systems have become an integral part of online platforms. Every\nday the volume of training data is expanding and the number of user\ninteractions is constantly increasing. The exploration of larger and more\nexpressive models has become a necessary pursuit to improve user experience.\nHowever, this progression carries with it an increased computational burden. In\ncommercial settings, once a recommendation system model has been trained and\ndeployed it typically needs to be updated frequently as new client data arrive.\nCumulatively, the mounting volume of data is guaranteed to eventually make full\nbatch retraining of the model from scratch computationally infeasible. Naively\nfine-tuning solely on the new data runs into the well-documented problem of\ncatastrophic forgetting. Despite the fact that negative sampling is a crucial\npart of training with implicit feedback, no specialized technique exists that\nis tailored to the incremental learning framework. In this work, we propose a\npersonalized negative reservoir strategy, which is used to obtain negative\nsamples for the standard triplet loss of graph-based recommendation systems.\nOur technique balances alleviation of forgetting with plasticity by encouraging\nthe model to remember stable user preferences and selectively forget when user\ninterests change. We derive the mathematical formulation of a negative sampler\nto populate and update the reservoir. We integrate our design in three SOTA and\ncommonly used incremental recommendation models. We show that these concrete\nrealizations of our negative reservoir framework achieve state-of-the-art\nresults for standard benchmarks using multiple top-k evaluation metrics.\n","authors":["Antonios Valkanas","Yuening Wang","Yingxue Zhang","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2403.03993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05502v3","updated":"2025-02-11T18:17:53Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v3.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07683v1","updated":"2025-02-11T16:35:04Z","published":"2025-02-11T16:35:04Z","title":"exHarmony: Authorship and Citations for Benchmarking the Reviewer\n  Assignment Problem","summary":"  The peer review process is crucial for ensuring the quality and reliability\nof scholarly work, yet assigning suitable reviewers remains a significant\nchallenge. Traditional manual methods are labor-intensive and often\nineffective, leading to nonconstructive or biased reviews. This paper\nintroduces the exHarmony (eHarmony but for connecting experts to manuscripts)\nbenchmark, designed to address these challenges by re-imagining the Reviewer\nAssignment Problem (RAP) as a retrieval task. Utilizing the extensive data from\nOpenAlex, we propose a novel approach that considers a host of signals from the\nauthors, most similar experts, and the citation relations as potential\nindicators for a suitable reviewer for a manuscript. This approach allows us to\ndevelop a standard benchmark dataset for evaluating the reviewer assignment\nproblem without needing explicit labels. We benchmark various methods,\nincluding traditional lexical matching, static neural embeddings, and\ncontextualized neural embeddings, and introduce evaluation metrics that assess\nboth relevance and diversity in the context of RAP. Our results indicate that\nwhile traditional methods perform reasonably well, contextualized embeddings\ntrained on scholarly literature show the best performance. The findings\nunderscore the importance of further research to enhance the diversity and\neffectiveness of reviewer assignments.\n","authors":["Sajad Ebrahimi","Sara Salamat","Negar Arabzadeh","Mahdi Bashari","Ebrahim Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.07683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07658v1","updated":"2025-02-11T15:46:28Z","published":"2025-02-11T15:46:28Z","title":"IU4Rec: Interest Unit-Based Product Organization and Recommendation for\n  E-Commerce Platform","summary":"  Most recommendation systems typically follow a product-based paradigm\nutilizing user-product interactions to identify the most engaging items for\nusers. However, this product-based paradigm has notable drawbacks for\nXianyu~\\footnote{Xianyu is China's largest online C2C e-commerce platform where\na large portion of the product are post by individual sellers}. Most of the\nproduct on Xianyu posted from individual sellers often have limited stock\navailable for distribution, and once the product is sold, it's no longer\navailable for distribution. This result in most items distributed product on\nXianyu having relatively few interactions, affecting the effectiveness of\ntraditional recommendation depending on accumulating user-item interactions. To\naddress these issues, we introduce \\textbf{IU4Rec}, an \\textbf{I}nterest\n\\textbf{U}nit-based two-stage \\textbf{Rec}ommendation system framework. We\nfirst group products into clusters based on attributes such as category, image,\nand semantics. These IUs are then integrated into the Recommendation system,\ndelivering both product and technological innovations. IU4Rec begins by\ngrouping products into clusters based on attributes such as category, image,\nand semantics, forming Interest Units (IUs). Then we redesign the\nrecommendation process into two stages. In the first stage, the focus is on\nrecommend these Interest Units, capturing broad-level interests. In the second\nstage, it guides users to find the best option among similar products within\nthe selected Interest Unit. User-IU interactions are incorporated into our\nranking models, offering the advantage of more persistent IU behaviors compared\nto item-specific interactions. Experimental results on the production dataset\nand online A/B testing demonstrate the effectiveness and superiority of our\nproposed IU-centric recommendation approach.\n","authors":["Wenhao Wu","Xiaojie Li","Lin Wang","Jialiang Zhou","Di Wu","Qinye Xie","Qingheng Zhang","Yin Zhang","Shuguang Han","Fei Huang","Junfeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07658v1.pdf","comment":"Under review at KDD25 ADS. This work has already been deployed on the\n  Xianyu platform in Alibaba. arXiv admin note: substantial text overlap with\n  arXiv:2403.06747"},{"id":"http://arxiv.org/abs/2502.06097v2","updated":"2025-02-11T14:44:47Z","published":"2025-02-10T02:06:17Z","title":"NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized\n  Recommendation Systems","summary":"  Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list. Due to the inherent challenges of\ncombinatorial search spaces, some current research adopts an\nevaluator-generator paradigm, with a generator generating feasible sequences\nand an evaluator selecting the best sequence based on the estimated list\nutility. However, these methods still face two issues. Firstly, due to the goal\ninconsistency problem between the evaluator and generator, the generator tends\nto fit the local optimal solution of exposure distribution rather than\ncombinatorial space optimization. Secondly, the strategy of generating target\nitems one by one is difficult to achieve optimality because it ignores the\ninformation of subsequent items.\n  To address these issues, we propose a utilizing Neighbor Lists model for\nGenerative Reranking (NLGR), which aims to improve the performance of the\ngenerator in the combinatorial space. NLGR follows the evaluator-generator\nparadigm and improves the generator's training and generating methods.\nSpecifically, we use neighbor lists in combination space to enhance the\ntraining process, making the generator perceive the relative scores and find\nthe optimization direction. Furthermore, we propose a novel sampling-based\nnon-autoregressive generation method, which allows the generator to jump\nflexibly from the current list to any neighbor list. Extensive experiments on\npublic and industrial datasets validate NLGR's effectiveness and we have\nsuccessfully deployed NLGR on the Meituan food delivery platform.\n","authors":["Shuli Wang","Xue Wei","Senjie Kou","Chi Wang","Wenshuai Chen","Qi Tang","Yinhua Zhu","Xiong Xiao","Xingxing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06097v2.pdf","comment":"Accepted by WWW 2025 Industry Track"},{"id":"http://arxiv.org/abs/2502.07491v1","updated":"2025-02-11T11:51:07Z","published":"2025-02-11T11:51:07Z","title":"Exploring Patterns Behind Sports","summary":"  This paper presents a comprehensive framework for time series prediction\nusing a hybrid model that combines ARIMA and LSTM. The model incorporates\nfeature engineering techniques, including embedding and PCA, to transform raw\ndata into a lower-dimensional representation while retaining key information.\nThe embedding technique is used to convert categorical data into continuous\nvectors, facilitating the capture of complex relationships. PCA is applied to\nreduce dimensionality and extract principal components, enhancing model\nperformance and computational efficiency. To handle both linear and nonlinear\npatterns in the data, the ARIMA model captures linear trends, while the LSTM\nmodel models complex nonlinear dependencies. The hybrid model is trained on\nhistorical data and achieves high accuracy, as demonstrated by low RMSE and MAE\nscores. Additionally, the paper employs the run test to assess the randomness\nof sequences, providing insights into the underlying patterns. Ablation studies\nare conducted to validate the roles of different components in the model,\ndemonstrating the significance of each module. The paper also utilizes the SHAP\nmethod to quantify the impact of traditional advantages on the predicted\nresults, offering a detailed understanding of feature importance. The KNN\nmethod is used to determine the optimal prediction interval, further enhancing\nthe model's accuracy. The results highlight the effectiveness of combining\ntraditional statistical methods with modern deep learning techniques for robust\ntime series forecasting in Sports.\n","authors":["Chang Liu","Chengcheng Ma","XuanQi Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07474v1","updated":"2025-02-11T11:34:33Z","published":"2025-02-11T11:34:33Z","title":"ETimeline: An Extensive Timeline Generation Dataset based on Large\n  Language Model","summary":"  Timeline generation is of great significance for a comprehensive\nunderstanding of the development of events over time. Its goal is to organize\nnews chronologically, which helps to identify patterns and trends that may be\nobscured when viewing news in isolation, making it easier to track the\ndevelopment of stories and understand the interrelationships between key\nevents. Timelines are now common in various commercial products, but academic\nresearch in this area is notably scarce. Additionally, the current datasets are\nin need of refinement for enhanced utility and expanded coverage. In this\npaper, we propose ETimeline, which encompasses over $13,000$ news articles,\nspanning $600$ bilingual timelines across $28$ news domains. Specifically, we\ngather a candidate pool of more than $120,000$ news articles and employ the\nlarge language model (LLM) Pipeline to improve performance, ultimately yielding\nthe ETimeline. The data analysis underscores the appeal of ETimeline.\nAdditionally, we also provide the news pool data for further research and\nanalysis. This work contributes to the advancement of timeline generation\nresearch and supports a wide range of tasks, including topic generation and\nevent relationships. We believe that this dataset will serve as a catalyst for\ninnovative research and bridge the gap between academia and industry in\nunderstanding the practical application of technology services. The dataset is\navailable at https://zenodo.org/records/11392212\n","authors":["Xiaochen Liu","Yanan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16125v3","updated":"2025-02-11T11:19:40Z","published":"2025-01-27T15:12:27Z","title":"SampleLLM: Optimizing Tabular Data Synthesis in Recommendations","summary":"  Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.\n","authors":["Jingtong Gao","Zhaocheng Du","Xiaopeng Li","Yichao Wang","Xiangyang Li","Huifeng Guo","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.16125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00321v3","updated":"2025-02-11T10:55:02Z","published":"2025-02-01T05:06:21Z","title":"MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior\n  Modeling","summary":"  Click-Through Rate (CTR) prediction is a crucial task in recommendation\nsystems, online searches, and advertising platforms, where accurately capturing\nusers' real interests in content is essential for performance. However,\nexisting methods heavily rely on ID embeddings, which fail to reflect users'\ntrue preferences for content such as images and titles. This limitation becomes\nparticularly evident in cold-start and long-tail scenarios, where traditional\napproaches struggle to deliver effective results. To address these challenges,\nwe propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which\nconsists of three key stages: Pre-training, Content-Interest-Aware Supervised\nFine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training\nstage adapts foundational models to domain-specific data, enabling the\nextraction of high-quality multi-modal embeddings. The C-SFT stage bridges the\nsemantic gap between content and user interests by leveraging user behavior\nsignals to guide the alignment of embeddings with user preferences. Finally,\nthe CiUBM stage integrates multi-modal embeddings and ID-based collaborative\nfiltering signals into a unified framework. Comprehensive offline experiments\nand online A/B tests conducted on the Taobao, one of the world's largest\ne-commerce platforms, demonstrated the effectiveness and efficiency of MIM\nmethod. The method has been successfully deployed online, achieving a\nsignificant increase of +14.14% in CTR and +4.12% in RPM, showcasing its\nindustrial applicability and substantial impact on platform performance. To\npromote further research, we have publicly released the code and dataset at\nhttps://pan.quark.cn/s/8fc8ec3e74f3.\n","authors":["Bencheng Yan","Si Chen","Shichang Jia","Jianyu Liu","Yueran Liu","Chenghan Fu","Wanxian Guan","Hui Zhao","Xiang Zhang","Kai Zhang","Wenbo Su","Pengjie Wang","Jian Xu","Bo Zheng","Baolin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.00321v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17670v2","updated":"2025-02-11T09:52:43Z","published":"2025-01-29T14:20:42Z","title":"Distinguished Quantized Guidance for Diffusion-based Sequence\n  Recommendation","summary":"  Diffusion models (DMs) have emerged as promising approaches for sequential\nrecommendation due to their strong ability to model data distributions and\ngenerate high-quality items. Existing work typically adds noise to the next\nitem and progressively denoises it guided by the user's interaction sequence,\ngenerating items that closely align with user interests. However, we identify\ntwo key issues in this paradigm. First, the sequences are often heterogeneous\nin length and content, exhibiting noise due to stochastic user behaviors. Using\nsuch sequences as guidance may hinder DMs from accurately understanding user\ninterests. Second, DMs are prone to data bias and tend to generate only the\npopular items that dominate the training dataset, thus failing to meet the\npersonalized needs of different users. To address these issues, we propose\nDistinguished Quantized Guidance for Diffusion-based Sequence Recommendation\n(DiQDiff), which aims to extract robust guidance to understand user interests\nand generate distinguished items for personalized user interests within DMs. To\nextract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ)\nto quantize sequences into semantic vectors (e.g., collaborative signals and\ncategory interests) using a codebook, which can enrich the guidance to better\nunderstand user interests. To generate distinguished items, DiQDiff\npersonalizes the generation through Contrastive Discrepancy Maximization (CDM),\nwhich maximizes the distance between denoising trajectories using contrastive\nloss to prevent biased generation for different users. Extensive experiments\nare conducted to compare DiQDiff with multiple baseline models across four\nwidely-used datasets. The superior recommendation performance of DiQDiff\nagainst leading approaches demonstrates its effectiveness in sequential\nrecommendation tasks.\n","authors":["Wenyu Mao","Shuchang Liu","Haoyang Liu","Haozhe Liu","Xiang Li","Lantao Hu"],"pdf_url":"https://arxiv.org/pdf/2501.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03984v2","updated":"2025-02-11T09:07:15Z","published":"2023-10-06T02:45:21Z","title":"AURO: Reinforcement Learning for Adaptive User Retention Optimization in\n  Recommender Systems","summary":"  The field of Reinforcement Learning (RL) has garnered increasing attention\nfor its ability of optimizing user retention in recommender systems. A primary\nobstacle in this optimization process is the environment non-stationarity\nstemming from the continual and complex evolution of user behavior patterns\nover time, such as variations in interaction rates and retention propensities.\nThese changes pose significant challenges to existing RL algorithms for\nrecommendations, leading to issues with dynamics and reward distribution\nshifts. This paper introduces a novel approach called \\textbf{A}daptive\n\\textbf{U}ser \\textbf{R}etention \\textbf{O}ptimization (AURO) to address this\nchallenge. To navigate the recommendation policy in non-stationary\nenvironments, AURO introduces an state abstraction module in the policy\nnetwork. The module is trained with a new value-based loss function, aligning\nits output with the estimated performance of the current policy. As the policy\nperformance of RL is sensitive to environment drifts, the loss function enables\nthe state abstraction to be reflective of environment changes and notify the\nrecommendation policy to adapt accordingly. Additionally, the non-stationarity\nof the environment introduces the problem of implicit cold start, where the\nrecommendation policy continuously interacts with users displaying novel\nbehavior patterns. AURO encourages exploration guarded by performance-based\nrejection sampling to maintain a stable recommendation quality in the\ncost-sensitive online environment. Extensive empirical analysis are conducted\nin a user retention simulator, the MovieLens dataset, and a live short-video\nrecommendation platform, demonstrating AURO's superior performance against all\nevaluated baseline algorithms.\n","authors":["Zhenghai Xue","Qingpeng Cai","Bin Yang","Lantao Hu","Peng Jiang","Kun Gai","Bo An"],"pdf_url":"https://arxiv.org/pdf/2310.03984v2.pdf","comment":"The Web Conference 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.07327v1","updated":"2025-02-11T07:43:47Z","published":"2025-02-11T07:43:47Z","title":"Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated\n  Videos","summary":"  With the rapid development of AI-generated content (AIGC), the creation of\nhigh-quality AI-generated videos has become faster and easier, resulting in the\nInternet being flooded with all kinds of video content. However, the impact of\nthese videos on the content ecosystem remains largely unexplored. Video\ninformation retrieval remains a fundamental approach for accessing video\ncontent. Building on the observation that retrieval models often favor\nAI-generated content in ad-hoc and image retrieval tasks, we investigate\nwhether similar biases emerge in the context of challenging video retrieval,\nwhere temporal and visual factors may further influence model behavior. To\nexplore this, we first construct a comprehensive benchmark dataset containing\nboth real and AI-generated videos, along with a set of fair and rigorous\nmetrics to assess bias. This benchmark consists of 13,000 videos generated by\ntwo state-of-the-art open-source video generation models. We meticulously\ndesign a suite of rigorous metrics to accurately measure this preference,\naccounting for potential biases arising from the limited frame rate and\nsuboptimal quality of AIGC videos. We then applied three off-the-shelf video\nretrieval models to perform retrieval tasks on this hybrid dataset. Our\nfindings reveal a clear preference for AI-generated videos in retrieval.\nFurther investigation shows that incorporating AI-generated videos into the\ntraining set of retrieval models exacerbates this bias. Unlike the preference\nobserved in image modalities, we find that video retrieval bias arises from\nboth unseen visual and temporal information, making the root causes of video\nbias a complex interplay of these two factors. To mitigate this bias, we\nfine-tune the retrieval models using a contrastive learning approach. The\nresults of this study highlight the potential implications of AI-generated\nvideos on retrieval systems.\n","authors":["Haowen Gao","Liang Pang","Shicheng Xu","Leigang Qu","Tat-Seng Chua","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05558v2","updated":"2025-02-11T07:36:07Z","published":"2025-02-08T13:08:11Z","title":"Large Memory Network for Recommendation","summary":"  Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day.\n","authors":["Hui Lu","Zheng Chai","Yuchao Zheng","Zhe Chen","Deping Xie","Peng Xu","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.05558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20163v2","updated":"2025-02-11T07:35:58Z","published":"2024-10-26T12:34:07Z","title":"UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers","summary":"  Existing information retrieval (IR) models often assume a homogeneous\nstructure for knowledge sources and user queries, limiting their applicability\nin real-world settings where retrieval is inherently heterogeneous and diverse.\nIn this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous\nknowledge retriever that (1) builds a unified retrieval space for heterogeneous\nknowledge and (2) follows diverse user instructions to retrieve knowledge of\nspecified types. UniHGKR consists of three principal stages: heterogeneous\nself-supervised pretraining, text-anchored embedding alignment, and\ninstruction-aware retriever fine-tuning, enabling it to generalize across\nvaried retrieval contexts. This framework is highly scalable, with a BERT-based\nversion and a UniHGKR-7B version trained on large language models. Also, we\nintroduce CompMix-IR, the first native heterogeneous knowledge retrieval\nbenchmark. It includes two retrieval scenarios with various instructions, over\n9,400 question-answer (QA) pairs, and a corpus of 10 million entries, covering\nfour different types of data. Extensive experiments show that UniHGKR\nconsistently outperforms state-of-the-art methods on CompMix-IR, achieving up\nto 6.36% and 54.23% relative improvements in two scenarios, respectively.\nFinally, by equipping our retriever for open-domain heterogeneous QA systems,\nwe achieve a new state-of-the-art result on the popular ConvMix task, with an\nabsolute improvement of up to 5.90 points.\n","authors":["Dehai Min","Zhiyang Xu","Guilin Qi","Lifu Huang","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2410.20163v2.pdf","comment":"NAACL 2025, Main, Long Paper"},{"id":"http://arxiv.org/abs/2502.07315v1","updated":"2025-02-11T07:25:57Z","published":"2025-02-11T07:25:57Z","title":"Prompt-Based Document Modifications In Ranking Competitions","summary":"  We study prompting-based approaches with Large Language Models (LLMs) for\nmodifying documents so as to promote their ranking in a competitive search\nsetting. Our methods are inspired by prior work on leveraging LLMs as rankers.\nWe evaluate our approach by deploying it as a bot in previous ranking\ncompetitions and in competitions we organized. Our findings demonstrate that\nour approach effectively improves document ranking while preserving high levels\nof faithfulness to the original content and maintaining overall document\nquality.\n","authors":["Niv Bardas","Tommy Mordo","Oren Kurland","Moshe Tennenholtz","Gal Zur"],"pdf_url":"https://arxiv.org/pdf/2502.07315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05523v2","updated":"2025-02-11T07:21:41Z","published":"2025-02-08T11:05:22Z","title":"Adaptive Domain Scaling for Personalized Sequential Modeling in\n  Recommenders","summary":"  Users generally exhibit complex behavioral patterns and diverse intentions in\nmultiple business scenarios of super applications like Douyin, presenting great\nchallenges to current industrial multi-domain recommenders. To mitigate the\ndiscrepancies across diverse domains, researches and industrial practices\ngenerally emphasize sophisticated network structures to accomodate diverse data\ndistributions, while neglecting the inherent understanding of user behavioral\nsequence from the multi-domain perspective. In this paper, we present Adaptive\nDomain Scaling (ADS) model, which comprehensively enhances the personalization\ncapability in target-aware sequence modeling across multiple domains.\nSpecifically, ADS comprises of two major modules, including personalized\nsequence representation generation (PSRG) and personalized candidate\nrepresentation generation (PCRG). The modules contribute to the tailored\nmulti-domain learning by dynamically learning both the user behavioral sequence\nitem representation and the candidate target item representation under\ndifferent domains, facilitating adaptive user intention understanding.\nExperiments are performed on both a public dataset and two billion-scaled\nindustrial datasets, and the extensive results verify the high effectiveness\nand compatibility of ADS. Besides, we conduct online experiments on two\ninfluential business scenarios including Douyin Advertisement Platform and\nDouyin E-commerce Service Platform, both of which show substantial business\nimprovements. Currently, ADS has been fully deployed in many recommendation\nservices at ByteDance, serving billions of users.\n","authors":["Zheng Chai","Hui Lu","Di Chen","Qin Ren","Yuchao Zheng","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.05523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07307v1","updated":"2025-02-11T07:09:49Z","published":"2025-02-11T07:09:49Z","title":"CreAgent: Towards Long-Term Evaluation of Recommender System under\n  Platform-Creator Information Asymmetry","summary":"  Ensuring the long-term sustainability of recommender systems (RS) emerges as\na crucial issue. Traditional offline evaluation methods for RS typically focus\non immediate user feedback, such as clicks, but they often neglect the\nlong-term impact of content creators. On real-world content platforms, creators\ncan strategically produce and upload new items based on user feedback and\npreference trends. While previous studies have attempted to model creator\nbehavior, they often overlook the role of information asymmetry. This asymmetry\narises because creators primarily have access to feedback on the items they\nproduce, while platforms possess data on the entire spectrum of user feedback.\nCurrent RS simulators, however, fail to account for this asymmetry, leading to\ninaccurate long-term evaluations. To address this gap, we propose CreAgent, a\nLarge Language Model (LLM)-empowered creator simulation agent. By incorporating\ngame theory's belief mechanism and the fast-and-slow thinking framework,\nCreAgent effectively simulates creator behavior under conditions of information\nasymmetry. Additionally, we enhance CreAgent's simulation ability by\nfine-tuning it using Proximal Policy Optimization (PPO). Our credibility\nvalidation experiments show that CreAgent aligns well with the behaviors\nbetween real-world platform and creator, thus improving the reliability of\nlong-term RS evaluations. Moreover, through the simulation of RS involving\nCreAgents, we can explore how fairness- and diversity-aware RS algorithms\ncontribute to better long-term performance for various stakeholders. CreAgent\nand the simulation platform are publicly available at\nhttps://github.com/shawnye2000/CreAgent.\n","authors":["Xiaopeng Ye","Chen Xu","Zhongxiang Sun","Jun Xu","Gang Wang","Zhenhua Dong","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.07307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07303v1","updated":"2025-02-11T07:01:19Z","published":"2025-02-11T07:01:19Z","title":"Flow Matching for Collaborative Filtering","summary":"  Generative models have shown great promise in collaborative filtering by\ncapturing the underlying distribution of user interests and preferences.\nHowever, existing approaches struggle with inaccurate posterior approximations\nand misalignment with the discrete nature of recommendation data, limiting\ntheir expressiveness and real-world performance. To address these limitations,\nwe propose FlowCF, a novel flow-based recommendation system leveraging flow\nmatching for collaborative filtering. We tailor flow matching to the unique\nchallenges in recommendation through two key innovations: (1) a behavior-guided\nprior that aligns with user behavior patterns to handle the sparse and\nheterogeneous user-item interactions, and (2) a discrete flow framework to\npreserve the binary nature of implicit feedback while maintaining the benefits\nof flow matching, such as stable training and efficient inference. Extensive\nexperiments demonstrate that FlowCF achieves state-of-the-art recommendation\naccuracy across various datasets with the fastest inference speed, making it a\ncompelling approach for real-world recommender systems.\n","authors":["Chengkai Liu","Yangtian Zhang","Jianling Wang","Rex Ying","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2502.07303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09401v6","updated":"2025-02-11T06:26:34Z","published":"2023-10-13T20:53:50Z","title":"CROWN: A Novel Approach to Comprehending Users' Preferences for Accurate\n  Personalized News Recommendation","summary":"  Personalized news recommendation aims to assist users in finding news\narticles that align with their interests, which plays a pivotal role in\nmitigating users' information overload problem. Although many recent works have\nbeen studied for better personalized news recommendation, the following\nchallenges should be explored more: (C1) Comprehending manifold intents coupled\nwithin a news article, (C2) Differentiating varying post-read preferences of\nnews articles, and (C3) Addressing the cold-start user problem. To tackle the\naforementioned challenges together, in this paper, we propose a novel\npersonalized news recommendation framework (CROWN) that employs (1)\ncategory-guided intent disentanglement for (C1), (2) consistency-based news\nrepresentation for (C2), and (3) GNN-enhanced hybrid user representation for\n(C3). Furthermore, we incorporate a category prediction into the training\nprocess of CROWN as an auxiliary task, which provides supplementary supervisory\nsignals to enhance intent disentanglement. Extensive experiments on two\nreal-world datasets reveal that (1) CROWN provides consistent performance\nimprovements over ten state-of-the-art news recommendation methods and (2) the\nproposed strategies significantly improve the accuracy of CROWN.\n","authors":["Yunyong Ko","Seongeun Ryu","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2310.09401v6.pdf","comment":"11 pages, 7 figures, 9 tables, the ACM Web Conference (WWW) 2025"},{"id":"http://arxiv.org/abs/2502.06101v2","updated":"2025-02-11T05:53:00Z","published":"2025-02-10T02:15:12Z","title":"RALLRec: Improving Retrieval Augmented Large Language Model\n  Recommendation with Representation Learning","summary":"  Large Language Models (LLMs) have been integrated into recommendation systems\nto enhance user behavior comprehension. The Retrieval Augmented Generation\n(RAG) technique is further incorporated into these systems to retrieve more\nrelevant items and improve system performance. However, existing RAG methods\nrely primarily on textual semantics and often fail to incorporate the most\nrelevant items, limiting the effectiveness of the systems.\n  In this paper, we propose Representation learning for retrieval-Augmented\nLarge Language model Recommendation (RALLRec). Specifically, we enhance textual\nsemantics by prompting LLMs to generate more detailed item descriptions,\nfollowed by joint representation learning of textual and collaborative\nsemantics, which are extracted by the LLM and recommendation models,\nrespectively. Considering the potential time-varying characteristics of user\ninterest, a simple yet effective reranking method is further introduced to\ncapture the dynamics of user preference. We conducted extensive experiments on\nthree real-world datasets, and the evaluation results validated the\neffectiveness of our method. Code is made public at\nhttps://github.com/JianXu95/RALLRec.\n","authors":["Jian Xu","Sichun Luo","Xiangyu Chen","Haoming Huang","Hanxu Hou","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2502.06101v2.pdf","comment":"Accepted by TheWebConf'25 (WWW'25) as a Short Paper"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.00950v3","updated":"2025-02-11T18:24:41Z","published":"2025-02-02T23:04:55Z","title":"Fast Audio Codec Identification Using Overlapping LCS","summary":"  Audio data are widely exchanged over telecommunications networks. Due to the\nlimitations of network resources, these data are typically compressed before\ntransmission. Various methods are available for compressing audio data. To\naccess such audio information, it is first necessary to identify the codec used\nfor compression. One of the most effective approaches for audio codec\nidentification involves analyzing the content of received packets. In these\nmethods, statistical features extracted from the packets are utilized to\ndetermine the codec employed. This paper proposes a novel method for audio\ncodec classification based on features derived from the overlapped longest\ncommon sub-string and sub-sequence (LCS). The simulation results, which\nachieved an accuracy of 97% for 8 KB packets, demonstrate the superiority of\nthe proposed method over conventional approaches. This method divides each 8 KB\npacket into fifteen 1 KB packets with a 50% overlap. The results indicate that\nthis division has no significant impact on the simulation outcomes, while\nsignificantly speeding up the feature extraction, being eight times faster than\nthe traditional method for extracting LCS features.\n","authors":["Farzane Jafari"],"pdf_url":"https://arxiv.org/pdf/2502.00950v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.07711v1","updated":"2025-02-11T17:10:31Z","published":"2025-02-11T17:10:31Z","title":"RenderBox: Expressive Performance Rendering with Text Control","summary":"  Expressive music performance rendering involves interpreting symbolic scores\nwith variations in timing, dynamics, articulation, and instrument-specific\ntechniques, resulting in performances that capture musical can emotional\nintent. We introduce RenderBox, a unified framework for text-and-score\ncontrolled audio performance generation across multiple instruments, applying\ncoarse-level controls through natural language descriptions and granular-level\ncontrols using music scores. Based on a diffusion transformer architecture and\ncross-attention joint conditioning, we propose a curriculum-based paradigm that\ntrains from plain synthesis to expressive performance, gradually incorporating\ncontrollable factors such as speed, mistakes, and style diversity.\n  RenderBox achieves high performance compared to baseline models across key\nmetrics such as FAD and CLAP, and also tempo and pitch accuracy under different\nprompting tasks. Subjective evaluation further demonstrates that RenderBox is\nable to generate controllable expressive performances that sound natural and\nmusically engaging, aligning well with prompts and intent.\n","authors":["Huan Zhang","Akira Maezawa","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2502.07711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07411v1","updated":"2025-02-11T09:45:06Z","published":"2025-02-11T09:45:06Z","title":"EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering","summary":"  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for\negocentric QA assistance involving scene text. EgoTextVQA contains 1.5K\nego-view videos and 7K scene-text aware questions that reflect real-user needs\nin outdoor driving and indoor house-keeping activities. The questions are\ndesigned to elicit identification and reasoning on scene text in an egocentric\nand dynamic environment. With EgoTextVQA, we comprehensively evaluate 10\nprominent multimodal large language models. Currently, all models struggle, and\nthe best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the\nsevere deficiency of these techniques in egocentric QA assistance. Our further\ninvestigations suggest that precise temporal grounding and multi-frame\nreasoning, along with high resolution and auxiliary scene-text inputs, are key\nfor better performance. With thorough analyses and heuristic suggestions, we\nhope EgoTextVQA can serve as a solid testbed for research in egocentric\nscene-text QA assistance.\n","authors":["Sheng Zhou","Junbin Xiao","Qingyun Li","Yicong Li","Xun Yang","Dan Guo","Meng Wang","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.07411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07160v1","updated":"2025-02-11T00:56:44Z","published":"2025-02-11T00:56:44Z","title":"HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates","summary":"  Image compression under ultra-low bitrates remains challenging for both\nconventional learned image compression (LIC) and generative vector-quantized\n(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy\nquantization, while generative VQ modeling gives poor fidelity due to the\nmismatch between learned generative priors and specific inputs. In this work,\nwe propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream\nframework that utilizes both generative VQ-modeling and diffusion models, as\nwell as conventional LIC, to achieve both high fidelity and high perceptual\nquality. Different from previous hybrid methods that directly use pre-trained\nLIC models to generate low-quality fidelity-preserving information from heavily\nquantized latent, we use diffusion models to extract high-quality complimentary\nfidelity information from the ground-truth input, which can enhance the system\nperformance in several aspects: improving indices map prediction, enhancing the\nfidelity-preserving output of the LIC stream, and refining conditioned image\nreconstruction with VQ-latent correction. In addition, our diffusion model is\nbased on a dense representative vector (DRV), which is lightweight with very\nsimple sampling schedulers. Extensive experiments demonstrate that our\nHDCompression outperforms the previous conventional LIC, generative\nVQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative\nvisualization, providing balanced robust compression performance at ultra-low\nbitrates.\n","authors":["Lei Lu","Yize Li","Yanzhi Wang","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07160v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2404.03161v2","updated":"2025-02-11T00:45:46Z","published":"2024-04-04T02:22:37Z","title":"BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro\n  QR Codes","summary":"  This paper introduces BioVL-QR, a biochemical vision-and-language dataset\ncomprising 23 egocentric experiment videos, corresponding protocols, and\nvision-and-language alignments. A major challenge in understanding biochemical\nvideos is detecting equipment, reagents, and containers because of the\ncluttered environment and indistinguishable objects. Previous studies assumed\nmanual object annotation, which is costly and time-consuming. To address the\nissue, we focus on Micro QR Codes. However, detecting objects using only Micro\nQR Codes is still difficult due to blur and occlusion caused by object\nmanipulation. To overcome this, we propose an object labeling method combining\na Micro QR Code detector with an off-the-shelf hand object detector. As an\napplication of the method and BioVL-QR, we tackled the task of localizing the\nprocedural steps in an instructional video. The experimental results show that\nusing Micro QR Codes and our method improves biochemical video understanding.\nData and code are available through https://nishi10mo.github.io/BioVL-QR/\n","authors":["Tomohiro Nishimoto","Taichi Nishimura","Koki Yamamoto","Keisuke Shirai","Hirotaka Kameko","Yuto Haneji","Tomoya Yoshida","Keiya Kajimura","Taiyu Cui","Chihiro Nishiwaki","Eriko Daikoku","Natsuko Okuda","Fumihito Ono","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2404.03161v2.pdf","comment":"6 pages"}]},"2025-02-10T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.07067v1","updated":"2025-02-10T21:59:01Z","published":"2025-02-10T21:59:01Z","title":"Repository-level Code Search with Neural Retrieval Methods","summary":"  This paper presents a multi-stage reranking system for repository-level code\nsearch, which leverages the vastly available commit histories of large\nopen-source repositories to aid in bug fixing. We define the task of\nrepository-level code search as retrieving the set of files from the current\nstate of a code repository that are most relevant to addressing a user's\nquestion or bug. The proposed approach combines BM25-based retrieval over\ncommit messages with neural reranking using CodeBERT to identify the most\npertinent files. By learning patterns from diverse repositories and their\ncommit histories, the system can surface relevant files for the task at hand.\nThe system leverages both commit messages and source code for relevance\nmatching, and is evaluated in both normal and oracle settings. Experiments on a\nnew dataset created from 7 popular open-source repositories demonstrate\nsubstantial improvements of up to 80% in MAP, MRR and P@1 over the BM25\nbaseline, across a diverse set of queries, demonstrating the effectiveness this\napproach. We hope this work aids LLM agents as a tool for better code search\nand understanding. Our code and results obtained are publicly available.\n","authors":["Siddharth Gandhi","Luyu Gao","Jamie Callan"],"pdf_url":"https://arxiv.org/pdf/2502.07067v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2412.06949v2","updated":"2025-02-10T21:34:00Z","published":"2024-12-09T19:53:13Z","title":"Bridging Conversational and Collaborative Signals for Conversational\n  Recommendation","summary":"  Conversational recommendation systems (CRS) leverage contextual information\nfrom conversations to generate recommendations but often struggle due to a lack\nof collaborative filtering (CF) signals, which capture user-item interaction\npatterns essential for accurate recommendations. We introduce Reddit-ML32M, a\ndataset that links Reddit conversations with interactions on MovieLens 32M, to\nenrich item representations by leveraging collaborative knowledge and\naddressing interaction sparsity in conversational datasets. We propose an\nLLM-based framework that uses Reddit-ML32M to align LLM-generated\nrecommendations with CF embeddings, refining rankings for better performance.\nWe evaluate our framework against three sets of baselines: CF-based\nrecommenders using only interactions from CRS tasks, traditional CRS models,\nand LLM-based methods relying on conversational context without item\nrepresentations. Our approach achieves consistent improvements, including a\n12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the\nbest-performing baseline that relies on conversational context but lacks\ncollaborative item representations.\n","authors":["Ahmad Bin Rabiah","Nafis Sadeq","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2412.06949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06705v1","updated":"2025-02-10T17:33:22Z","published":"2025-02-10T17:33:22Z","title":"RSAttAE: An Information-Aware Attention-based Autoencoder Recommender\n  System","summary":"  Recommender systems play a crucial role in modern life, including information\nretrieval, the pharmaceutical industry, retail, and entertainment. The\nentertainment sector, in particular, attracts significant attention and\ngenerates substantial profits. This work proposes a new method for predicting\nunknown user-movie ratings to enhance customer satisfaction. To achieve this,\nwe utilize the MovieLens 100K dataset. Our approach introduces an\nattention-based autoencoder to create meaningful representations and the\nXGBoost method for rating predictions. The results demonstrate that our\nproposal outperforms most of the existing state-of-the-art methods.\nAvailability: github.com/ComputationIASBS/RecommSys\n","authors":["Amirhossein Dadashzadeh Taromi","Sina Heydari","Mohsen Hooshmand","Majid Ramezani"],"pdf_url":"https://arxiv.org/pdf/2502.06705v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.06648v1","updated":"2025-02-10T16:38:03Z","published":"2025-02-10T16:38:03Z","title":"The 2021 Tokyo Olympics Multilingual News Article Dataset","summary":"  In this paper, we introduce a dataset of multilingual news articles covering\nthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from\n1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and\npublished between July 1, 2021, and August 14, 2021. These articles are written\nin nine languages from different language families and in different scripts. To\ncreate the dataset, the raw news articles were first retrieved via a service\nthat collects and analyzes news articles. Then, the articles were grouped using\nan online clustering algorithm, with each group containing articles reporting\non the same sub-event. Finally, the groups were manually annotated and\nevaluated. The development of this dataset aims to provide a resource for\nevaluating the performance of multilingual news clustering algorithms, for\nwhich limited datasets are available. It can also be used to analyze the\ndynamics and events of the 2021 Tokyo Olympics from different perspectives. The\ndataset is available in CSV format and can be accessed from the CLARIN.SI\nrepository.\n","authors":["Erik Novak","Erik Calcina","Dunja Mladenić","Marko Grobelnik"],"pdf_url":"https://arxiv.org/pdf/2502.06648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06557v1","updated":"2025-02-10T15:24:55Z","published":"2025-02-10T15:24:55Z","title":"LiveForesighter: Generating Future Information for Live-Streaming\n  Recommendations at Kuaishou","summary":"  Live-streaming, as a new-generation media to connect users and authors, has\nattracted a lot of attention and experienced rapid growth in recent years.\nCompared with the content-static short-video recommendation, the live-streaming\nrecommendation faces more challenges in giving our users a satisfactory\nexperience: (1) Live-streaming content is dynamically ever-changing along time.\n(2) valuable behaviors (e.g., send digital-gift, buy products) always require\nusers to watch for a long-time (>10 min). Combining the two attributes, here\nraising a challenging question for live-streaming recommendation: How to\ndiscover the live-streamings that the content user is interested in at the\ncurrent moment, and further a period in the future?\n","authors":["Yucheng Lu","Jiangxia Cao","Xu Kuan","Wei Cheng","Wei Jiang","Jiaming Zhang","Yang Shuang","Liu Zhaojie","Liyin Hong"],"pdf_url":"https://arxiv.org/pdf/2502.06557v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.09425v2","updated":"2025-02-10T15:17:49Z","published":"2024-11-14T13:22:41Z","title":"MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity","summary":"  Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.\n","authors":["Xiao Lv","Jiangxia Cao","Shijie Guan","Xiaoyou Zhou","Zhiguang Qi","Yaqiang Zang","Ming Li","Ben Wang","Kun Gai","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.09425v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.06062v4","updated":"2025-02-10T10:55:08Z","published":"2024-10-08T14:09:12Z","title":"LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs","summary":"  We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.\n","authors":["Vincent Emonet","Jerven Bolleman","Severine Duvaud","Tarcisio Mendes de Farias","Ana Claudia Sima"],"pdf_url":"https://arxiv.org/pdf/2410.06062v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06269v1","updated":"2025-02-10T09:08:37Z","published":"2025-02-10T09:08:37Z","title":"Progressive Collaborative and Semantic Knowledge Fusion for Generative\n  Recommendation","summary":"  With the recent surge in interest surrounding generative paradigms,\ngenerative recommendation has increasingly attracted the attention of\nresearchers in the recommendation community. This paradigm generally consists\nof two stages. In the first stage, pretrained semantic embeddings or\ncollaborative ID embeddings are quantized to create item codes, aiming to\ncapture and preserve rich semantic or collaborative knowledge within these\ncodes. The second stage involves utilizing these discrete codes to perform an\nautoregressive sequence generation task. Existing methods often either overlook\ncollaborative or semantic knowledge, or combine the two roughly. In this paper,\nwe observe that naively concatenating representations from semantic and\ncollaborative modality leads to a semantic domination issue, where the\nresulting representation is overly influenced by semantic information,\neffectively overshadowing the collaborative representation. Consequently,\ndownstream recommendation tasks fail to fully exploit the knowledge from both\nmodalities, resulting in suboptimal performance. To address this, we propose a\nprogressive collaborative and semantic knowledge fusion model for generative\nrecommendation, named PRORec, which integrates semantic and collaborative\nknowledge with a unified code through a two-stage framework. Specifically, in\nthe first stage, we propose a cross-modality knowledge alignment task, which\nintegrates semantic knowledge into collaborative embeddings, enhancing their\nrepresentational capability. In the second stage, we propose an in-modality\nknowledge distillation task, designed to effectively capture and integrate\nknowledge from both semantic and collaborative modalities. Extensive\nexperiments on three widely used benchmarks validate the effectiveness of our\napproach, demonstrating its superiority compared to existing methods.\n","authors":["Longtao Xiao","Haozhao Wang","Cheng Wang","Linfei Ji","Yifan Wang","Jieming Zhu","Zhenhua Dong","Rui Zhang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.06269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01457v2","updated":"2025-02-10T08:58:30Z","published":"2024-11-03T06:47:45Z","title":"Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential\n  Recommendation","summary":"  Sequential recommendation (SR) systems excel at capturing users' dynamic\npreferences by leveraging their interaction histories. Most existing SR systems\nassign a single embedding vector to each item to represent its features, and\nvarious types of models are adopted to combine these item embeddings into a\nsequence representation vector to capture the user intent. However, we argue\nthat this representation alone is insufficient to capture an item's\nmulti-faceted nature (e.g., movie genres, starring actors). Besides, users\noften exhibit complex and varied preferences within these facets (e.g., liking\nboth action and musical films in the facet of genre), which are challenging to\nfully represent. To address the issues above, we propose a novel structure\ncalled Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential\nRecommendation (FAME). We leverage sub-embeddings from each head in the last\nmulti-head attention layer to predict the next item separately. This approach\ncaptures the potential multi-faceted nature of items without increasing model\ncomplexity. A gating mechanism integrates recommendations from each head and\ndynamically determines their importance. Furthermore, we introduce a\nMixture-of-Experts (MoE) network in each attention head to disentangle various\nuser preferences within each facet. Each expert within the MoE focuses on a\nspecific preference. A learnable router network is adopted to compute the\nimportance weight for each expert and aggregate them. We conduct extensive\nexperiments on four public sequential recommendation datasets and the results\ndemonstrate the effectiveness of our method over existing baseline models.\n","authors":["Mingrui Liu","Sixiao Zhang","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2411.01457v2.pdf","comment":"This paper has been accepted by WSDM'25"},{"id":"http://arxiv.org/abs/2412.21009v2","updated":"2025-02-10T08:55:18Z","published":"2024-12-30T15:21:36Z","title":"Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline","summary":"  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n","authors":["Nicola Messina","Lucia Vadicamo","Leo Maltese","Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/2412.21009v2.pdf","comment":"Accepted as full paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2502.06252v1","updated":"2025-02-10T08:33:47Z","published":"2025-02-10T08:33:47Z","title":"Evaluating Entity Retrieval in Electronic Health Records: a Semantic Gap\n  Perspective","summary":"  Entity retrieval plays a crucial role in the utilization of Electronic Health\nRecords (EHRs) and is applied across a wide range of clinical practices.\nHowever, a comprehensive evaluation of this task is lacking due to the absence\nof a public benchmark. In this paper, we propose the development and release of\na novel benchmark for evaluating entity retrieval in EHRs, with a particular\nfocus on the semantic gap issue. Using discharge summaries from the MIMIC-III\ndataset, we incorporate ICD codes and prescription labels associated with the\nnotes as queries, and annotate relevance judgments using GPT-4. In total, we\nuse 1,000 patient notes, generate 1,246 queries, and provide over 77,000\nrelevance annotations. To offer the first assessment of the semantic gap, we\nintroduce a novel classification system for relevance matches. Leveraging\nGPT-4, we categorize each relevant pair into one of five categories: string,\nsynonym, abbreviation, hyponym, and implication. Using the proposed benchmark,\nwe evaluate several retrieval methods, including BM25, query expansion, and\nstate-of-the-art dense retrievers. Our findings show that BM25 provides a\nstrong baseline but struggles with semantic matches. Query expansion\nsignificantly improves performance, though it slightly reduces string match\ncapabilities. Dense retrievers outperform traditional methods, particularly for\nsemantic matches, and general-domain dense retrievers often surpass those\ntrained specifically in the biomedical domain.\n","authors":["Zhengyun Zhao","Hongyi Yuan","Jingjing Liu","Haichao Chen","Huaiyuan Ying","Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06252v1.pdf","comment":"Under review, and the dataset will be made public upon reception of\n  our paper"},{"id":"http://arxiv.org/abs/2502.06220v1","updated":"2025-02-10T07:52:47Z","published":"2025-02-10T07:52:47Z","title":"FunduSAM: A Specialized Deep Learning Model for Enhanced Optic Disc and\n  Cup Segmentation in Fundus Images","summary":"  The Segment Anything Model (SAM) has gained popularity as a versatile image\nsegmentation method, thanks to its strong generalization capabilities across\nvarious domains. However, when applied to optic disc (OD) and optic cup (OC)\nsegmentation tasks, SAM encounters challenges due to the complex structures,\nlow contrast, and blurred boundaries typical of fundus images, leading to\nsuboptimal performance. To overcome these challenges, we introduce a novel\nmodel, FunduSAM, which incorporates several Adapters into SAM to create a deep\nnetwork specifically designed for OD and OC segmentation. The FunduSAM utilizes\nAdapter into each transformer block after encoder for parameter fine-tuning\n(PEFT). It enhances SAM's feature extraction capabilities by designing a\nConvolutional Block Attention Module (CBAM), addressing issues related to\nblurred boundaries and low contrast. Given the unique requirements of OD and OC\nsegmentation, polar transformation is used to convert the original fundus OD\nimages into a format better suited for training and evaluating FunduSAM. A\njoint loss is used to achieve structure preservation between the OD and OC,\nwhile accurate segmentation. Extensive experiments on the REFUGE dataset,\ncomprising 1,200 fundus images, demonstrate the superior performance of\nFunduSAM compared to five mainstream approaches.\n","authors":["Jinchen Yu","Yongwei Nie","Fei Qi","Wenxiong Liao","Hongmin Cai"],"pdf_url":"https://arxiv.org/pdf/2502.06220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15797v4","updated":"2025-02-10T05:31:43Z","published":"2025-01-27T05:46:06Z","title":"LemmaHead: RAG Assisted Proof Generation Using Large Language Models","summary":"  Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.\n","authors":["Tianbo Yang","Mingqi Yan","Hongyi Zhao","Tianshuo Yang"],"pdf_url":"https://arxiv.org/pdf/2501.15797v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06148v1","updated":"2025-02-10T04:29:36Z","published":"2025-02-10T04:29:36Z","title":"Optimizing Knowledge Integration in Retrieval-Augmented Generation with\n  Self-Selection","summary":"  Retrieval-Augmented Generation (RAG), which integrates external knowledge\ninto Large Language Models (LLMs), has proven effective in enabling LLMs to\nproduce more accurate and reliable responses. However, it remains a significant\nchallenge how to effectively integrate external retrieved knowledge with\ninternal parametric knowledge in LLMs. In this work, we propose a novel\nSelf-Selection RAG framework, where the LLM is made to select from pairwise\nresponses generated with internal parametric knowledge solely and with external\nretrieved knowledge together to achieve enhanced accuracy. To this end, we\ndevise a Self-Selection-RGP method to enhance the capabilities of the LLM in\nboth generating and selecting the correct answer, by training the LLM with\nDirect Preference Optimization (DPO) over a curated Retrieval Generation\nPreference (RGP) dataset. Experimental results with two open-source LLMs (i.e.,\nLlama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our\napproach over other baseline methods on Natural Questions (NQ) and TrivialQA\ndatasets.\n","authors":["Yan Weng","Fengbin Zhu","Tong Ye","Haoyan Liu","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.06148v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.12949v2","updated":"2025-02-10T00:45:01Z","published":"2024-11-20T00:43:32Z","title":"Epidemiology-informed Network for Robust Rumor Detection","summary":"  The rapid spread of rumors on social media has posed significant challenges\nto maintaining public trust and information integrity. Since an information\ncascade process is essentially a propagation tree, recent rumor detection\nmodels leverage graph neural networks to additionally capture information\npropagation patterns, thus outperforming text-only solutions. Given the\nvariations in topics and social impact of the root node, different source\ninformation naturally has distinct outreach capabilities, resulting in\ndifferent heights of propagation trees. This variation, however, impedes the\ndata-driven design of existing graph-based rumor detectors. Given a shallow\npropagation tree with limited interactions, it is unlikely for graph-based\napproaches to capture sufficient cascading patterns, questioning their ability\nto handle less popular news or early detection needs. In contrast, a deep\npropagation tree is prone to noisy user responses, and this can in turn\nobfuscate the predictions. In this paper, we propose a novel\nEpidemiology-informed Network (EIN) that integrates epidemiological knowledge\nto enhance performance by overcoming data-driven methods sensitivity to data\nquality. Meanwhile, to adapt epidemiology theory to rumor detection, it is\nexpected that each users stance toward the source information will be\nannotated. To bypass the costly and time-consuming human labeling process, we\ntake advantage of large language models to generate stance labels, facilitating\noptimization objectives for learning epidemiology-informed representations. Our\nexperimental results demonstrate that the proposed EIN not only outperforms\nstate-of-the-art methods on real-world datasets but also exhibits enhanced\nrobustness across varying tree depths.\n","authors":["Wei Jiang","Tong Chen","Xinyi Gao","Wentao Zhang","Lizhen Cui","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12949v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.07128v1","updated":"2025-02-10T23:47:35Z","published":"2025-02-10T23:47:35Z","title":"Cardiverse: Harnessing LLMs for Novel Card Game Prototyping","summary":"  The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game designs, an LLM-driven system for consistent\ngame code generation validated by gameplay records, and a gameplay AI\nconstructing method that uses an ensemble of LLM-generated action-value\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers.\n","authors":["Danrui Li","Sen Zhang","Sam S. Sohn","Kaidong Hu","Muhammad Usman","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2502.07128v1.pdf","comment":"13 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.06710v1","updated":"2025-02-10T17:41:57Z","published":"2025-02-10T17:41:57Z","title":"Learning Musical Representations for Music Performance Question\n  Answering","summary":"  Music performances are representative scenarios for audio-visual modeling.\nUnlike common scenarios with sparse audio, music performances continuously\ninvolve dense audio signals throughout. While existing multimodal learning\nmethods on the audio-video QA demonstrate impressive capabilities in general\nscenarios, they are incapable of dealing with fundamental problems within the\nmusic performances: they underexplore the interaction between the multimodal\nsignals in performance and fail to consider the distinctive characteristics of\ninstruments and music. Therefore, existing methods tend to answer questions\nregarding musical performances inaccurately. To bridge the above research gaps,\n(i) given the intricate multimodal interconnectivity inherent to music data,\nour primary backbone is designed to incorporate multimodal interactions within\nthe context of music; (ii) to enable the model to learn music characteristics,\nwe annotate and release rhythmic and music sources in the current music\ndatasets; (iii) for time-aware audio-visual modeling, we align the model's\nmusic predictions with the temporal dimension. Our experiments show\nstate-of-the-art effects on the Music AVQA datasets. Our code is available at\nhttps://github.com/xid32/Amuse.\n","authors":["Xingjian Diao","Chunhui Zhang","Tingxuan Wu","Ming Cheng","Zhongyu Ouyang","Weiyi Wu","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2502.06710v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2502.06616v1","updated":"2025-02-10T16:12:47Z","published":"2025-02-10T16:12:47Z","title":"From Code to Canvas","summary":"  The web-based dynamic geometry software CindyJS is a versatile tool to create\ninteractive applications for mathematics and other topics. In this workshop, we\nwill look at a code package that makes the creation of animations in CindyJS\neasier and more streamlined. Animations, which can then be embedded into\npresentations or be used in (lecture) videos. The focus lies on the creation of\nthe animations themselves and some of the technical and artistic fundamentals\nto do so.\n","authors":["Bernhard O. Werner"],"pdf_url":"https://arxiv.org/pdf/2502.06616v1.pdf","comment":"A workshop paper for the Bridges 2025 conference"},{"id":"http://arxiv.org/abs/2502.06490v1","updated":"2025-02-10T14:08:25Z","published":"2025-02-10T14:08:25Z","title":"Recent Advances in Discrete Speech Tokens: A Review","summary":"  The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.\n","authors":["Yiwei Guo","Zhihan Li","Hankun Wang","Bohan Li","Chongtian Shao","Hanglei Zhang","Chenpeng Du","Xie Chen","Shujie Liu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06490v1.pdf","comment":"26 pages, 8 figures, 3 tables. Work in progress"},{"id":"http://arxiv.org/abs/2408.14823v2","updated":"2025-02-10T11:59:52Z","published":"2024-08-27T07:06:49Z","title":"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming","summary":"  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS with 23% of the original model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n","authors":["Yuang Shi","Géraldine Morin","Simone Gasparini","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2408.14823v2.pdf","comment":"3DV 2025; Project Page: https://yuang-ian.github.io/lapisgs/ ; Code:\n  https://github.com/nus-vv-streams/lapis-gs"},{"id":"http://arxiv.org/abs/2412.21009v2","updated":"2025-02-10T08:55:18Z","published":"2024-12-30T15:21:36Z","title":"Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline","summary":"  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n","authors":["Nicola Messina","Lucia Vadicamo","Leo Maltese","Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/2412.21009v2.pdf","comment":"Accepted as full paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2406.02345v2","updated":"2025-02-10T06:05:46Z","published":"2024-06-04T14:21:41Z","title":"Progressive Confident Masking Attention Network for Audio-Visual\n  Segmentation","summary":"  Audio and visual signals typically occur simultaneously, and humans possess\nan innate ability to correlate and synchronize information from these two\nmodalities. Recently, a challenging problem known as Audio-Visual Segmentation\n(AVS) has emerged, intending to produce segmentation maps for sounding objects\nwithin a scene. However, the methods proposed so far have not sufficiently\nintegrated audio and visual information, and the computational costs have been\nextremely high. Additionally, the outputs of different stages have not been\nfully utilized. To facilitate this research, we introduce a novel Progressive\nConfident Masking Attention Network (PMCANet). It leverages attention\nmechanisms to uncover the intrinsic correlations between audio signals and\nvisual frames. Furthermore, we design an efficient and effective\ncross-attention module to enhance semantic perception by selecting query\ntokens. This selection is determined through confidence-driven units based on\nthe network's multi-stage predictive outputs. Experiments demonstrate that our\nnetwork outperforms other AVS methods while requiring less computational\nresources. The code is available at: https://github.com/PrettyPlate/PCMANet.\n","authors":["Yuxuan Wang","Jinchao Zhu","Feng Dong","Shuyue Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.02345v2.pdf","comment":"23 pages, 11 figures, submitted to Elsevier Knowledge-Based System"}]},"2025-02-09T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.06065v1","updated":"2025-02-09T23:01:03Z","published":"2025-02-09T23:01:03Z","title":"Benchmarking Prompt Sensitivity in Large Language Models","summary":"  Large language Models (LLMs) are highly sensitive to variations in prompt\nformulation, which can significantly impact their ability to generate accurate\nresponses. In this paper, we introduce a new task, Prompt Sensitivity\nPrediction, and a dataset PromptSET designed to investigate the effects of\nslight prompt variations on LLM performance. Using TriviaQA and HotpotQA\ndatasets as the foundation of our work, we generate prompt variations and\nevaluate their effectiveness across multiple LLMs. We benchmark the prompt\nsensitivity prediction task employing state-of-the-art methods from related\ntasks, including LLM-based self-evaluation, text classification, and query\nperformance prediction techniques. Our findings reveal that existing methods\nstruggle to effectively address prompt sensitivity prediction, underscoring the\nneed to understand how information needs should be phrased for accurate LLM\nresponses.\n","authors":["Amirhossein Razavi","Mina Soltangheis","Negar Arabzadeh","Sara Salamat","Morteza Zihayat","Ebrahim Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.06065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10036v2","updated":"2025-02-09T20:56:46Z","published":"2024-01-18T15:00:01Z","title":"LOCALINTEL: Generating Organizational Threat Intelligence from Global\n  and Local Cyber Knowledge","summary":"  Security Operations Center (SoC) analysts gather threat reports from openly\naccessible global threat repositories and tailor the information to their\norganization's needs, such as developing threat intelligence and security\npolicies. They also depend on organizational internal repositories, which act\nas private local knowledge database. These local knowledge databases store\ncredible cyber intelligence, critical operational and infrastructure details.\nSoCs undertake a manual labor-intensive task of utilizing these global threat\nrepositories and local knowledge databases to create both organization-specific\nthreat intelligence and mitigation policies. Recently, Large Language Models\n(LLMs) have shown the capability to process diverse knowledge sources\nefficiently. We leverage this ability to automate this organization-specific\nthreat intelligence generation. We present LocalIntel, a novel automated threat\nintelligence contextualization framework that retrieves zero-day vulnerability\nreports from the global threat repositories and uses its local knowledge\ndatabase to determine implications and mitigation strategies to alert and\nassist the SoC analyst. LocalIntel comprises two key phases: knowledge\nretrieval and contextualization. Quantitative and qualitative assessment has\nshown effectiveness in generating up to 93% accurate organizational threat\nintelligence with 64% inter-rater agreement.\n","authors":["Shaswata Mitra","Subash Neupane","Trisha Chakraborty","Sudip Mittal","Aritran Piplai","Manas Gaur","Shahram Rahimi"],"pdf_url":"https://arxiv.org/pdf/2401.10036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06006v1","updated":"2025-02-09T19:51:00Z","published":"2025-02-09T19:51:00Z","title":"FactIR: A Real-World Zero-shot Open-Domain Retrieval Benchmark for\n  Fact-Checking","summary":"  The field of automated fact-checking increasingly depends on retrieving\nweb-based evidence to determine the veracity of claims in real-world scenarios.\nA significant challenge in this process is not only retrieving relevant\ninformation, but also identifying evidence that can both support and refute\ncomplex claims. Traditional retrieval methods may return documents that\ndirectly address claims or lean toward supporting them, but often struggle with\nmore complex claims requiring indirect reasoning. While some existing\nbenchmarks and methods target retrieval for fact-checking, a comprehensive\nreal-world open-domain benchmark has been lacking. In this paper, we present a\nreal-world retrieval benchmark FactIR, derived from Factiverse production logs,\nenhanced with human annotations. We rigorously evaluate state-of-the-art\nretrieval models in a zero-shot setup on FactIR and offer insights for\ndeveloping practical retrieval systems for fact-checking. Code and data are\navailable at https://github.com/factiverse/factIR.\n","authors":["Venktesh V","Vinay Setty"],"pdf_url":"https://arxiv.org/pdf/2502.06006v1.pdf","comment":"Accepted to WWW 2025 resource track"},{"id":"http://arxiv.org/abs/2411.04798v2","updated":"2025-02-09T19:37:50Z","published":"2024-11-07T15:38:14Z","title":"Orbit: A Framework for Designing and Evaluating Multi-objective Rankers","summary":"  Machine learning in production needs to balance multiple objectives: This is\nparticularly evident in ranking or recommendation models, where conflicting\nobjectives such as user engagement, satisfaction, diversity, and novelty must\nbe considered at the same time. However, designing multi-objective rankers is\ninherently a dynamic wicked problem -- there is no single optimal solution, and\nthe needs evolve over time. Effective design requires collaboration between\ncross-functional teams and careful analysis of a wide range of information. In\nthis work, we introduce Orbit, a conceptual framework for Objective-centric\nRanker Building and Iteration. The framework places objectives at the center of\nthe design process, to serve as boundary objects for communication and guide\npractitioners for design and evaluation. We implement Orbit as an interactive\nsystem, which enables stakeholders to interact with objective spaces directly\nand supports real-time exploration and evaluation of design trade-offs. We\nevaluate Orbit through a user study involving twelve industry practitioners,\nshowing that it supports efficient design space exploration, leads to more\ninformed decision-making, and enhances awareness of the inherent trade-offs of\nmultiple objectives. Orbit (1) opens up new opportunities of an\nobjective-centric design process for any multi-objective ML models, as well as\n(2) sheds light on future designs that push practitioners to go beyond a narrow\nmetric-centric or example-centric mindset.\n","authors":["Chenyang Yang","Tesi Xiao","Michael Shavlovsky","Christian Kästner","Tongshuang Wu"],"pdf_url":"https://arxiv.org/pdf/2411.04798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05924v1","updated":"2025-02-09T14:57:25Z","published":"2025-02-09T14:57:25Z","title":"Multi-Branch Collaborative Learning Network for Video Quality Assessment\n  in Industrial Video Search","summary":"  Video Quality Assessment (VQA) is vital for large-scale video retrieval\nsystems, aimed at identifying quality issues to prioritize high-quality videos.\nIn industrial systems, low-quality video characteristics fall into four\ncategories: visual-related issues like mosaics and black boxes, textual issues\nfrom video titles and OCR content, and semantic issues like frame incoherence\nand frame-text mismatch from AI-generated videos. Despite their prevalence in\nindustrial settings, these low-quality videos have been largely overlooked in\nacademic research, posing a challenge for accurate identification. To address\nthis, we introduce the Multi-Branch Collaborative Network (MBCN) tailored for\nindustrial video retrieval systems. MBCN features four branches, each designed\nto tackle one of the aforementioned quality issues. After each branch\nindependently scores videos, we aggregate these scores using a weighted\napproach and a squeeze-and-excitation mechanism to dynamically address quality\nissues across different scenarios. We implement point-wise and pair-wise\noptimization objectives to ensure score stability and reasonableness. Extensive\noffline and online experiments on a world-level video search engine demonstrate\nMBCN's effectiveness in identifying video quality issues, significantly\nenhancing the retrieval system's ranking performance. Detailed experimental\nanalyses confirm the positive contribution of all four evaluation branches.\nFurthermore, MBCN significantly improves recognition accuracy for low-quality\nAI-generated videos compared to the baseline.\n","authors":["Hengzhu Tang","Zefeng Zhang","Zhiping Li","Zhenyu Zhang","Xing Wu","Li Gao","Suqi Cheng","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2502.05924v1.pdf","comment":"KDD 2025 ADS"},{"id":"http://arxiv.org/abs/2310.09874v5","updated":"2025-02-09T12:44:58Z","published":"2023-10-15T16:15:07Z","title":"TF-DCon: Leveraging Large Language Models (LLMs) to Empower\n  Training-Free Dataset Condensation for Content-Based Recommendation","summary":"  Modern techniques in Content-based Recommendation (CBR) leverage item content\ninformation to provide personalized services to users, but suffer from\nresource-intensive training on large datasets. To address this issue, we\nexplore the dataset condensation for textual CBR in this paper. The goal of\ndataset condensation is to synthesize a small yet informative dataset, upon\nwhich models can achieve performance comparable to those trained on large\ndatasets. While existing condensation approaches are tailored to classification\ntasks for continuous data like images or embeddings, direct application of them\nto CBR has limitations. To bridge this gap, we investigate efficient dataset\ncondensation for content-based recommendation. Inspired by the remarkable\nabilities of large language models (LLMs) in text comprehension and generation,\nwe leverage LLMs to empower the generation of textual content during\ncondensation. To handle the interaction data involving both users and items, we\ndevise a dual-level condensation method: content-level and user-level. At\ncontent-level, we utilize LLMs to condense all contents of an item into a new\ninformative title. At user-level, we design a clustering-based synthesis\nmodule, where we first utilize LLMs to extract user interests. Then, the user\ninterests and user embeddings are incorporated to condense users and generate\ninteractions for condensed users. Notably, the condensation paradigm of this\nmethod is forward and free from iterative optimization on the synthesized\ndataset. Extensive empirical findings from our study, conducted on three\nauthentic datasets, substantiate the efficacy of the proposed method.\nParticularly, we are able to approximate up to 97% of the original performance\nwhile reducing the dataset size by 95% (i.e., on dataset MIND).\n","authors":["Jiahao Wu","Qijiong Liu","Hengchang Hu","Wenqi Fan","Shengcai Liu","Qing Li","Xiao-Ming Wu","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2310.09874v5.pdf","comment":"Full version of TheWebConf'25 accepted paper"},{"id":"http://arxiv.org/abs/2502.05863v1","updated":"2025-02-09T11:46:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07365v2","updated":"2025-02-09T10:46:50Z","published":"2025-01-13T14:34:26Z","title":"Multimodal semantic retrieval for product search","summary":"  Semantic retrieval (also known as dense retrieval) based on textual data has\nbeen extensively studied for both web search and product search application\nfields, where the relevance of a query and a potential target document is\ncomputed by their dense vector representation comparison. Product image is\ncrucial for e-commerce search interactions and is a key factor for customers at\nproduct explorations. However, its impact on semantic retrieval has not been\nwell studied yet. In this research, we build a multimodal representation for\nproduct items in e-commerce search in contrast to pure-text representation of\nproducts, and investigate the impact of such representations. The models are\ndeveloped and evaluated on e-commerce datasets. We demonstrate that a\nmultimodal representation scheme for a product can show improvement either on\npurchase recall or relevance accuracy in semantic retrieval. Additionally, we\nprovide numerical analysis for exclusive matches retrieved by a multimodal\nsemantic retrieval model versus a text-only semantic retrieval model, to\ndemonstrate the validation of multimodal solutions.\n","authors":["Dong Liu","Esther Lopez Ramos"],"pdf_url":"https://arxiv.org/pdf/2501.07365v2.pdf","comment":"Accepted at EReL@MIR WWW 2025"},{"id":"http://arxiv.org/abs/2502.05836v1","updated":"2025-02-09T10:07:05Z","published":"2025-02-09T10:07:05Z","title":"LegalSeg: Unlocking the Structure of Indian Legal Judgments Through\n  Rhetorical Role Classification","summary":"  In this paper, we address the task of semantic segmentation of legal\ndocuments through rhetorical role classification, with a focus on Indian legal\njudgments. We introduce LegalSeg, the largest annotated dataset for this task,\ncomprising over 7,000 documents and 1.4 million sentences, labeled with 7\nrhetorical roles. To benchmark performance, we evaluate multiple\nstate-of-the-art models, including Hierarchical BiLSTM-CRF,\nTransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and\nRole-Aware Transformers, alongside an exploratory RhetoricLLaMA, an\ninstruction-tuned large language model. Our results demonstrate that models\nincorporating broader context, structural relationships, and sequential\nsentence information outperform those relying solely on sentence-level\nfeatures. Additionally, we conducted experiments using surrounding context and\npredicted or actual labels of neighboring sentences to assess their impact on\nclassification accuracy. Despite these advancements, challenges persist in\ndistinguishing between closely related roles and addressing class imbalance.\nOur work underscores the potential of advanced techniques for improving legal\ndocument understanding and sets a strong foundation for future research in\nlegal NLP.\n","authors":["Shubham Kumar Nigam","Tanmay Dubey","Govind Sharma","Noel Shallum","Kripabandhu Ghosh","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2502.05836v1.pdf","comment":"Accepted on NAACL 2025"},{"id":"http://arxiv.org/abs/2502.05822v1","updated":"2025-02-09T09:07:11Z","published":"2025-02-09T09:07:11Z","title":"HCMRM: A High-Consistency Multimodal Relevance Model for Search Ads","summary":"  Search advertising is essential for merchants to reach the target users on\nshort video platforms. Short video ads aligned with user search intents are\ndisplayed through relevance matching and bid ranking mechanisms. This paper\nfocuses on improving query-to-video relevance matching to enhance the\neffectiveness of ranking in ad systems. Recent vision-language pre-training\nmodels have demonstrated promise in various multimodal tasks. However, their\ncontribution to downstream query-video relevance tasks is limited, as the\nalignment between the pair of visual signals and text differs from the modeling\nof the triplet of the query, visual signals, and video text. In addition, our\nprevious relevance model provides limited ranking capabilities, largely due to\nthe discrepancy between the binary cross-entropy fine-tuning objective and the\nranking objective. To address these limitations, we design a high-consistency\nmultimodal relevance model (HCMRM). It utilizes a simple yet effective method\nto enhance the consistency between pre-training and relevance tasks.\nSpecifically, during the pre-training phase, along with aligning visual signals\nand video text, several keywords are extracted from the video text as\npseudo-queries to perform the triplet relevance modeling. For the fine-tuning\nphase, we introduce a hierarchical softmax loss, which enables the model to\nlearn the order within labels while maximizing the distinction between positive\nand negative samples. This promotes the fusion ranking of relevance and bidding\nin the subsequent ranking stage. The proposed method has been deployed in the\nKuaishou search advertising system for over a year, contributing to a 6.1%\nreduction in the proportion of irrelevant ads and a 1.4% increase in ad\nrevenue.\n","authors":["Guobing Gan","Kaiming Gao","Li Wang","Shen Jiang","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05822v1.pdf","comment":"Accepted by WWW 2025 (Industry Track)"},{"id":"http://arxiv.org/abs/2502.05803v1","updated":"2025-02-09T08:14:11Z","published":"2025-02-09T08:14:11Z","title":"FlashCheck: Exploration of Efficient Evidence Retrieval for Fast\n  Fact-Checking","summary":"  The advances in digital tools have led to the rampant spread of\nmisinformation. While fact-checking aims to combat this, manual fact-checking\nis cumbersome and not scalable. It is essential for automated fact-checking to\nbe efficient for aiding in combating misinformation in real-time and at the\nsource. Fact-checking pipelines primarily comprise a knowledge retrieval\ncomponent which extracts relevant knowledge to fact-check a claim from large\nknowledge sources like Wikipedia and a verification component. The existing\nworks primarily focus on the fact-verification part rather than evidence\nretrieval from large data collections, which often face scalability issues for\npractical applications such as live fact-checking. In this study, we address\nthis gap by exploring various methods for indexing a succinct set of factual\nstatements from large collections like Wikipedia to enhance the retrieval phase\nof the fact-checking pipeline. We also explore the impact of vector\nquantization to further improve the efficiency of pipelines that employ dense\nretrieval approaches for first-stage retrieval. We study the efficiency and\neffectiveness of the approaches on fact-checking datasets such as HoVer and\nWiCE, leveraging Wikipedia as the knowledge source. We also evaluate the\nreal-world utility of the efficient retrieval approaches by fact-checking 2024\npresidential debate and also open source the collection of claims with\ncorresponding labels identified in the debate. Through a combination of indexed\nfacts together with Dense retrieval and Index compression, we achieve up to a\n10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the\nclassical fact-checking pipelines over large collections.\n","authors":["Kevin Nanekhan","Venktesh V","Erik Martin","Henrik Vatndal","Vinay Setty","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2502.05803v1.pdf","comment":"Accepted to ECIR 2024, 15 pages"},{"id":"http://arxiv.org/abs/2410.20142v2","updated":"2025-02-09T07:58:23Z","published":"2024-10-26T10:43:39Z","title":"Mask-based Membership Inference Attacks for Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) has been an effective approach to\nmitigate hallucinations in large language models (LLMs) by incorporating\nup-to-date and domain-specific knowledge. Recently, there has been a trend of\nstoring up-to-date or copyrighted data in RAG knowledge databases instead of\nusing it for LLM training. This practice has raised concerns about Membership\nInference Attacks (MIAs), which aim to detect if a specific target document is\nstored in the RAG system's knowledge database so as to protect the rights of\ndata producers. While research has focused on enhancing the trustworthiness of\nRAG systems, existing MIAs for RAG systems remain largely insufficient.\nPrevious work either relies solely on the RAG system's judgment or is easily\ninfluenced by other documents or the LLM's internal knowledge, which is\nunreliable and lacks explainability. To address these limitations, we propose a\nMask-Based Membership Inference Attacks (MBA) framework. Our framework first\nemploys a masking algorithm that effectively masks a certain number of words in\nthe target document. The masked text is then used to prompt the RAG system, and\nthe RAG system is required to predict the mask values. If the target document\nappears in the knowledge database, the masked text will retrieve the complete\ntarget document as context, allowing for accurate mask prediction. Finally, we\nadopt a simple yet effective threshold-based method to infer the membership of\ntarget document by analyzing the accuracy of mask prediction. Our mask-based\napproach is more document-specific, making the RAG system's generation less\nsusceptible to distractions from other documents or the LLM's internal\nknowledge. Extensive experiments demonstrate the effectiveness of our approach\ncompared to existing baseline models.\n","authors":["Mingrui Liu","Sixiao Zhang","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2410.20142v2.pdf","comment":"This paper is accepted by conference WWW 2025"},{"id":"http://arxiv.org/abs/2408.13986v2","updated":"2025-02-09T06:16:18Z","published":"2024-08-26T02:36:55Z","title":"AgentMove: A Large Language Model based Agentic Framework for Zero-shot\n  Next Location Prediction","summary":"  Next location prediction plays a crucial role in various real-world\napplications. Recently, due to the limitation of existing deep learning\nmethods, attempts have been made to apply large language models (LLMs) to\nzero-shot next location prediction task. However, they directly generate the\nfinal output using LLMs without systematic design, which limits the potential\nof LLMs to uncover complex mobility patterns and underestimates their extensive\nreserve of global geospatial knowledge. In this paper, we introduce AgentMove,\na systematic agentic prediction framework to achieve generalized next location\nprediction. In AgentMove, we first decompose the mobility prediction task and\ndesign specific modules to complete them, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments utilizing mobility data from two distinct sources reveal\nthat AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of\n12 metrics and it shows robust predictions with various LLMs as base and also\nless geographical bias across cities. Our codes are available via\nhttps://github.com/tsinghua-fib-lab/AgentMove.\n","authors":["Jie Feng","Yuwei Du","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2408.13986v2.pdf","comment":"Accepted by NAACL 2025 as main conference paper,\n  https://github.com/tsinghua-fib-lab/AgentMove"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.06020v1","updated":"2025-02-09T20:26:30Z","published":"2025-02-09T20:26:30Z","title":"Temporal Working Memory: Query-Guided Segment Refinement for Enhanced\n  Multimodal Understanding","summary":"  Multimodal foundation models (MFMs) have demonstrated significant success in\ntasks such as visual captioning, question answering, and image-text retrieval.\nHowever, these models face inherent limitations due to their finite internal\ncapacity, which restricts their ability to process extended temporal sequences,\na crucial requirement for comprehensive video and audio analysis. To overcome\nthese challenges, we introduce a specialized cognitive module, temporal working\nmemory (TWM), which aims to enhance the temporal modeling capabilities of MFMs.\nIt selectively retains task-relevant information across temporal dimensions,\nensuring that critical details are preserved throughout the processing of video\nand audio content. The TWM uses a query-guided attention approach to focus on\nthe most informative multimodal segments within temporal sequences. By\nretaining only the most relevant content, TWM optimizes the use of the model's\nlimited capacity, enhancing its temporal modeling ability. This plug-and-play\nmodule can be easily integrated into existing MFMs. With our TWM, nine\nstate-of-the-art models exhibit significant performance improvements across\ntasks such as video captioning, question answering, and video-text retrieval.\nBy enhancing temporal modeling, TWM extends the capability of MFMs to handle\ncomplex, time-sensitive data effectively. Our code is available at\nhttps://github.com/xid32/NAACL_2025_TWM.\n","authors":["Xingjian Diao","Chunhui Zhang","Weiyi Wu","Zhongyu Ouyang","Peijun Qing","Ming Cheng","Soroush Vosoughi","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2502.06020v1.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06012v1","updated":"2025-02-09T20:06:42Z","published":"2025-02-09T20:06:42Z","title":"Speaker Embedding Informed Audiovisual Active Speaker Detection for\n  Egocentric Recordings","summary":"  Audiovisual active speaker detection (ASD) addresses the task of determining\nthe speech activity of a candidate speaker given acoustic and visual data.\nTypically, systems model the temporal correspondence of audiovisual cues, such\nas the synchronisation between speech and lip movement. Recent work has\nexplored extending this paradigm by additionally leveraging speaker embeddings\nextracted from candidate speaker reference speech. This paper proposes the\nspeaker comparison auxiliary network (SCAN) which uses speaker-specific\ninformation from both reference speech and the candidate audio signal to\ndisambiguate challenging scenes when the visual signal is unresolvable.\nFurthermore, an improved method for enrolling face-speaker libraries is\ndeveloped, which implements a self-supervised approach to video-based face\nrecognition. Fitting with the recent proliferation of wearable devices, this\nwork focuses on improving speaker-embedding-informed ASD in the context of\negocentric recordings, which can be characterised by acoustic noise and highly\ndynamic scenes. SCAN is implemented with two well-established baselines, namely\nTalkNet and Light-ASD; yielding a relative improvement in mAP of 14.5% and\n10.3% on the Ego4D benchmark, respectively.\n","authors":["Jason Clarke","Yoshihiko Gotoh","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2502.06012v1.pdf","comment":"Accepted to ICASSP 2025. 5 pages, 4 figures. To appear in Proceedings\n  of IEEE International Conference on Acoustics, Speech and Signal Processing\n  (ICASSP), April 6-11, 2025, Hyderabad, India"},{"id":"http://arxiv.org/abs/2502.05922v1","updated":"2025-02-09T14:32:40Z","published":"2025-02-09T14:32:40Z","title":"A Large-scale Dataset with Behavior, Attributes, and Content of Mobile\n  Short-video Platform","summary":"  Short-video platforms show an increasing impact on people's daily lives\nnowadays, with billions of active users spending plenty of time each day. The\ninteractions between users and online platforms give rise to many scientific\nproblems across computational social science and artificial intelligence.\nHowever, despite the rapid development of short-video platforms, currently\nthere are serious shortcomings in existing relevant datasets on three aspects:\ninadequate user-video feedback, limited user attributes and lack of video\ncontent. To address these problems, we provide a large-scale dataset with rich\nuser behavior, attributes and video content from a real mobile short-video\nplatform. This dataset covers 10,000 voluntary users and 153,561 videos, and we\nconduct four-fold technical validations of the dataset. First, we verify the\nrichness of the behavior and attribute data. Second, we confirm the\nrepresenting ability of the content features. Third, we provide benchmarking\nresults on recommendation algorithms with our dataset. Finally, we explore the\nfilter bubble phenomenon on the platform using the dataset. We believe the\ndataset could support the broad research community, including but not limited\nto user modeling, social science, human behavior understanding, etc. The\ndataset and code is available at\nhttps://github.com/tsinghua-fib-lab/ShortVideo_dataset.\n","authors":["Yu Shang","Chen Gao","Nian Li","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2502.05922v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2502.06893v1","updated":"2025-02-09T12:37:48Z","published":"2025-02-09T12:37:48Z","title":"A New Hybrid Intelligent Approach for Multimodal Detection of Suspected\n  Disinformation on TikTok","summary":"  In the context of the rapid dissemination of multimedia content, identifying\ndisinformation on social media platforms such as TikTok represents a\nsignificant challenge. This study introduces a hybrid framework that combines\nthe computational power of deep learning with the interpretability of fuzzy\nlogic to detect suspected disinformation in TikTok videos. The methodology is\ncomprised of two core components: a multimodal feature analyser that extracts\nand evaluates data from text, audio, and video; and a multimodal disinformation\ndetector based on fuzzy logic. These systems operate in conjunction to evaluate\nthe suspicion of spreading disinformation, drawing on human behavioural cues\nsuch as body language, speech patterns, and text coherence. Two experiments\nwere conducted: one focusing on context-specific disinformation and the other\non the scalability of the model across broader topics. For each video\nevaluated, high-quality, comprehensive, well-structured reports are generated,\nproviding a detailed view of the disinformation behaviours.\n","authors":["Jared D. T. Guerrero-Sosa","Andres Montoro-Montarroso","Francisco P. Romero","Jesus Serrano-Guerrero","Jose A. Olivas"],"pdf_url":"https://arxiv.org/pdf/2502.06893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05863v1","updated":"2025-02-09T11:46:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v1.pdf","comment":null}]},"2025-02-08T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.05575v1","updated":"2025-02-08T14:03:43Z","published":"2025-02-08T14:03:43Z","title":"Graph-Based Vector Search: An Experimental Evaluation of the\n  State-of-the-Art","summary":"  Vector data is prevalent across business and scientific applications, and its\npopularity is growing with the proliferation of learned embeddings. Vector data\ncollections often reach billions of vectors with thousands of dimensions, thus,\nincreasing the complexity of their analysis. Vector search is the backbone of\nmany critical analytical tasks, and graph-based methods have become the best\nchoice for analytical tasks that do not require guarantees on the quality of\nthe answers. We briefly survey in-memory graph-based vector search, outline the\nchronology of the different methods and classify them according to five main\ndesign paradigms: seed selection, incremental insertion, neighborhood\npropagation, neighborhood diversification, and divide-and-conquer. We conduct\nan exhaustive experimental evaluation of twelve state-of-the-art methods on\nseven real data collections, with sizes up to 1 billion vectors. We share key\ninsights about the strengths and limitations of these methods; e.g., the best\napproaches are typically based on incremental insertion and neighborhood\ndiversification, and the choice of the base graph can hurt scalability.\nFinally, we discuss open research directions, such as the importance of\ndevising more sophisticated data-adaptive seed selection and diversification\nstrategies.\n","authors":["Ilias Azizi","Karima Echihabi","Themis Palpanas"],"pdf_url":"https://arxiv.org/pdf/2502.05575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03835v2","updated":"2025-02-08T11:13:23Z","published":"2025-01-07T14:45:30Z","title":"TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification","summary":"  Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendations, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity to the item embedding. It leverages contrastive training with\ntaxonomy-aware hard negative sampling and employs adaptive inference with\ndynamic thresholds. TACLR offers three key advantages: (1) it effectively\nhandles implicit and OOD values while producing normalized outputs; (2) it\nscales to thousands of categories, tens of thousands of attributes, and\nmillions of values; and (3) it supports efficient inference for high-load\nindustrial scenarios. Extensive experiments on proprietary and public datasets\nvalidate the effectiveness and efficiency of TACLR. Moreover, it has been\nsuccessfully deployed in a real-world e-commerce platform, processing millions\nof product listings daily while supporting dynamic, large-scale attribute\ntaxonomies.\n","authors":["Yindu Su","Huike Zou","Lin Sun","Ting Zhang","Haiyang Yang","Liyu Chen","David Lo","Qingheng Zhang","Shuguang Han","Jufeng Chen"],"pdf_url":"https://arxiv.org/pdf/2501.03835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17245v2","updated":"2025-02-08T05:54:48Z","published":"2024-12-23T03:37:58Z","title":"GraphHash: Graph Clustering Enables Parameter Efficiency in Recommender\n  Systems","summary":"  Deep recommender systems rely heavily on large embedding tables to handle\nhigh-cardinality categorical features such as user/item identifiers, and face\nsignificant memory constraints at scale. To tackle this challenge, hashing\ntechniques are often employed to map multiple entities to the same embedding\nand thus reduce the size of the embedding tables. Concurrently, graph-based\ncollaborative signals have emerged as powerful tools in recommender systems,\nyet their potential for optimizing embedding table reduction remains\nunexplored. This paper introduces GraphHash, the first graph-based approach\nthat leverages modularity-based bipartite graph clustering on user-item\ninteraction graphs to reduce embedding table sizes. We demonstrate that the\nmodularity objective has a theoretical connection to message-passing, which\nprovides a foundation for our method. By employing fast clustering algorithms,\nGraphHash serves as a computationally efficient proxy for message-passing\nduring preprocessing and a plug-and-play graph-based alternative to traditional\nID hashing. Extensive experiments show that GraphHash substantially outperforms\ndiverse hashing baselines on both retrieval and click-through-rate prediction\ntasks. In particular, GraphHash achieves on average a 101.52% improvement in\nrecall when reducing the embedding table size by more than 75%, highlighting\nthe value of graph-based collaborative information for model reduction. Our\ncode is available at https://github.com/snap-research/GraphHash.\n","authors":["Xinyi Wu","Donald Loveland","Runjin Chen","Yozen Liu","Xin Chen","Leonardo Neves","Ali Jadbabaie","Clark Mingxuan Ju","Neil Shah","Tong Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.17245v2.pdf","comment":"ACM Web Conference (WWW) 2025, Oral"},{"id":"http://arxiv.org/abs/2411.13045v2","updated":"2025-02-08T02:56:02Z","published":"2024-11-20T05:30:15Z","title":"Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning","summary":"  Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.\n","authors":["Gang Zhao","Ximing Zhang","Chenji Lu","Hui Zhao","Tianshu Wu","Pengjie Wang","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.13045v2.pdf","comment":"Accepted by WWW 2025 oral"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.05695v1","updated":"2025-02-08T21:14:28Z","published":"2025-02-08T21:14:28Z","title":"Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks","summary":"  This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.\n","authors":["Zijiang Yan","Jianhua Pei","Hongda Wu","Hina Tabassum","Ping Wang"],"pdf_url":"https://arxiv.org/pdf/2502.05695v1.pdf","comment":"Submission for possible publication"},{"id":"http://arxiv.org/abs/2502.03897v2","updated":"2025-02-08T09:37:13Z","published":"2025-02-06T09:18:30Z","title":"UniForm: A Unified Diffusion Transformer for Audio-Video Generation","summary":"  As a natural multimodal content, audible video delivers an immersive sensory\nexperience. Consequently, audio-video generation systems have substantial\npotential. However, existing diffusion-based studies mainly employ relatively\nindependent modules for generating each modality, which lack exploration of\nshared-weight generative modules. This approach may under-use the intrinsic\ncorrelations between audio and visual modalities, potentially resulting in\nsub-optimal generation quality. To address this, we propose UniForm, a unified\ndiffusion transformer designed to enhance cross-modal consistency. By\nconcatenating auditory and visual information, UniForm learns to generate audio\nand video simultaneously within a unified latent space, facilitating the\ncreation of high-quality and well-aligned audio-visual pairs. Extensive\nexperiments demonstrate the superior performance of our method in joint\naudio-video generation, audio-guided video generation, and video-guided audio\ngeneration tasks. Our demos are available at https://uniform-t2av.github.io/.\n","authors":["Lei Zhao","Linfeng Feng","Dongxu Ge","Fangqiu Yi","Chi Zhang","Xiao-Lei Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.03897v2.pdf","comment":"Our demos are available at https://uniform-t2av.github.io/"},{"id":"http://arxiv.org/abs/2403.08505v5","updated":"2025-02-08T09:33:17Z","published":"2024-03-13T13:12:57Z","title":"CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code\nis available at https://github.com/Xinjie-Q/CAMSIC.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v5.pdf","comment":"Accepted by AAAI 2025"}]},"2025-02-07T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.05364v1","updated":"2025-02-07T22:31:38Z","published":"2025-02-07T22:31:38Z","title":"Hypencoder: Hypernetworks for Information Retrieval","summary":"  The vast majority of retrieval models depend on vector inner products to\nproduce a relevance score between a query and a document. This naturally limits\nthe expressiveness of the relevance score that can be employed. We propose a\nnew paradigm, instead of producing a vector to represent the query we produce a\nsmall neural network which acts as a learned relevance function. This small\nneural network takes in a representation of the document, in this paper we use\na single vector, and produces a scalar relevance score. To produce the little\nneural network we use a hypernetwork, a network that produce the weights of\nother networks, as our query encoder or as we call it a Hypencoder. Experiments\non in-domain search tasks show that Hypencoder is able to significantly\noutperform strong dense retrieval models and has higher metrics then reranking\nmodels and models an order of magnitude larger. Hypencoder is also shown to\ngeneralize well to out-of-domain search tasks. To assess the extent of\nHypencoder's capabilities, we evaluate on a set of hard retrieval tasks\nincluding tip-of-the-tongue retrieval and instruction-following retrieval tasks\nand find that the performance gap widens substantially compared to standard\nretrieval tasks. Furthermore, to demonstrate the practicality of our method we\nimplement an approximate search algorithm and show that our model is able to\nsearch 8.8M documents in under 60ms.\n","authors":["Julian Killingback","Hansi Zeng","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2502.05364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16450v3","updated":"2025-02-07T22:11:01Z","published":"2025-01-27T19:14:52Z","title":"360Brew: A Decoder-only Foundation Model for Personalized Ranking and\n  Recommendation","summary":"  Ranking and recommendation systems are the foundation for numerous online\nexperiences, ranging from search results to personalized content delivery.\nThese systems have evolved into complex, multilayered architectures that\nleverage vast datasets and often incorporate thousands of predictive models.\nThe maintenance and enhancement of these models is a labor intensive process\nthat requires extensive feature engineering. This approach not only exacerbates\ntechnical debt but also hampers innovation in extending these systems to\nemerging problem domains. In this report, we present our research to address\nthese challenges by utilizing a large foundation model with a textual interface\nfor ranking and recommendation tasks. We illustrate several key advantages of\nour approach: (1) a single model can manage multiple predictive tasks involved\nin ranking and recommendation, (2) decoder models with textual interface due to\ntheir comprehension of reasoning capabilities, can generalize to new\nrecommendation surfaces and out-of-domain problems, and (3) by employing\nnatural language interfaces for task definitions and verbalizing member\nbehaviors and their social connections, we eliminate the need for feature\nengineering and the maintenance of complex directed acyclic graphs of model\ndependencies. We introduce our research pre-production model, 360Brew V1.0, a\n150B parameter, decoder-only model that has been trained and fine-tuned on\nLinkedIn's data and tasks. This model is capable of solving over 30 predictive\ntasks across various segments of the LinkedIn platform, achieving performance\nlevels comparable to or exceeding those of current production systems based on\noffline metrics, without task-specific fine-tuning. Notably, each of these\ntasks is conventionally addressed by dedicated models that have been developed\nand maintained over multiple years by teams of a similar or larger size than\nour own.\n","authors":["Hamed Firooz","Maziar Sanjabi","Adrian Englhardt","Aman Gupta","Ben Levine","Dre Olgiati","Gungor Polatkan","Iuliia Melnychuk","Karthik Ramgopal","Kirill Talanine","Kutta Srinivasan","Luke Simon","Natesh Sivasubramoniapillai","Necip Fazil Ayan","Qingquan Song","Samira Sriram","Souvik Ghosh","Tao Song","Tejas Dharamsi","Vignesh Kothapalli","Xiaoling Zhai","Ya Xu","Yu Wang","Yun Dai"],"pdf_url":"https://arxiv.org/pdf/2501.16450v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05333v1","updated":"2025-02-07T21:14:21Z","published":"2025-02-07T21:14:21Z","title":"A Tutorial On Intersectionality in Fair Rankings","summary":"  We address the critical issue of biased algorithms and unfair rankings, which\nhave permeated various sectors, including search engines, recommendation\nsystems, and workforce management. These biases can lead to discriminatory\noutcomes in a data-driven world, especially against marginalized and\nunderrepresented groups. Efforts towards responsible data science and\nresponsible artificial intelligence aim to mitigate these biases and promote\nfairness, diversity, and transparency. However, most fairness-aware ranking\nmethods singularly focus on protected attributes such as race, gender, or\nsocio-economic status, neglecting the intersectionality of these attributes,\ni.e., the interplay between multiple social identities. Understanding\nintersectionality is crucial to ensure that existing inequalities are not\npreserved by fair rankings. We offer a description of the main ways to\nincorporate intersectionality in fair ranking systems through practical\nexamples and provide a comparative overview of existing literature and a\nsynoptic table summarizing the various methodologies. Our analysis highlights\nthe need for intersectionality to attain fairness, while also emphasizing that\nfairness, alone, does not necessarily imply intersectionality.\n","authors":["Chiara Criscuolo","Davide Martinenghi","Giuseppe Piccirillo"],"pdf_url":"https://arxiv.org/pdf/2502.05333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07926v3","updated":"2025-02-07T17:34:37Z","published":"2024-02-05T18:16:04Z","title":"From Data Creator to Data Reuser: Distance Matters","summary":"  Sharing research data is necessary, but not sufficient, for data reuse. Open\nscience policies focus more heavily on data sharing than on reuse, yet both are\ncomplex, labor-intensive, expensive, and require infrastructure investments by\nmultiple stakeholders. The value of data reuse lies in relationships between\ncreators and reusers. By addressing knowledge exchange, rather than mere\ntransactions between stakeholders, investments in data management and knowledge\ninfrastructures can be made more wisely. Drawing upon empirical studies of data\nsharing and reuse, we develop the metaphor of distance between data creator and\ndata reuser, identifying six dimensions of distance that influence the ability\nto transfer knowledge effectively: domain, methods, collaboration, curation,\npurposes, and time and temporality. We explore how social and socio-technical\naspects of these dimensions may decrease -- or increase -- distances to be\ntraversed between creators and reusers. Our theoretical framing of the distance\nbetween data creators and prospective reusers leads to recommendations to four\ncategories of stakeholders on how to make data sharing and reuse more\neffective: data creators, data reusers, data archivists, and funding agencies.\n'It takes a village' to share research data -- and a village to reuse data. Our\naim is to provoke new research questions, new research, and new investments in\neffective and efficient circulation of research data; and to identify criteria\nfor investments at each stage of data and research life cycles.\n","authors":["Christine L. Borgman","Paul T. Groth"],"pdf_url":"https://arxiv.org/pdf/2402.07926v3.pdf","comment":"41 pages, single-spaced, consisting of Title information, Abstract,\n  26 page narrative, 1 box, 1 figure, 1 table, 16 pages references. Original\n  work"},{"id":"http://arxiv.org/abs/2407.15831v2","updated":"2025-02-07T15:17:18Z","published":"2024-07-22T17:50:31Z","title":"NV-Retriever: Improving text embedding models with effective\n  hard-negative mining","summary":"  Text embedding models have been popular for information retrieval\napplications such as semantic search and Question-Answering systems based on\nRetrieval-Augmented Generation (RAG). Those models are typically Transformer\nmodels that are fine-tuned with contrastive learning objectives. One of the\nchallenging aspects of fine-tuning embedding models is the selection of high\nquality hard-negative passages for contrastive learning. In this paper we\nintroduce a family of positive-aware mining methods that use the positive\nrelevance score as an anchor for effective false negative removal, leading to\nfaster training and more accurate retrieval models. We provide an ablation\nstudy on hard-negative mining methods over their configurations, exploring\ndifferent teacher and base models. We further demonstrate the efficacy of our\nproposed mining methods at scale with the NV-Retriever-v1 model, which scores\n60.9 on MTEB Retrieval (BEIR) benchmark and placed 1st when it was published to\nthe MTEB Retrieval on July, 2024.\n","authors":["Gabriel de Souza P. Moreira","Radek Osmulski","Mengyao Xu","Ronay Ak","Benedikt Schifferer","Even Oldridge"],"pdf_url":"https://arxiv.org/pdf/2407.15831v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04266v2","updated":"2025-02-07T14:30:29Z","published":"2025-02-06T18:05:30Z","title":"Digital Gatekeeping: An Audit of Search Engine Results shows tailoring\n  of queries on the Israel-Palestine Conflict","summary":"  Search engines, often viewed as reliable gateways to information, tailor\nsearch results using customization algorithms based on user preferences,\nlocation, and more. While this can be useful for routine queries, it raises\nconcerns when the topics are sensitive or contentious, possibly limiting\nexposure to diverse viewpoints and increasing polarization.\n  To examine the extent of this tailoring, we focused on the Israel-Palestine\nconflict and developed a privacy-protecting tool to audit the behavior of three\nsearch engines: DuckDuckGo, Google and Yahoo. Our study focused on two main\nquestions: (1) How do search results for the same query about the conflict vary\namong different users? and (2) Are these results influenced by the user's\nlocation and browsing history?\n  Our findings revealed significant customization based on location and\nbrowsing preferences, unlike previous studies that found only mild\npersonalization for general topics. Moreover, queries related to the conflict\nwere more customized than unrelated queries, and the results were not neutral\nconcerning the conflict's portrayal.\n","authors":["Íris Damião","José M. Reis","Paulo Almeida","Nuno Santos","Joana Gonçalves-Sá"],"pdf_url":"https://arxiv.org/pdf/2502.04266v2.pdf","comment":"10 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.17614v2","updated":"2025-02-07T12:13:31Z","published":"2024-10-23T07:11:48Z","title":"Extending and Applying Automated HERMES Software Publication Workflows","summary":"  Research software is an important output of research and must be published\naccording to the FAIR Principles for Research Software. This can be achieved by\npublishing software with metadata under a persistent identifier. HERMES is a\ntool that leverages continuous integration to automate the publication of\nsoftware with rich metadata. In this work, we describe the HERMES workflow\nitself, and how to extend it to meet the needs of specific research software\nmetadata or infrastructure. We introduce the HERMES plugin architecture and\nprovide the example of creating a new HERMES plugin that harvests metadata from\na metadata source in source code repositories. We show how to use HERMES as an\nend user, both via the command line interface, and as a step in a continuous\nintegration pipeline. Finally, we report three informal case studies whose\nresults provide a preliminary evaluation of the feasibility and applicability\nof HERMES workflows, and the extensibility of the hermes software package.\n","authors":["Sophie Kernchen","Michael Meinel","Stephan Druskat","Michael Fritzsche","David Pape","Oliver Bertuch"],"pdf_url":"https://arxiv.org/pdf/2410.17614v2.pdf","comment":"18 pages, 3 figures, 2 tables, 1 code listing, revision as accepted\n  to a special issue of Electronic Communications of the EASST collecting\n  submissions of deRSE24, Conference for Research Software Engineers"},{"id":"http://arxiv.org/abs/2410.18140v2","updated":"2025-02-07T12:11:26Z","published":"2024-10-22T11:20:47Z","title":"Tethering Broken Themes: Aligning Neural Topic Models with Labels and\n  Authors","summary":"  Topic models are a popular approach for extracting semantic information from\nlarge document collections. However, recent studies suggest that the topics\ngenerated by these models often do not align well with human intentions.\nAlthough metadata such as labels and authorship information are available, it\nhas not yet been effectively incorporated into neural topic models. To address\nthis gap, we introduce FANToM, a novel method to align neural topic models with\nboth labels and authorship information. FANToM allows for the inclusion of this\nmetadata when available, producing interpretable topics and author\ndistributions for each topic. Our approach demonstrates greater expressiveness\nthan conventional topic models by learning the alignment between labels,\ntopics, and authors. Experimental results show that FANToM improves existing\nmodels in terms of both topic quality and alignment. Additionally, it\nidentifies author interests and similarities.\n","authors":["Mayank Nagda","Phil Ostheimer","Sophie Fellenz"],"pdf_url":"https://arxiv.org/pdf/2410.18140v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12229v2","updated":"2025-02-07T09:08:17Z","published":"2024-10-16T04:44:34Z","title":"Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems","summary":"  In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive world knowledge and remarkable reasoning capabilities of LLMs enable\nour method to supplement missing facts in KGs. Additionally, their powerful\ntext understanding abilities allow for better utilization of semantic\ninformation. Specifically, CoLaKG extracts useful information from the KG at\nboth local and global levels. By employing item-centered subgraph extraction\nand prompt engineering, it accurately captures the local KG. Subsequently,\nthrough retrieval-based neighbor enhancement, it supplements the current item\nby capturing related items from the entire KG, thereby effectively utilizing\nglobal information. The local and global information extracted by the LLM are\neffectively integrated into the recommendation model through a representation\nfusion module and a retrieval-augmented representation learning module,\nrespectively, thereby improving recommendation performance. Extensive\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod.\n","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Fuyuan Lyu","Dugang Liu","Xiuqiang He","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04751v1","updated":"2025-02-07T08:36:39Z","published":"2025-02-07T08:36:39Z","title":"Holistically Guided Monte Carlo Tree Search for Intricate Information\n  Seeking","summary":"  In the era of vast digital information, the sheer volume and heterogeneity of\navailable information present significant challenges for intricate information\nseeking. Users frequently face multistep web search tasks that involve\nnavigating vast and varied data sources. This complexity demands every step\nremains comprehensive, accurate, and relevant. However, traditional search\nmethods often struggle to balance the need for localized precision with the\nbroader context required for holistic understanding, leaving critical facets of\nintricate queries underexplored. In this paper, we introduce an LLM-based\nsearch assistant that adopts a new information seeking paradigm with\nholistically guided Monte Carlo tree search (HG-MCTS). We reformulate the task\nas a progressive information collection process with a knowledge memory and\nunite an adaptive checklist with multi-perspective reward modeling in MCTS. The\nadaptive checklist provides explicit sub-goals to guide the MCTS process toward\ncomprehensive coverage of complex user queries. Simultaneously, our\nmulti-perspective reward modeling offers both exploration and retrieval\nrewards, along with progress feedback that tracks completed and remaining\nsub-goals, refining the checklist as the tree search progresses. By striking a\nbalance between localized tree expansion and global guidance, HG-MCTS reduces\nredundancy in search paths and ensures that all crucial aspects of an intricate\nquery are properly addressed. Extensive experiments on real-world intricate\ninformation seeking tasks demonstrate that HG-MCTS acquires thorough knowledge\ncollections and delivers more accurate final responses compared with existing\nbaselines.\n","authors":["Ruiyang Ren","Yuhao Wang","Junyi Li","Jinhao Jiang","Wayne Xin Zhao","Wenjie Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.04751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02959v2","updated":"2025-02-07T08:32:24Z","published":"2024-11-05T09:58:36Z","title":"HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems","summary":"  Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial RAG\nsystems have used Web search engines as their major retrieval systems.\nTypically, such RAG systems retrieve search results, download HTML sources of\nthe results, and then extract plain texts from the HTML sources. Plain text\ndocuments or chunks are fed into the LLMs to augment the generation. However,\nmuch of the structural and semantic information inherent in HTML, such as\nheadings and table structures, is lost during this plain-text-based RAG\nprocess. To alleviate this problem, we propose HtmlRAG, which uses HTML instead\nof plain text as the format of retrieved knowledge in RAG. We believe HTML is\nbetter than plain text in modeling knowledge in external documents, and most\nLLMs possess robust capacities to understand HTML. However, utilizing HTML\npresents new challenges. HTML contains additional content such as tags,\nJavaScript, and CSS specifications, which bring extra input tokens and noise to\nthe RAG system. To address this issue, we propose HTML cleaning, compression,\nand a two-step block-tree-based pruning strategy, to shorten the HTML while\nminimizing the loss of information. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems.\n","authors":["Jiejun Tan","Zhicheng Dou","Wen Wang","Mang Wang","Weipeng Chen","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2411.02959v2.pdf","comment":"Accepted by WWW 2025 main conference. Repo:\n  https://github.com/plageon/HtmlRAG"},{"id":"http://arxiv.org/abs/2502.04666v1","updated":"2025-02-07T05:19:13Z","published":"2025-02-07T05:19:13Z","title":"Enhancing Health Information Retrieval with RAG by Prioritizing Topical\n  Relevance and Factual Accuracy","summary":"  The exponential surge in online health information, coupled with its\nincreasing use by non-experts, highlights the pressing need for advanced Health\nInformation Retrieval models that consider not only topical relevance but also\nthe factual accuracy of the retrieved information, given the potential risks\nassociated with health misinformation. To this aim, this paper introduces a\nsolution driven by Retrieval-Augmented Generation (RAG), which leverages the\ncapabilities of generative Large Language Models (LLMs) to enhance the\nretrieval of health-related documents grounded in scientific evidence. In\nparticular, we propose a three-stage model: in the first stage, the user's\nquery is employed to retrieve topically relevant passages with associated\nreferences from a knowledge base constituted by scientific literature. In the\nsecond stage, these passages, alongside the initial query, are processed by\nLLMs to generate a contextually relevant rich text (GenText). In the last\nstage, the documents to be retrieved are evaluated and ranked both from the\npoint of view of topical relevance and factual accuracy by means of their\ncomparison with GenText, either through stance detection or semantic\nsimilarity. In addition to calculating factual accuracy, GenText can offer a\nlayer of explainability for it, aiding users in understanding the reasoning\nbehind the retrieval. Experimental evaluation of our model on benchmark\ndatasets and against baseline models demonstrates its effectiveness in\nenhancing the retrieval of both topically relevant and factually accurate\nhealth information, thus presenting a significant step forward in the health\nmisinformation mitigation problem.\n","authors":["Rishabh Uapadhyay","Marco Viviani"],"pdf_url":"https://arxiv.org/pdf/2502.04666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05233v1","updated":"2025-02-07T04:24:07Z","published":"2025-02-07T04:24:07Z","title":"Efficient Knowledge Feeding to Language Models: A Novel Integrated\n  Encoder-Decoder Architecture","summary":"  This paper introduces a novel approach to efficiently feeding knowledge to\nlanguage models (LLMs) during prediction by integrating retrieval and\ngeneration processes within a unified framework. While the Retrieval-Augmented\nGeneration (RAG) model addresses gaps in LLMs' training data and knowledge\nlimits, it is hindered by token limit restrictions and dependency on the\nretrieval system's accuracy. Our proposed architecture incorporates in-context\nvectors (ICV) to overcome these challenges. ICV recasts in-context learning by\nusing latent embeddings of LLMs to create a vector that captures essential task\ninformation. This vector is then used to shift the latent states of the LLM,\nenhancing the generation process without adding demonstration examples to the\nprompt. ICV directly integrates information into the model, enabling it to\nprocess this information more effectively. Our extensive experimental\nevaluation demonstrates that ICV outperforms standard in-context learning and\nfine-tuning across question-answering, information retrieval, and other tasks.\nThis approach mitigates the limitations of current RAG models and offers a more\nrobust solution for handling extensive and diverse datasets. Despite leveraging\na fraction of the parameters, our ICV-enhanced model achieves competitive\nperformance against models like LLaMA-3, Gemma, and Phi-3, significantly\nreducing computational costs and memory requirements. ICV reduces prompt\nlength, is easy to control, surpasses token limitations, and is computationally\nefficient compared to fine-tuning.\n","authors":["S Santosh Kumar","Rishi Gottimukkala","Supriya Devidutta","Karthikeyan S"],"pdf_url":"https://arxiv.org/pdf/2502.05233v1.pdf","comment":"Submitted to ACM TIST journal: under revision stage, 8 pages, 2\n  figures"},{"id":"http://arxiv.org/abs/2502.04645v1","updated":"2025-02-07T04:08:57Z","published":"2025-02-07T04:08:57Z","title":"Cross-Encoder Rediscovers a Semantic Variant of BM25","summary":"  Neural Ranking Models (NRMs) have rapidly advanced state-of-the-art\nperformance on information retrieval tasks. In this work, we investigate a\nCross-Encoder variant of MiniLM to determine which relevance features it\ncomputes and where they are stored. We find that it employs a semantic variant\nof the traditional BM25 in an interpretable manner, featuring localized\ncomponents: (1) Transformer attention heads that compute soft term frequency\nwhile controlling for term saturation and document length effects, and (2) a\nlow-rank component of its embedding matrix that encodes inverse document\nfrequency information for the vocabulary. This suggests that the Cross-Encoder\nuses the same fundamental mechanisms as BM25, but further leverages their\ncapacity to capture semantics for improved retrieval performance. The granular\nunderstanding lays the groundwork for model editing to enhance model\ntransparency, addressing safety concerns, and improving scalability in training\nand real-world applications.\n","authors":["Meng Lu","Catherine Chen","Carsten Eickhoff"],"pdf_url":"https://arxiv.org/pdf/2502.04645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18216v2","updated":"2025-02-07T03:34:08Z","published":"2025-01-30T09:17:04Z","title":"Behavior Modeling Space Reconstruction for E-Commerce Search","summary":"  Delivering superior search services is crucial for enhancing customer\nexperience and driving revenue growth. Conventionally, search systems model\nuser behaviors by combining user preference and query item relevance\nstatically, often through a fixed logical 'and' relationship. This paper\nreexamines existing approaches through a unified lens using both causal graphs\nand Venn diagrams, uncovering two prevalent yet significant issues: entangled\npreference and relevance effects, and a collapsed modeling space. To surmount\nthese challenges, our research introduces a novel framework, DRP, which\nenhances search accuracy through two components to reconstruct the behavior\nmodeling space. Specifically, we implement preference editing to proactively\nremove the relevance effect from preference predictions, yielding untainted\nuser preferences. Additionally, we employ adaptive fusion, which dynamically\nadjusts fusion criteria to align with the varying patterns of relevance and\npreference, facilitating more nuanced and tailored behavior predictions within\nthe reconstructed modeling space. Empirical validation on two public datasets\nand a proprietary search dataset underscores the superiority of our proposed\nmethodology, demonstrating marked improvements in performance over existing\napproaches.\n","authors":["Yejing Wang","Chi Zhang","Xiangyu Zhao","Qidong Liu","Maolin Wang","Xuewei Tao","Zitao Liu","Xing Shi","Xudong Yang","Ling Zhong","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2501.18216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08521v2","updated":"2025-02-07T00:09:43Z","published":"2024-08-16T04:32:10Z","title":"MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement\n  Framework for Multimodal Question Answering","summary":"  Recent advancements in retrieval-augmented generation (RAG) have demonstrated\nimpressive performance in the question-answering (QA) task. However, most\nprevious works predominantly focus on text-based answers. While some studies\naddress multimodal data, they still fall short in generating comprehensive\nmultimodal answers, particularly for explaining concepts or providing\nstep-by-step tutorials on how to accomplish specific goals. This capability is\nespecially valuable for applications such as enterprise chatbots and settings\nsuch as customer service and educational systems, where the answers are sourced\nfrom multimodal data. In this paper, we introduce a simple and effective\nframework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR\nenhances text-based answers by retrieving relevant multimodal data and refining\nthe responses to create coherent multimodal answers. This framework can be\neasily extended to support multimodal answers in enterprise chatbots with\nminimal modifications. Human evaluation results indicate that multimodal\nanswers generated by MuRAR are more useful and readable compared to plain text\nanswers.\n","authors":["Zhengyuan Zhu","Daniel Lee","Hong Zhang","Sai Sree Harsha","Loic Feujio","Akash Maharaj","Yunyao Li"],"pdf_url":"https://arxiv.org/pdf/2408.08521v2.pdf","comment":"Accepted at COLING 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.03238v2","updated":"2025-02-07T18:37:47Z","published":"2025-02-05T14:57:23Z","title":"Long-tailed Medical Diagnosis with Relation-aware Representation\n  Learning and Iterative Classifier Calibration","summary":"  Recently computer-aided diagnosis has demonstrated promising performance,\neffectively alleviating the workload of clinicians. However, the inherent\nsample imbalance among different diseases leads algorithms biased to the\nmajority categories, leading to poor performance for rare categories. Existing\nworks formulated this challenge as a long-tailed problem and attempted to\ntackle it by decoupling the feature representation and classification. Yet, due\nto the imbalanced distribution and limited samples from tail classes, these\nworks are prone to biased representation learning and insufficient classifier\ncalibration. To tackle these problems, we propose a new Long-tailed Medical\nDiagnosis (LMD) framework for balanced medical image classification on\nlong-tailed datasets. In the initial stage, we develop a Relation-aware\nRepresentation Learning (RRL) scheme to boost the representation ability by\nencouraging the encoder to capture intrinsic semantic features through\ndifferent data augmentations. In the subsequent stage, we propose an Iterative\nClassifier Calibration (ICC) scheme to calibrate the classifier iteratively.\nThis is achieved by generating a large number of balanced virtual features and\nfine-tuning the encoder using an Expectation-Maximization manner. The proposed\nICC compensates for minority categories to facilitate unbiased classifier\noptimization while maintaining the diagnostic knowledge in majority classes.\nComprehensive experiments on three public long-tailed medical datasets\ndemonstrate that our LMD framework significantly surpasses state-of-the-art\napproaches. The source code can be accessed at\nhttps://github.com/peterlipan/LMD.\n","authors":["Li Pan","Yupei Zhang","Qiushi Yang","Tan Li","Zhen Chen"],"pdf_url":"https://arxiv.org/pdf/2502.03238v2.pdf","comment":"This work has been accepted in Computers in Biology and Medicine"},{"id":"http://arxiv.org/abs/2502.05130v1","updated":"2025-02-07T18:02:47Z","published":"2025-02-07T18:02:47Z","title":"Latent Swap Joint Diffusion for Long-Form Audio Generation","summary":"  Previous work on long-form audio generation using global-view diffusion or\niterative generation demands significant training or inference costs. While\nrecent advancements in multi-view joint diffusion for panoramic generation\nprovide an efficient option, they struggle with spectrum generation with severe\noverlap distortions and high cross-view consistency costs. We initially explore\nthis phenomenon through the connectivity inheritance of latent maps and uncover\nthat averaging operations excessively smooth the high-frequency components of\nthe latent map. To address these issues, we propose Swap Forward (SaFa), a\nframe-level latent swap framework that synchronizes multiple diffusions to\nproduce a globally coherent long audio with more spectrum details in a\nforward-only manner. At its core, the bidirectional Self-Loop Latent Swap is\napplied between adjacent views, leveraging stepwise diffusion trajectory to\nadaptively enhance high-frequency components without disrupting low-frequency\ncomponents. Furthermore, to ensure cross-view consistency, the unidirectional\nReference-Guided Latent Swap is applied between the reference and the\nnon-overlap regions of each subview during the early stages, providing\ncentralized trajectory guidance. Quantitative and qualitative experiments\ndemonstrate that SaFa significantly outperforms existing joint diffusion\nmethods and even training-based long audio generation models. Moreover, we find\nthat it also adapts well to panoramic generation, achieving comparable\nstate-of-the-art performance with greater efficiency and model\ngeneralizability. Project page is available at https://swapforward.github.io/.\n","authors":["Yusheng Dai","Chenxi Wang","Chang Li","Chen Wang","Jun Du","Kewei Li","Ruoyu Wang","Jiefeng Ma","Lei Sun","Jianqing Gao"],"pdf_url":"https://arxiv.org/pdf/2502.05130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04976v1","updated":"2025-02-07T14:50:10Z","published":"2025-02-07T14:50:10Z","title":"Towards Multimodal Empathetic Response Generation: A Rich\n  Text-Speech-Vision Avatar-based Benchmark","summary":"  Empathetic Response Generation (ERG) is one of the key tasks of the affective\ncomputing area, which aims to produce emotionally nuanced and compassionate\nresponses to user's queries. However, existing ERG research is predominantly\nconfined to the singleton text modality, limiting its effectiveness since human\nemotions are inherently conveyed through multiple modalities. To combat this,\nwe introduce an avatar-based Multimodal ERG (MERG) task, entailing rich text,\nspeech, and facial vision information. We first present a large-scale\nhigh-quality benchmark dataset, \\textbf{AvaMERG}, which extends traditional\ntext ERG by incorporating authentic human speech audio and dynamic talking-face\navatar videos, encompassing a diverse range of avatar profiles and broadly\ncovering various topics of real-world scenarios. Further, we deliberately\ntailor a system, named \\textbf{Empatheia}, for MERG. Built upon a Multimodal\nLarge Language Model (MLLM) with multimodal encoder, speech and avatar\ngenerators, Empatheia performs end-to-end MERG, with Chain-of-Empathetic\nreasoning mechanism integrated for enhanced empathy understanding and\nreasoning. Finally, we devise a list of empathetic-enhanced tuning strategies,\nstrengthening the capabilities of emotional accuracy and content,\navatar-profile consistency across modalities. Experimental results on AvaMERG\ndata demonstrate that Empatheia consistently shows superior performance than\nbaseline methods on both textual ERG and MERG. Overall, this work is expected\nto pioneer the MERG research by introducing a novel benchmark and an end-to-end\nmodel, laying a solid foundation for future advancements in multimodal\nempathetic response generation.\n","authors":["Han Zhang","Zixiang Meng","Meng Luo","Hong Han","Lizi Liao","Erik Cambria","Hao Fei"],"pdf_url":"https://arxiv.org/pdf/2502.04976v1.pdf","comment":"Accepted by TheWebConf (WWW) 2025"},{"id":"http://arxiv.org/abs/2403.05192v3","updated":"2025-02-07T14:34:28Z","published":"2024-03-08T10:14:32Z","title":"An End-to-End Pipeline Perspective on Video Streaming in Best-Effort\n  Networks: A Survey and Tutorial","summary":"  Remaining a dominant force in Internet traffic, video streaming captivates\nend users, service providers, and researchers. This paper takes a pragmatic\napproach to reviewing recent advances in the field by focusing on the prevalent\nstreaming paradigm that involves delivering long-form two-dimensional videos\nover the best-effort Internet with client-side adaptive bitrate (ABR)\nalgorithms and assistance from content delivery networks (CDNs). To enhance\naccessibility, we supplement the survey with tutorial material. Unlike existing\nsurveys that offer fragmented views, our work provides a holistic perspective\non the entire end-to-end streaming pipeline, from video capture by a\ncamera-equipped device to playback by the end user. Our novel perspective\ncovers the ingestion, processing, and distribution stages of the pipeline and\naddresses key challenges such as video compression, upload, transcoding, ABR\nalgorithms, CDN support, and quality of experience. We review over 200 papers\nand classify streaming designs by their problem-solving methodology, whether\nbased on intuition (simple heuristics), theory (formal optimization), or\nmachine learning (generalizable data patterns). The survey further refines\nthese methodology-based categories and characterizes each design by additional\ntraits such as compatible codecs and use of super resolution. We connect the\nreviewed research to real-world applications by discussing the practices of\ncommercial streaming platforms. Finally, the survey highlights prominent\ncurrent trends and outlines future directions in video streaming.\n","authors":["Leonardo Peroni","Sergey Gorinsky"],"pdf_url":"https://arxiv.org/pdf/2403.05192v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04691v1","updated":"2025-02-07T06:35:50Z","published":"2025-02-07T06:35:50Z","title":"PDStream: Slashing Long-Tail Delay in Interactive Video Streaming via\n  Pseudo-Dual Streaming","summary":"  End-to-end (E2E) delay is critical for interactive video streaming (IVS)\nexperiences, but remains unsatisfactory for its long-tail distribution caused\nby periodic large keyframes. Conventional optimization strategies, such as\njitter buffer, bitrate adaptation, and customized encoding, either sacrifice\nclarity, average delay, or compatibility. To address this issue, we propose\nPDStream, a novel pseudo-dual streaming algorithm, aimed at minimizing E2E\ndelay while maintaining video clarity. The core idea is to split the two\nfunctions, delay-sensitive playback and delay-tolerant reference, on keyframes\nthrough dual streaming. Specifically, the playback function is held by a second\nparallel stream, which comprises much smaller non-keyframes and is allocated\nmore immediate bandwidth for real-time performance. The reference function is\nensured by the first stream with keyframe preservation, allocated more\nsubsequent bandwidth to smooth out bursty traffic. Additionally, ``pseudo''\nminimizes computational and transmission overheads by restricting dual streams\nto brief activation only when keyframes appear, supported by corresponding\ndual-stream bitrate allocation and adaptation to ensure delay and clarity. We\nimplement PDStream on a WebRTC-based IVS testbed with real-world network\ntraces. Results show that PDStream significantly outperforms prior algorithms,\nreducing average E2E delay by 17.5\\% and slashing its 97th percentile by\n33.3\\%, while keeping clarity under varying bandwidth.\n","authors":["Xuedou Xiao","Yingying Zuo","Mingxuan Yan","Kezhong Liu","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.04691v1.pdf","comment":"IEEE INFOCOM 2025"}]},"2025-02-06T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.19241v3","updated":"2025-02-06T21:09:25Z","published":"2025-01-31T15:55:14Z","title":"Emancipatory Information Retrieval","summary":"  Our world today is facing a confluence of several mutually reinforcing crises\neach of which intersects with concerns of social justice and emancipation. This\npaper is a provocation for the role of computer-mediated information access in\nour emancipatory struggles. We define emancipatory information retrieval as the\nstudy and development of information access methods that challenge various\nforms of human oppression, and situates its activities within broader\ncollective emancipatory praxis. The term \"emancipatory\" here signifies the\nmoral concerns of universal humanization of all peoples and the elimination of\noppression to create the conditions under which we can collectively flourish.\nTo develop an emancipatory research agenda for information retrieval (IR), in\nthis paper we speculate about the practices that the community can adopt,\nenumerate some of the projects that the field should undertake, and discuss\nprovocations to spark new ideas and directions for research. We challenge the\nfield of IR research to embrace humanistic values and commit to universal\nemancipation and social justice as part of our research. In that process, the\ncommunity must both imagine post-oppressive worlds, and reimagine the role of\nIR in that world and in the journey that leads us there.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2501.19241v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04176v1","updated":"2025-02-06T16:07:24Z","published":"2025-02-06T16:07:24Z","title":"MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented\n  Multimodal Generation","summary":"  Recent advancements in Retrieval-Augmented Generation (RAG) have shown\nremarkable performance in enhancing response accuracy and relevance by\nintegrating external knowledge into generative models. However, existing RAG\nmethods primarily focus on providing text-only answers, even in multimodal\nretrieval-augmented generation scenarios. In this work, we introduce the\nMultimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims\nto generate answers that combine both text and images, fully leveraging the\nmultimodal data within a corpus. Despite the importance of this task, there is\na notable absence of a comprehensive benchmark to effectively evaluate MRAMG\nperformance. To bridge this gap, we introduce the MRAMG-Bench, a carefully\ncurated, human-annotated dataset comprising 4,346 documents, 14,190 images, and\n4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, and\nLifestyle. The dataset incorporates diverse difficulty levels and complex\nmulti-image scenarios, providing a robust foundation for evaluating multimodal\ngeneration tasks. To facilitate rigorous evaluation, our MRAMG-Bench\nincorporates a comprehensive suite of both statistical and LLM-based metrics,\nenabling a thorough analysis of the performance of popular generative models in\nthe MRAMG task. Besides, we propose an efficient multimodal answer generation\nframework that leverages both LLMs and MLLMs to generate multimodal responses.\nOur datasets are available at: https://huggingface.co/MRAMG.\n","authors":["Qinhan Yu","Zhiyou Xiao","Binghui Li","Zhengren Wang","Chong Chen","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.04176v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.17421v3","updated":"2025-02-06T14:41:05Z","published":"2024-03-26T06:34:23Z","title":"MA4DIV: Multi-Agent Reinforcement Learning for Search Result\n  Diversification","summary":"  Search result diversification (SRD), which aims to ensure that documents in a\nranking list cover a broad range of subtopics, is a significant and widely\nstudied problem in Information Retrieval and Web Search. Existing methods\nprimarily utilize a paradigm of \"greedy selection\", i.e., selecting one\ndocument with the highest diversity score at a time or optimize an\napproximation of the objective function. These approaches tend to be\ninefficient and are easily trapped in a suboptimal state. To address these\nchallenges, we introduce Multi-Agent reinforcement learning (MARL) for search\nresult DIVersity, which called MA4DIV. In this approach, each document is an\nagent and the search result diversification is modeled as a cooperative task\namong multiple agents. By modeling the SRD ranking problem as a cooperative\nMARL problem, this approach allows for directly optimizing the diversity\nmetrics, such as $\\alpha$-NDCG, while achieving high training efficiency. We\nconducted experiments on public TREC datasets and a larger scale dataset in the\nindustrial setting. The experiemnts show that MA4DIV achieves substantial\nimprovements in both effectiveness and efficiency than existing baselines,\nespecially on the industrial dataset. The code of MA4DIV can be seen on\nhttps://github.com/chenyiqun/MA4DIV.\n","authors":["Yiqun Chen","Jiaxin Mao","Yi Zhang","Dehong Ma","Long Xia","Jun Fan","Daiting Shi","Zhicong Cheng","Simiu Gu","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.17421v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11678v2","updated":"2025-02-06T14:40:52Z","published":"2024-06-17T15:58:22Z","title":"TourRank: Utilizing Large Language Models for Documents Ranking with a\n  Tournament-Inspired Strategy","summary":"  Large Language Models (LLMs) are increasingly employed in zero-shot documents\nranking, yielding commendable results. However, several significant challenges\nstill persist in LLMs for ranking: (1) LLMs are constrained by limited input\nlength, precluding them from processing a large number of documents\nsimultaneously; (2) The output document sequence is influenced by the input\norder of documents, resulting in inconsistent ranking outcomes; (3) Achieving a\nbalance between cost and ranking performance is challenging. To tackle these\nissues, we introduce a novel documents ranking method called TourRank, which is\ninspired by the sport tournaments, such as FIFA World Cup. Specifically, we 1)\novercome the limitation in input length and reduce the ranking latency by\nincorporating a multi-stage grouping strategy similar to the parallel group\nstage of sport tournaments; 2) improve the ranking performance and robustness\nto input orders by using a points system to ensemble multiple ranking results.\nWe test TourRank with different LLMs on the TREC DL datasets and the BEIR\nbenchmark. The experimental results demonstrate that TourRank delivers\nstate-of-the-art performance at a modest cost. The code of TourRank can be seen\non https://github.com/chenyiqun/TourRank.\n","authors":["Yiqun Chen","Qi Liu","Yi Zhang","Weiwei Sun","Xinyu Ma","Wei Yang","Daiting Shi","Jiaxin Mao","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2406.11678v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05582v2","updated":"2025-02-06T14:22:03Z","published":"2023-03-09T21:13:32Z","title":"Generalization analysis of an unfolding network for analysis-based\n  Compressed Sensing","summary":"  Unfolding networks have shown promising results in the Compressed Sensing\n(CS) field. Yet, the investigation of their generalization ability is still in\nits infancy. In this paper, we perform a generalization analysis of a\nstate-of-the-art ADMM-based unfolding network, which jointly learns a decoder\nfor CS and a sparsifying redundant analysis operator. To this end, we first\nimpose a structural constraint on the learnable sparsifier, which parametrizes\nthe network's hypothesis class. For the latter, we estimate its Rademacher\ncomplexity. With this estimate in hand, we deliver generalization error bounds\n-- which scale like the square root of the number of layers -- for the examined\nnetwork. Finally, the validity of our theory is assessed and numerical\ncomparisons to a state-of-the-art unfolding network are made, on synthetic and\nreal-world datasets. Our experimental results demonstrate that our proposed\nframework complies with our theoretical findings and outperforms the baseline,\nconsistently for all datasets.\n","authors":["Vicky Kouni","Yannis Panagakis"],"pdf_url":"https://arxiv.org/pdf/2303.05582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07770v2","updated":"2025-02-06T12:52:46Z","published":"2024-02-12T16:32:37Z","title":"Had enough of experts? Quantitative knowledge retrieval from large\n  language models","summary":"  Large language models (LLMs) have been extensively studied for their\nabilities to generate convincing natural language sequences, however their\nutility for quantitative information retrieval is less well understood. Here we\nexplore the feasibility of LLMs as a mechanism for quantitative knowledge\nretrieval to aid two data analysis tasks: elicitation of prior distributions\nfor Bayesian models and imputation of missing data. We introduce a framework\nthat leverages LLMs to enhance Bayesian workflows by eliciting expert-like\nprior knowledge and imputing missing data. Tested on diverse datasets, this\napproach can improve predictive accuracy and reduce data requirements, offering\nsignificant potential in healthcare, environmental science and engineering\napplications. We discuss the implications and challenges of treating LLMs as\n'experts'.\n","authors":["David Selby","Kai Spriestersbach","Yuichiro Iwashita","Mohammad Saad","Dennis Bappert","Archana Warrier","Sumantrak Mukherjee","Koichi Kise","Sebastian Vollmer"],"pdf_url":"https://arxiv.org/pdf/2402.07770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04413v1","updated":"2025-02-06T12:27:35Z","published":"2025-02-06T12:27:35Z","title":"MedRAG: Enhancing Retrieval-augmented Generation with Knowledge\n  Graph-Elicited Reasoning for Healthcare Copilot","summary":"  Retrieval-augmented generation (RAG) is a well-suited technique for\nretrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a\nkey module of the healthcare copilot, helping reduce misdiagnosis for\nhealthcare practitioners and patients. However, the diagnostic accuracy and\nspecificity of existing heuristic-based RAG models used in the medical domain\nare inadequate, particularly for diseases with similar manifestations. This\npaper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited\nreasoning for the medical domain that retrieves diagnosis and treatment\nrecommendations based on manifestations. MedRAG systematically constructs a\ncomprehensive four-tier hierarchical diagnostic KG encompassing critical\ndiagnostic differences of various diseases. These differences are dynamically\nintegrated with similar EHRs retrieved from an EHR database, and reasoned\nwithin a large language model. This process enables more accurate and specific\ndecision support, while also proactively providing follow-up questions to\nenhance personalized medical decision-making. MedRAG is evaluated on both a\npublic dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD)\ncollected from Tan Tock Seng Hospital, and its performance is compared against\nvarious existing RAG methods. Experimental results show that, leveraging the\ninformation integration and relational abilities of the KG, our MedRAG provides\nmore specific diagnostic insights and outperforms state-of-the-art models in\nreducing misdiagnosis rates. Our code will be available at\nhttps://github.com/SNOWTEAM2023/MedRAG\n","authors":["Xuejiao Zhao","Siyan Liu","Su-Yin Yang","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2502.04413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09243v3","updated":"2025-02-06T12:03:33Z","published":"2024-12-12T12:53:30Z","title":"SPRec: Self-Play to Debias LLM-based Recommendation","summary":"  Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current work primarily applies supervised fine-tuning\n(SFT) to adapt the model for recommendation tasks. However, SFT on positive\nexamples only limits the model's ability to align with user preference. To\naddress this, researchers recently introduced Direct Preference Optimization\n(DPO), which explicitly aligns LLMs with user preferences using offline\npreference ranking data. However, we found that DPO inherently biases the model\ntowards a few items, exacerbating the filter bubble issue and ultimately\ndegrading user experience.\n  In this paper, we propose SPRec, a novel self-play framework designed to\nmitigate over-recommendation and improve fairness without requiring additional\ndata or manual intervention. In each self-play iteration, the model undergoes\nan SFT step followed by a DPO step, treating offline interaction data as\npositive samples and the predicted outputs from the previous iteration as\nnegative samples. This effectively re-weights the DPO loss function using the\nmodel's logits, adaptively suppressing biased items. Extensive experiments on\nmultiple real-world datasets demonstrate SPRec's effectiveness in enhancing\nrecommendation accuracy and fairness. The implementation is available via\nhttps://github.com/RegionCh/SPRec\n","authors":["Chongming Gao","Ruijun Chen","Shuai Yuan","Kexin Huang","Yuanqing Yu","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2412.09243v3.pdf","comment":"Accepted by WWW 2025"},{"id":"http://arxiv.org/abs/2412.01269v4","updated":"2025-02-06T11:12:09Z","published":"2024-12-02T08:35:54Z","title":"CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search","summary":"  Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines.\n","authors":["Kaixin Wu","Yixin Ji","Zeyuan Chen","Qiang Wang","Cunxiang Wang","Hong Liu","Baijun Ji","Jia Xu","Zhongyi Liu","Jinjie Gu","Yuan Zhou","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2412.01269v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20005v2","updated":"2025-02-06T10:37:17Z","published":"2024-12-28T04:01:30Z","title":"OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction\n  System","summary":"  We introduce OneKE, a dockerized schema-guided knowledge extraction system,\nwhich can extract knowledge from the Web and raw PDF Books, and support various\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\nagents and a configure knowledge base. Different agents perform their\nrespective roles, enabling support for various extraction scenarios. The\nconfigure knowledge base facilitates schema configuration, error case debugging\nand correction, further improving the performance. Empirical evaluations on\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\nelucidate its adaptability to diverse tasks across multiple domains,\nhighlighting its potential for broad applications. We have open-sourced the\nCode at https://github.com/zjunlp/OneKE and released a Video at\nhttp://oneke.openkg.cn/demo.mp4.\n","authors":["Yujie Luo","Xiangyuan Ru","Kangwei Liu","Lin Yuan","Mengshu Sun","Ningyu Zhang","Lei Liang","Zhiqiang Zhang","Jun Zhou","Lanning Wei","Da Zheng","Haofen Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2412.20005v2.pdf","comment":"WWW 2025 Demonstration"},{"id":"http://arxiv.org/abs/2407.01449v5","updated":"2025-02-06T09:57:56Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nbut also figures, page layouts, tables, or even fonts. Since modern retrieval\nsystems mainly rely on the textual information they extract from document pages\nto index documents -often through lengthy and brittle processes-, they struggle\nto exploit key visual cues efficiently. This limits their capabilities in many\npractical document retrieval applications such as Retrieval Augmented\nGeneration (RAG). To benchmark current systems on visually rich document\nretrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,\ncomposed of various page-level retrieval tasks spanning multiple domains,\nlanguages, and practical settings. The inherent complexity and performance\nshortcomings of modern systems motivate a new concept; doing document retrieval\nby directly embedding the images of the document pages. We release ColPali, a\nVision Language Model trained to produce high-quality multi-vector embeddings\nfrom images of document pages. Combined with a late interaction matching\nmechanism, ColPali largely outperforms modern document retrieval pipelines\nwhile being drastically simpler, faster and end-to-end trainable. We release\nmodels, data, code and benchmarks under open licenses at https://hf.co/vidore.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v5.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.03891v1","updated":"2025-02-06T09:05:41Z","published":"2025-02-06T09:05:41Z","title":"Counterfactual Query Rewriting to Use Historical Relevance Feedback","summary":"  When a retrieval system receives a query it has encountered before, previous\nrelevance feedback, such as clicks or explicit judgments can help to improve\nretrieval results. However, the content of a previously relevant document may\nhave changed, or the document might not be available anymore. Despite this\nevolved corpus, we counterfactually use these previously relevant documents as\nrelevance signals. In this paper we proposed approaches to rewrite user queries\nand compare them against a system that directly uses the previous qrels for the\nranking. We expand queries with terms extracted from the previously relevant\ndocuments or derive so-called keyqueries that rank the previously relevant\ndocuments to the top of the current corpus. Our evaluation in the CLEF LongEval\nscenario shows that rewriting queries with historical relevance feedback\nimproves the retrieval effectiveness and even outperforms computationally\nexpensive transformer-based approaches.\n","authors":["Jüri Keller","Maik Fröbe","Gijs Hendriksen","Daria Alexander","Martin Potthast","Matthias Hagen","Philipp Schaer"],"pdf_url":"https://arxiv.org/pdf/2502.03891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02854v2","updated":"2025-02-06T08:29:46Z","published":"2025-02-05T03:13:25Z","title":"TD3: Tucker Decomposition Based Dataset Distillation Method for\n  Sequential Recommendation","summary":"  In the era of data-centric AI, the focus of recommender systems has shifted\nfrom model-centric innovations to data-centric approaches. The success of\nmodern AI models is built on large-scale datasets, but this also results in\nsignificant training costs. Dataset distillation has emerged as a key solution,\ncondensing large datasets to accelerate model training while preserving model\nperformance. However, condensing discrete and sequentially correlated user-item\ninteractions, particularly with extensive item sets, presents considerable\nchallenges. This paper introduces \\textbf{TD3}, a novel \\textbf{T}ucker\n\\textbf{D}ecomposition based \\textbf{D}ataset \\textbf{D}istillation method\nwithin a meta-learning framework, designed for sequential recommendation. TD3\ndistills a fully expressive \\emph{synthetic sequence summary} from original\ndata. To efficiently reduce computational complexity and extract refined latent\npatterns, Tucker decomposition decouples the summary into four factors:\n\\emph{synthetic user latent factor}, \\emph{temporal dynamics latent factor},\n\\emph{shared item latent factor}, and a \\emph{relation core} that models their\ninterconnections. Additionally, a surrogate objective in bi-level optimization\nis proposed to align feature spaces extracted from models trained on both\noriginal data and synthetic sequence summary beyond the na\\\"ive performance\nmatching approach. In the \\emph{inner-loop}, an augmentation technique allows\nthe learner to closely fit the synthetic summary, ensuring an accurate update\nof it in the \\emph{outer-loop}. To accelerate the optimization process and\naddress long dependencies, RaT-BPTT is employed for bi-level optimization.\nExperiments and analyses on multiple public datasets have confirmed the\nsuperiority and cross-architecture generalizability of the proposed designs.\nCodes are released at https://github.com/USTC-StarTeam/TD3.\n","authors":["Jiaqing Zhang","Mingjia Yin","Hao Wang","Yawen Li","Yuyang Ye","Xingyu Lou","Junping Du","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.02854v2.pdf","comment":"This work has been accepted by WWW2025"},{"id":"http://arxiv.org/abs/2502.03715v1","updated":"2025-02-06T02:06:48Z","published":"2025-02-06T02:06:48Z","title":"Boosting Knowledge Graph-based Recommendations through Confidence-Aware\n  Augmentation with Large Language Models","summary":"  Knowledge Graph-based recommendations have gained significant attention due\nto their ability to leverage rich semantic relationships. However, constructing\nand maintaining Knowledge Graphs (KGs) is resource-intensive, and the accuracy\nof KGs can suffer from noisy, outdated, or irrelevant triplets. Recent\nadvancements in Large Language Models (LLMs) offer a promising way to improve\nthe quality and relevance of KGs for recommendation tasks. Despite this,\nintegrating LLMs into KG-based systems presents challenges, such as efficiently\naugmenting KGs, addressing hallucinations, and developing effective joint\nlearning methods. In this paper, we propose the Confidence-aware KG-based\nRecommendation Framework with LLM Augmentation (CKG-LLMA), a novel framework\nthat combines KGs and LLMs for recommendation task. The framework includes: (1)\nan LLM-based subgraph augmenter for enriching KGs with high-quality\ninformation, (2) a confidence-aware message propagation mechanism to filter\nnoisy triplets, and (3) a dual-view contrastive learning method to integrate\nuser-item interactions and KG data. Additionally, we employ a confidence-aware\nexplanation generation process to guide LLMs in producing realistic\nexplanations for recommendations. Finally, extensive experiments demonstrate\nthe effectiveness of CKG-LLMA across multiple public datasets.\n","authors":["Rui Cai","Chao Wang","Qianyi Cai","Dazhong Shen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.03715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03699v1","updated":"2025-02-06T01:22:06Z","published":"2025-02-06T01:22:06Z","title":"LLM Alignment as Retriever Optimization: An Information Retrieval\n  Perspective","summary":"  Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research.\n","authors":["Bowen Jin","Jinsung Yoon","Zhen Qin","Ziqi Wang","Wei Xiong","Yu Meng","Jiawei Han","Sercan O. Arik"],"pdf_url":"https://arxiv.org/pdf/2502.03699v1.pdf","comment":"26 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.18855v2","updated":"2025-02-06T22:25:39Z","published":"2024-11-28T01:51:46Z","title":"Improving Accuracy and Generalization for Efficient Visual Tracking","summary":"  Efficient visual trackers overfit to their training distributions and lack\ngeneralization abilities, resulting in them performing well on their respective\nin-distribution (ID) test sets and not as well on out-of-distribution (OOD)\nsequences, imposing limitations to their deployment in-the-wild under\nconstrained resources. We introduce SiamABC, a highly efficient Siamese tracker\nthat significantly improves tracking performance, even on OOD sequences.\nSiamABC takes advantage of new architectural designs in the way it bridges the\ndynamic variability of the target, and of new losses for training. Also, it\ndirectly addresses OOD tracking generalization by including a fast\nbackward-free dynamic test-time adaptation method that continuously adapts the\nmodel according to the dynamic visual changes of the target. Our extensive\nexperiments suggest that SiamABC shows remarkable performance gains in OOD sets\nwhile maintaining accurate performance on the ID benchmarks. SiamABC\noutperforms MixFormerV2-S by 7.6\\% on the OOD AVisT benchmark while being 3x\nfaster (100 FPS) on a CPU. Our code and models are available at\nhttps://wvuvl.github.io/SiamABC/.\n","authors":["Ram Zaveri","Shivang Patel","Yu Gu","Gianfranco Doretto"],"pdf_url":"https://arxiv.org/pdf/2411.18855v2.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2502.04128v1","updated":"2025-02-06T15:04:00Z","published":"2025-02-06T15:04:00Z","title":"Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis","summary":"  Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.\n","authors":["Zhen Ye","Xinfa Zhu","Chi-Min Chan","Xinsheng Wang","Xu Tan","Jiahe Lei","Yi Peng","Haohe Liu","Yizhu Jin","Zheqi DAI","Hongzhan Lin","Jianyi Chen","Xingjian Du","Liumeng Xue","Yunlin Chen","Zhifei Li","Lei Xie","Qiuqiang Kong","Yike Guo","Wei Xue"],"pdf_url":"https://arxiv.org/pdf/2502.04128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04078v1","updated":"2025-02-06T13:42:07Z","published":"2025-02-06T13:42:07Z","title":"CDIO: Cross-Domain Inference Optimization with Resource Preference\n  Prediction for Edge-Cloud Collaboration","summary":"  Currently, massive video tasks are processed by edge-cloud collaboration.\nHowever, the diversity of task requirements and the dynamics of resources pose\ngreat challenges to efficient inference, resulting in many wasted resources. In\nthis paper, we present CDIO, a cross-domain inference optimization framework\ndesigned for edge-cloud collaboration. For diverse input tasks, CDIO can\npredict resource preference types by analyzing spatial complexity and\nprocessing requirements of the task. Subsequently, a cross-domain collaborative\noptimization algorithm is employed to guide resource allocation in the\nedge-cloud system. By ensuring that each task is matched with the ideal\nservers, the edge-cloud system can achieve higher efficiency inference. The\nevaluation results on public datasets demonstrate that CDIO can effectively\nmeet the accuracy and delay requirements for task processing. Compared to\nstate-of-the-art edge-cloud solutions, CDIO achieves a computing and bandwidth\nconsumption reduction of 20%-40%. And it can reduce energy consumption by more\nthan 40%.\n","authors":["Zheming Yang","Wen Ji","Qi Guo","Dieli Hu","Chang Zhao","Xiaowei Li","Xuanlei Zhao","Yi Zhao","Chaoyu Gong","Yang You"],"pdf_url":"https://arxiv.org/pdf/2502.04078v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.04400v1","updated":"2025-02-06T07:28:05Z","published":"2025-02-06T07:28:05Z","title":"Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed\n  Modalities and Heterogeneous Tasks","summary":"  Multimodal Federated Learning (MFL) enables multiple clients to\ncollaboratively train models on multimodal data while ensuring clients'\nprivacy. However, modality and task heterogeneity hinder clients from learning\na unified representation, weakening local model generalization, especially in\nMFL with mixed modalities where only some clients have multimodal data. In this\nwork, we propose an Adaptive prototype-based Multimodal Federated Learning\n(AproMFL) framework for mixed modalities and heterogeneous tasks to address the\naforementioned issues. Our AproMFL transfers knowledge through\nadaptively-constructed prototypes without a prior public dataset. Clients\nadaptively select prototype construction methods in line with tasks; server\nconverts client prototypes into unified multimodal prototypes and aggregates\nthem to form global prototypes, avoid clients keeping unified labels. We divide\nthe model into various modules and only aggregate mapping modules to reduce\ncommunication and computation overhead. To address aggregation issues in\nheterogeneity, we develop a client relationship graph-based scheme to\ndynamically adjust aggregation weights. Extensive experiments on representative\ndatasets evidence effectiveness of AproMFL.\n","authors":["Keke Gai","Mohan Wang","Jing Yu","Dongjue Wang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2502.04400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03724v1","updated":"2025-02-06T02:26:47Z","published":"2025-02-06T02:26:47Z","title":"MD-BERT: Action Recognition in Dark Videos via Dynamic Multi-Stream\n  Fusion and Temporal Modeling","summary":"  Action recognition in dark, low-light (under-exposed) or noisy videos is a\nchallenging task due to visibility degradation, which can hinder critical\nspatiotemporal details. This paper proposes MD-BERT, a novel multi-stream\napproach that integrates complementary pre-processing techniques such as gamma\ncorrection and histogram equalization alongside raw dark frames to address\nthese challenges. We introduce the Dynamic Feature Fusion (DFF) module,\nextending existing attentional fusion methods to a three-stream setting,\nthereby capturing fine-grained and global contextual information across\ndifferent brightness and contrast enhancements. The fused spatiotemporal\nfeatures are then processed by a BERT-based temporal model, which leverages its\nbidirectional self-attention to effectively capture long-range dependencies and\ncontextual relationships across frames. Extensive experiments on the ARID V1.0\nand ARID V1.5 dark video datasets show that MD-BERT outperforms existing\nmethods, establishing a new state-of-the-art performance. Ablation studies\nfurther highlight the individual contributions of each input stream and the\neffectiveness of the proposed DFF and BERT modules. The official website of\nthis work is available at: https://github.com/HrishavBakulBarua/DarkBERT\n","authors":["Sharana Dharshikgan Suresh Dass","Hrishav Bakul Barua","Ganesh Krishnasamy","Raveendran Paramesran","Raphael C. -W. Phan"],"pdf_url":"https://arxiv.org/pdf/2502.03724v1.pdf","comment":null}]}}